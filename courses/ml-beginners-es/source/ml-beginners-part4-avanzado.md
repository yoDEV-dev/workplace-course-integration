# ML para Principiantes ü§ñ
## Parte 4: Temas Avanzados
**Series de Tiempo, Aprendizaje por Refuerzo y Aplicaciones**

---


# Series de Tiempo

<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "61342603bad8acadbc6b2e4e3aab3f66",
  "translation_date": "2025-09-03T22:41:30+00:00",
  "source_file": "7-TimeSeries/README.md",
  "language_code": "es"
}
-->
# Introducci√≥n a la predicci√≥n de series temporales

¬øQu√© es la predicci√≥n de series temporales? Se trata de predecir eventos futuros analizando las tendencias del pasado.

## Tema regional: uso mundial de electricidad ‚ú®

En estas dos lecciones, se te presentar√° la predicci√≥n de series temporales, un √°rea de aprendizaje autom√°tico algo menos conocida pero que es extremadamente valiosa para aplicaciones industriales y empresariales, entre otros campos. Aunque las redes neuronales pueden usarse para mejorar la utilidad de estos modelos, los estudiaremos en el contexto del aprendizaje autom√°tico cl√°sico, ya que los modelos ayudan a predecir el rendimiento futuro bas√°ndose en el pasado.

Nuestro enfoque regional es el uso el√©ctrico en el mundo, un conjunto de datos interesante para aprender sobre la predicci√≥n del consumo futuro de energ√≠a basado en patrones de carga pasados. Puedes ver c√≥mo este tipo de predicci√≥n puede ser extremadamente √∫til en un entorno empresarial.

![red el√©ctrica](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/translated_images/es/electric-grid.0c21d5214db09ffa.webp)

Foto de [Peddi Sai hrithik](https://unsplash.com/@shutter_log?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) de torres el√©ctricas en una carretera en Rajasthan en [Unsplash](https://unsplash.com/s/photos/electric-india?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)

## Lecciones

1. [Introducci√≥n a la predicci√≥n de series temporales](1-Introduction/README.md)
2. [Construcci√≥n de modelos ARIMA para series temporales](2-ARIMA/README.md)
3. [Construcci√≥n de un Support Vector Regressor para la predicci√≥n de series temporales](3-SVR/README.md)

## Cr√©ditos

"Introducci√≥n a la predicci√≥n de series temporales" fue escrito con ‚ö°Ô∏è por [Francesca Lazzeri](https://twitter.com/frlazzeri) y [Jen Looper](https://twitter.com/jenlooper). Los cuadernos aparecieron por primera vez en l√≠nea en el [repositorio de Azure "Deep Learning For Time Series"](https://github.com/Azure/DeepLearningForTimeSeriesForecasting) originalmente escrito por Francesca Lazzeri. La lecci√≥n de SVR fue escrita por [Anirban Mukherjee](https://github.com/AnirbanMukherjeeXD).

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por garantizar la precisi√≥n, tenga en cuenta que las traducciones automatizadas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err√≥neas que puedan surgir del uso de esta traducci√≥n.

---

<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "662b509c39eee205687726636d0a8455",
  "translation_date": "2025-09-04T22:15:29+00:00",
  "source_file": "7-TimeSeries/1-Introduction/README.md",
  "language_code": "es"
}
-->
# Introducci√≥n a la predicci√≥n de series temporales

![Resumen de series temporales en un sketchnote](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/sketchnotes/ml-timeseries.png)

> Sketchnote por [Tomomi Imura](https://www.twitter.com/girlie_mac)

En esta lecci√≥n y la siguiente, aprender√°s un poco sobre la predicci√≥n de series temporales, una parte interesante y valiosa del repertorio de un cient√≠fico de ML que es un poco menos conocida que otros temas. La predicci√≥n de series temporales es como una especie de 'bola de cristal': bas√°ndote en el rendimiento pasado de una variable como el precio, puedes predecir su valor potencial futuro.

[![Introducci√≥n a la predicci√≥n de series temporales](https://img.youtube.com/vi/cBojo1hsHiI/0.jpg)](https://youtu.be/cBojo1hsHiI "Introducci√≥n a la predicci√≥n de series temporales")

> üé• Haz clic en la imagen de arriba para ver un video sobre la predicci√≥n de series temporales

## [Cuestionario previo a la lecci√≥n](https://ff-quizzes.netlify.app/en/ml/)

Es un campo √∫til e interesante con un valor real para los negocios, dado su uso directo en problemas de precios, inventarios y cuestiones de la cadena de suministro. Aunque las t√©cnicas de aprendizaje profundo han comenzado a usarse para obtener m√°s informaci√≥n y predecir mejor el rendimiento futuro, la predicci√≥n de series temporales sigue siendo un campo muy influenciado por t√©cnicas cl√°sicas de ML.

> El √∫til curr√≠culo de series temporales de Penn State se puede encontrar [aqu√≠](https://online.stat.psu.edu/stat510/lesson/1)

## Introducci√≥n

Supongamos que gestionas una red de parqu√≠metros inteligentes que proporcionan datos sobre la frecuencia y duraci√≥n de su uso a lo largo del tiempo.

> ¬øQu√© pasar√≠a si pudieras predecir, bas√°ndote en el rendimiento pasado del parqu√≠metro, su valor futuro seg√∫n las leyes de la oferta y la demanda?

Predecir con precisi√≥n cu√°ndo actuar para lograr tu objetivo es un desaf√≠o que podr√≠a abordarse con la predicci√≥n de series temporales. No har√≠a feliz a la gente que se les cobre m√°s en horas pico cuando buscan un lugar para estacionar, ¬°pero ser√≠a una forma segura de generar ingresos para limpiar las calles!

Exploremos algunos de los tipos de algoritmos de series temporales y comencemos un cuaderno para limpiar y preparar algunos datos. Los datos que analizar√°s provienen de la competencia de predicci√≥n GEFCom2014. Consisten en 3 a√±os de valores horarios de carga el√©ctrica y temperatura entre 2012 y 2014. Dado el patr√≥n hist√≥rico de carga el√©ctrica y temperatura, puedes predecir valores futuros de carga el√©ctrica.

En este ejemplo, aprender√°s a predecir un paso de tiempo hacia adelante, utilizando √∫nicamente datos hist√≥ricos de carga. Sin embargo, antes de comenzar, es √∫til entender qu√© est√° sucediendo detr√°s de escena.

## Algunas definiciones

Cuando te encuentres con el t√©rmino 'series temporales', necesitas entender su uso en varios contextos diferentes.

üéì **Series temporales**

En matem√°ticas, "una serie temporal es una serie de puntos de datos indexados (o listados o graficados) en orden temporal. M√°s com√∫nmente, una serie temporal es una secuencia tomada en puntos sucesivos igualmente espaciados en el tiempo". Un ejemplo de una serie temporal es el valor de cierre diario del [Promedio Industrial Dow Jones](https://wikipedia.org/wiki/Time_series). El uso de gr√°ficos de series temporales y el modelado estad√≠stico se encuentra frecuentemente en el procesamiento de se√±ales, la predicci√≥n del clima, la predicci√≥n de terremotos y otros campos donde ocurren eventos y los puntos de datos pueden graficarse a lo largo del tiempo.

üéì **An√°lisis de series temporales**

El an√°lisis de series temporales es el an√°lisis de los datos de series temporales mencionados anteriormente. Los datos de series temporales pueden tomar formas distintas, incluyendo 'series temporales interrumpidas', que detectan patrones en la evoluci√≥n de una serie temporal antes y despu√©s de un evento interruptor. El tipo de an√°lisis necesario para la serie temporal depende de la naturaleza de los datos. Los datos de series temporales en s√≠ mismos pueden tomar la forma de series de n√∫meros o caracteres.

El an√°lisis que se realiza utiliza una variedad de m√©todos, incluidos dominio de frecuencia y dominio de tiempo, lineales y no lineales, y m√°s. [Aprende m√°s](https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc4.htm) sobre las muchas formas de analizar este tipo de datos.

üéì **Predicci√≥n de series temporales**

La predicci√≥n de series temporales es el uso de un modelo para predecir valores futuros basados en patrones mostrados por datos previamente recopilados a medida que ocurrieron en el pasado. Aunque es posible usar modelos de regresi√≥n para explorar datos de series temporales, con √≠ndices de tiempo como variables x en un gr√°fico, dichos datos se analizan mejor utilizando tipos especiales de modelos.

Los datos de series temporales son una lista de observaciones ordenadas, a diferencia de los datos que pueden analizarse mediante regresi√≥n lineal. El m√°s com√∫n es ARIMA, un acr√≥nimo que significa "Promedio M√≥vil Integrado Autorregresivo".

[Modelos ARIMA](https://online.stat.psu.edu/stat510/lesson/1/1.1) "relacionan el valor presente de una serie con valores pasados y errores de predicci√≥n pasados". Son m√°s apropiados para analizar datos en el dominio del tiempo, donde los datos est√°n ordenados a lo largo del tiempo.

> Hay varios tipos de modelos ARIMA, que puedes aprender [aqu√≠](https://people.duke.edu/~rnau/411arim.htm) y que abordar√°s en la pr√≥xima lecci√≥n.

En la pr√≥xima lecci√≥n, construir√°s un modelo ARIMA utilizando [Series Temporales Univariadas](https://itl.nist.gov/div898/handbook/pmc/section4/pmc44.htm), que se enfoca en una variable que cambia su valor a lo largo del tiempo. Un ejemplo de este tipo de datos es [este conjunto de datos](https://itl.nist.gov/div898/handbook/pmc/section4/pmc4411.htm) que registra la concentraci√≥n mensual de CO2 en el Observatorio Mauna Loa:

|   CO2   | YearMonth | Year  | Month |
| :-----: | :-------: | :---: | :---: |
| 330.62  |  1975.04  | 1975  |   1   |
| 331.40  |  1975.13  | 1975  |   2   |
| 331.87  |  1975.21  | 1975  |   3   |
| 333.18  |  1975.29  | 1975  |   4   |
| 333.92  |  1975.38  | 1975  |   5   |
| 333.43  |  1975.46  | 1975  |   6   |
| 331.85  |  1975.54  | 1975  |   7   |
| 330.01  |  1975.63  | 1975  |   8   |
| 328.51  |  1975.71  | 1975  |   9   |
| 328.41  |  1975.79  | 1975  |  10   |
| 329.25  |  1975.88  | 1975  |  11   |
| 330.97  |  1975.96  | 1975  |  12   |

‚úÖ Identifica la variable que cambia a lo largo del tiempo en este conjunto de datos.

## Caracter√≠sticas de los datos de series temporales a considerar

Al observar datos de series temporales, podr√≠as notar que tienen [ciertas caracter√≠sticas](https://online.stat.psu.edu/stat510/lesson/1/1.1) que necesitas tener en cuenta y mitigar para comprender mejor sus patrones. Si consideras los datos de series temporales como un posible 'se√±al' que deseas analizar, estas caracter√≠sticas pueden considerarse 'ruido'. A menudo necesitar√°s reducir este 'ruido' compensando algunas de estas caracter√≠sticas utilizando t√©cnicas estad√≠sticas.

Aqu√≠ hay algunos conceptos que deber√≠as conocer para trabajar con series temporales:

üéì **Tendencias**

Las tendencias se definen como aumentos y disminuciones medibles a lo largo del tiempo. [Lee m√°s](https://machinelearningmastery.com/time-series-trends-in-python). En el contexto de series temporales, se trata de c√≥mo usar y, si es necesario, eliminar tendencias de tus series temporales.

üéì **[Estacionalidad](https://machinelearningmastery.com/time-series-seasonality-with-python/)**

La estacionalidad se define como fluctuaciones peri√≥dicas, como las compras navide√±as que podr√≠an afectar las ventas, por ejemplo. [Echa un vistazo](https://itl.nist.gov/div898/handbook/pmc/section4/pmc443.htm) a c√≥mo diferentes tipos de gr√°ficos muestran la estacionalidad en los datos.

üéì **Valores at√≠picos**

Los valores at√≠picos est√°n lejos de la varianza est√°ndar de los datos.

üéì **Ciclo a largo plazo**

Independientemente de la estacionalidad, los datos podr√≠an mostrar un ciclo a largo plazo, como una recesi√≥n econ√≥mica que dura m√°s de un a√±o.

üéì **Varianza constante**

Con el tiempo, algunos datos muestran fluctuaciones constantes, como el uso de energ√≠a durante el d√≠a y la noche.

üéì **Cambios abruptos**

Los datos podr√≠an mostrar un cambio abrupto que podr√≠a necesitar un an√°lisis m√°s profundo. El cierre repentino de negocios debido al COVID, por ejemplo, caus√≥ cambios en los datos.

‚úÖ Aqu√≠ hay un [ejemplo de gr√°fico de series temporales](https://www.kaggle.com/kashnitsky/topic-9-part-1-time-series-analysis-in-python) que muestra el gasto diario en moneda dentro del juego durante algunos a√±os. ¬øPuedes identificar alguna de las caracter√≠sticas mencionadas anteriormente en estos datos?

![Gasto en moneda dentro del juego](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/1-Introduction/images/currency.png)

## Ejercicio - comenzando con datos de uso de energ√≠a

Comencemos creando un modelo de series temporales para predecir el uso futuro de energ√≠a dado el uso pasado.

> Los datos en este ejemplo provienen de la competencia de predicci√≥n GEFCom2014. Consisten en 3 a√±os de valores horarios de carga el√©ctrica y temperatura entre 2012 y 2014.
>
> Tao Hong, Pierre Pinson, Shu Fan, Hamidreza Zareipour, Alberto Troccoli y Rob J. Hyndman, "Probabilistic energy forecasting: Global Energy Forecasting Competition 2014 and beyond", International Journal of Forecasting, vol.32, no.3, pp 896-913, julio-septiembre, 2016.

1. En la carpeta `working` de esta lecci√≥n, abre el archivo _notebook.ipynb_. Comienza agregando bibliotecas que te ayudar√°n a cargar y visualizar datos.

    ```python
    import os
    import matplotlib.pyplot as plt
    from common.utils import load_data
    %matplotlib inline
    ```

    Nota: est√°s utilizando los archivos de la carpeta `common` incluida, que configuran tu entorno y manejan la descarga de los datos.

2. A continuaci√≥n, examina los datos como un dataframe llamando a `load_data()` y `head()`:

    ```python
    data_dir = './data'
    energy = load_data(data_dir)[['load']]
    energy.head()
    ```

    Puedes ver que hay dos columnas que representan la fecha y la carga:

    |                     |  load  |
    | :-----------------: | :----: |
    | 2012-01-01 00:00:00 | 2698.0 |
    | 2012-01-01 01:00:00 | 2558.0 |
    | 2012-01-01 02:00:00 | 2444.0 |
    | 2012-01-01 03:00:00 | 2402.0 |
    | 2012-01-01 04:00:00 | 2403.0 |

3. Ahora, grafica los datos llamando a `plot()`:

    ```python
    energy.plot(y='load', subplots=True, figsize=(15, 8), fontsize=12)
    plt.xlabel('timestamp', fontsize=12)
    plt.ylabel('load', fontsize=12)
    plt.show()
    ```

    ![gr√°fico de energ√≠a](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/1-Introduction/images/energy-plot.png)

4. Ahora, grafica la primera semana de julio de 2014, proporcionando esta fecha como entrada al `energy` en el patr√≥n `[desde fecha]: [hasta fecha]`:

    ```python
    energy['2014-07-01':'2014-07-07'].plot(y='load', subplots=True, figsize=(15, 8), fontsize=12)
    plt.xlabel('timestamp', fontsize=12)
    plt.ylabel('load', fontsize=12)
    plt.show()
    ```

    ![julio](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/1-Introduction/images/july-2014.png)

    ¬°Un gr√°fico hermoso! Observa estos gr√°ficos y ve si puedes determinar alguna de las caracter√≠sticas mencionadas anteriormente. ¬øQu√© podemos deducir al visualizar los datos?

En la pr√≥xima lecci√≥n, crear√°s un modelo ARIMA para generar algunas predicciones.

---

## üöÄDesaf√≠o

Haz una lista de todas las industrias y √°reas de investigaci√≥n que se te ocurran que podr√≠an beneficiarse de la predicci√≥n de series temporales. ¬øPuedes pensar en una aplicaci√≥n de estas t√©cnicas en las artes? ¬øEn econometr√≠a? ¬øEcolog√≠a? ¬øRetail? ¬øIndustria? ¬øFinanzas? ¬øD√≥nde m√°s?

## [Cuestionario posterior a la lecci√≥n](https://ff-quizzes.netlify.app/en/ml/)

## Revisi√≥n y autoestudio

Aunque no los cubriremos aqu√≠, las redes neuronales a veces se utilizan para mejorar los m√©todos cl√°sicos de predicci√≥n de series temporales. Lee m√°s sobre ellas [en este art√≠culo](https://medium.com/microsoftazure/neural-networks-for-forecasting-financial-and-economic-time-series-6aca370ff412).

## Tarea

[Visualiza m√°s series temporales](assignment.md)

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por lograr precisi√≥n, tenga en cuenta que las traducciones autom√°ticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err√≥neas que puedan surgir del uso de esta traducci√≥n.

---

<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "917dbf890db71a322f306050cb284749",
  "translation_date": "2025-09-04T22:14:48+00:00",
  "source_file": "7-TimeSeries/2-ARIMA/README.md",
  "language_code": "es"
}
-->
# Pron√≥stico de series temporales con ARIMA

En la lecci√≥n anterior, aprendiste un poco sobre el pron√≥stico de series temporales y cargaste un conjunto de datos que muestra las fluctuaciones de la carga el√©ctrica a lo largo de un per√≠odo de tiempo.

[![Introducci√≥n a ARIMA](https://img.youtube.com/vi/IUSk-YDau10/0.jpg)](https://youtu.be/IUSk-YDau10 "Introducci√≥n a ARIMA")

> üé• Haz clic en la imagen de arriba para ver un video: Una breve introducci√≥n a los modelos ARIMA. El ejemplo est√° hecho en R, pero los conceptos son universales.

## [Cuestionario previo a la lecci√≥n](https://ff-quizzes.netlify.app/en/ml/)

## Introducci√≥n

En esta lecci√≥n, descubrir√°s una forma espec√≠fica de construir modelos con [ARIMA: *A*uto*R*egressive *I*ntegrated *M*oving *A*verage](https://wikipedia.org/wiki/Autoregressive_integrated_moving_average). Los modelos ARIMA son particularmente adecuados para ajustar datos que muestran [no estacionariedad](https://wikipedia.org/wiki/Stationary_process).

## Conceptos generales

Para trabajar con ARIMA, hay algunos conceptos que necesitas conocer:

- üéì **Estacionariedad**. En un contexto estad√≠stico, la estacionariedad se refiere a datos cuya distribuci√≥n no cambia al desplazarse en el tiempo. Los datos no estacionarios, por lo tanto, muestran fluctuaciones debido a tendencias que deben transformarse para ser analizadas. La estacionalidad, por ejemplo, puede introducir fluctuaciones en los datos y puede eliminarse mediante un proceso de 'diferenciaci√≥n estacional'.

- üéì **[Diferenciaci√≥n](https://wikipedia.org/wiki/Autoregressive_integrated_moving_average#Differencing)**. La diferenciaci√≥n de datos, nuevamente desde un contexto estad√≠stico, se refiere al proceso de transformar datos no estacionarios para hacerlos estacionarios eliminando su tendencia no constante. "La diferenciaci√≥n elimina los cambios en el nivel de una serie temporal, eliminando la tendencia y la estacionalidad y, en consecuencia, estabilizando la media de la serie temporal." [Art√≠culo de Shixiong et al](https://arxiv.org/abs/1904.07632)

## ARIMA en el contexto de series temporales

Desglosemos las partes de ARIMA para entender mejor c√≥mo nos ayuda a modelar series temporales y hacer predicciones.

- **AR - de AutoRegresivo**. Los modelos autorregresivos, como su nombre lo indica, miran 'hacia atr√°s' en el tiempo para analizar valores previos en tus datos y hacer suposiciones sobre ellos. Estos valores previos se llaman 'lags' (rezagos). Un ejemplo ser√≠a un conjunto de datos que muestra las ventas mensuales de l√°pices. El total de ventas de cada mes se considerar√≠a una 'variable evolutiva' en el conjunto de datos. Este modelo se construye como "la variable evolutiva de inter√©s se regresa sobre sus propios valores rezagados (es decir, valores anteriores)." [wikipedia](https://wikipedia.org/wiki/Autoregressive_integrated_moving_average)

- **I - de Integrado**. A diferencia de los modelos similares 'ARMA', la 'I' en ARIMA se refiere a su aspecto *[integrado](https://wikipedia.org/wiki/Order_of_integration)*. Los datos se 'integran' cuando se aplican pasos de diferenciaci√≥n para eliminar la no estacionariedad.

- **MA - de Media M√≥vil**. El aspecto de [media m√≥vil](https://wikipedia.org/wiki/Moving-average_model) de este modelo se refiere a la variable de salida que se determina observando los valores actuales y pasados de los rezagos.

En resumen: ARIMA se utiliza para ajustar un modelo lo m√°s cerca posible a la forma especial de los datos de series temporales.

## Ejercicio - construir un modelo ARIMA

Abre la carpeta [_/working_](https://github.com/microsoft/ML-For-Beginners/tree/main/7-TimeSeries/2-ARIMA/working) en esta lecci√≥n y encuentra el archivo [_notebook.ipynb_](https://github.com/microsoft/ML-For-Beginners/blob/main/7-TimeSeries/2-ARIMA/working/notebook.ipynb).

1. Ejecuta el notebook para cargar la biblioteca de Python `statsmodels`; la necesitar√°s para los modelos ARIMA.

1. Carga las bibliotecas necesarias.

1. Ahora, carga varias bibliotecas m√°s √∫tiles para graficar datos:

    ```python
    import os
    import warnings
    import matplotlib.pyplot as plt
    import numpy as np
    import pandas as pd
    import datetime as dt
    import math

    from pandas.plotting import autocorrelation_plot
    from statsmodels.tsa.statespace.sarimax import SARIMAX
    from sklearn.preprocessing import MinMaxScaler
    from common.utils import load_data, mape
    from IPython.display import Image

    %matplotlib inline
    pd.options.display.float_format = '{:,.2f}'.format
    np.set_printoptions(precision=2)
    warnings.filterwarnings("ignore") # specify to ignore warning messages
    ```

1. Carga los datos del archivo `/data/energy.csv` en un dataframe de Pandas y √©chales un vistazo:

    ```python
    energy = load_data('./data')[['load']]
    energy.head(10)
    ```

1. Grafica todos los datos de energ√≠a disponibles desde enero de 2012 hasta diciembre de 2014. No deber√≠a haber sorpresas, ya que vimos estos datos en la lecci√≥n anterior:

    ```python
    energy.plot(y='load', subplots=True, figsize=(15, 8), fontsize=12)
    plt.xlabel('timestamp', fontsize=12)
    plt.ylabel('load', fontsize=12)
    plt.show()
    ```

    ¬°Ahora, construyamos un modelo!

### Crear conjuntos de datos de entrenamiento y prueba

Ahora que tus datos est√°n cargados, puedes separarlos en conjuntos de entrenamiento y prueba. Entrenar√°s tu modelo en el conjunto de entrenamiento. Como de costumbre, despu√©s de que el modelo haya terminado de entrenar, evaluar√°s su precisi√≥n utilizando el conjunto de prueba. Debes asegurarte de que el conjunto de prueba cubra un per√≠odo posterior en el tiempo al conjunto de entrenamiento para garantizar que el modelo no obtenga informaci√≥n de per√≠odos futuros.

1. Asigna un per√≠odo de dos meses desde el 1 de septiembre hasta el 31 de octubre de 2014 al conjunto de entrenamiento. El conjunto de prueba incluir√° el per√≠odo de dos meses del 1 de noviembre al 31 de diciembre de 2014:

    ```python
    train_start_dt = '2014-11-01 00:00:00'
    test_start_dt = '2014-12-30 00:00:00'
    ```

    Dado que estos datos reflejan el consumo diario de energ√≠a, hay un fuerte patr√≥n estacional, pero el consumo es m√°s similar al consumo de d√≠as m√°s recientes.

1. Visualiza las diferencias:

    ```python
    energy[(energy.index < test_start_dt) & (energy.index >= train_start_dt)][['load']].rename(columns={'load':'train'}) \
        .join(energy[test_start_dt:][['load']].rename(columns={'load':'test'}), how='outer') \
        .plot(y=['train', 'test'], figsize=(15, 8), fontsize=12)
    plt.xlabel('timestamp', fontsize=12)
    plt.ylabel('load', fontsize=12)
    plt.show()
    ```

    ![datos de entrenamiento y prueba](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/2-ARIMA/images/train-test.png)

    Por lo tanto, usar una ventana de tiempo relativamente peque√±a para entrenar los datos deber√≠a ser suficiente.

    > Nota: Dado que la funci√≥n que usamos para ajustar el modelo ARIMA utiliza validaci√≥n dentro de la muestra durante el ajuste, omitiremos los datos de validaci√≥n.

### Preparar los datos para el entrenamiento

Ahora necesitas preparar los datos para el entrenamiento realizando un filtrado y escalado de tus datos. Filtra tu conjunto de datos para incluir solo los per√≠odos de tiempo y columnas que necesitas, y escala los datos para asegurarte de que est√©n proyectados en el intervalo 0,1.

1. Filtra el conjunto de datos original para incluir solo los per√≠odos de tiempo mencionados por conjunto e incluyendo √∫nicamente la columna necesaria 'load' m√°s la fecha:

    ```python
    train = energy.copy()[(energy.index >= train_start_dt) & (energy.index < test_start_dt)][['load']]
    test = energy.copy()[energy.index >= test_start_dt][['load']]

    print('Training data shape: ', train.shape)
    print('Test data shape: ', test.shape)
    ```

    Puedes ver la forma de los datos:

    ```output
    Training data shape:  (1416, 1)
    Test data shape:  (48, 1)
    ```

1. Escala los datos para que est√©n en el rango (0, 1).

    ```python
    scaler = MinMaxScaler()
    train['load'] = scaler.fit_transform(train)
    train.head(10)
    ```

1. Visualiza los datos originales vs. los escalados:

    ```python
    energy[(energy.index >= train_start_dt) & (energy.index < test_start_dt)][['load']].rename(columns={'load':'original load'}).plot.hist(bins=100, fontsize=12)
    train.rename(columns={'load':'scaled load'}).plot.hist(bins=100, fontsize=12)
    plt.show()
    ```

    ![original](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/2-ARIMA/images/original.png)

    > Los datos originales

    ![escalados](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/2-ARIMA/images/scaled.png)

    > Los datos escalados

1. Ahora que has calibrado los datos escalados, puedes escalar los datos de prueba:

    ```python
    test['load'] = scaler.transform(test)
    test.head()
    ```

### Implementar ARIMA

¬°Es hora de implementar ARIMA! Ahora usar√°s la biblioteca `statsmodels` que instalaste anteriormente.

Ahora necesitas seguir varios pasos:

   1. Define el modelo llamando a `SARIMAX()` y pasando los par√°metros del modelo: par√°metros p, d y q, y par√°metros P, D y Q.
   2. Prepara el modelo para los datos de entrenamiento llamando a la funci√≥n `fit()`.
   3. Realiza predicciones llamando a la funci√≥n `forecast()` y especificando el n√∫mero de pasos (el `horizonte`) a pronosticar.

> üéì ¬øPara qu√© son todos estos par√°metros? En un modelo ARIMA hay 3 par√°metros que se utilizan para ayudar a modelar los aspectos principales de una serie temporal: estacionalidad, tendencia y ruido. Estos par√°metros son:

`p`: el par√°metro asociado con el aspecto autorregresivo del modelo, que incorpora valores *pasados*.  
`d`: el par√°metro asociado con la parte integrada del modelo, que afecta la cantidad de *diferenciaci√≥n* (üéì ¬ørecuerdas la diferenciaci√≥n üëÜ?) que se aplica a una serie temporal.  
`q`: el par√°metro asociado con la parte de media m√≥vil del modelo.  

> Nota: Si tus datos tienen un aspecto estacional - como en este caso -, usamos un modelo ARIMA estacional (SARIMA). En ese caso, necesitas usar otro conjunto de par√°metros: `P`, `D` y `Q`, que describen las mismas asociaciones que `p`, `d` y `q`, pero corresponden a los componentes estacionales del modelo.

1. Comienza configurando tu valor de horizonte preferido. Probemos con 3 horas:

    ```python
    # Specify the number of steps to forecast ahead
    HORIZON = 3
    print('Forecasting horizon:', HORIZON, 'hours')
    ```

    Seleccionar los mejores valores para los par√°metros de un modelo ARIMA puede ser un desaf√≠o, ya que es algo subjetivo y requiere tiempo. Podr√≠as considerar usar una funci√≥n `auto_arima()` de la [biblioteca `pyramid`](https://alkaline-ml.com/pmdarima/0.9.0/modules/generated/pyramid.arima.auto_arima.html).

1. Por ahora, intenta algunas selecciones manuales para encontrar un buen modelo.

    ```python
    order = (4, 1, 0)
    seasonal_order = (1, 1, 0, 24)

    model = SARIMAX(endog=train, order=order, seasonal_order=seasonal_order)
    results = model.fit()

    print(results.summary())
    ```

    Se imprime una tabla de resultados.

¬°Has construido tu primer modelo! Ahora necesitamos encontrar una forma de evaluarlo.

### Evaluar tu modelo

Para evaluar tu modelo, puedes realizar la llamada validaci√≥n `walk forward`. En la pr√°ctica, los modelos de series temporales se reentrenan cada vez que se dispone de nuevos datos. Esto permite que el modelo haga el mejor pron√≥stico en cada paso de tiempo.

Comenzando al principio de la serie temporal con esta t√©cnica, entrena el modelo en el conjunto de datos de entrenamiento. Luego realiza una predicci√≥n en el siguiente paso de tiempo. La predicci√≥n se eval√∫a en comparaci√≥n con el valor conocido. El conjunto de entrenamiento se ampl√≠a para incluir el valor conocido y el proceso se repite.

> Nota: Deber√≠as mantener fija la ventana del conjunto de entrenamiento para un entrenamiento m√°s eficiente, de modo que cada vez que agregues una nueva observaci√≥n al conjunto de entrenamiento, elimines la observaci√≥n del principio del conjunto.

Este proceso proporciona una estimaci√≥n m√°s robusta de c√≥mo se desempe√±ar√° el modelo en la pr√°ctica. Sin embargo, tiene el costo computacional de crear tantos modelos. Esto es aceptable si los datos son peque√±os o si el modelo es simple, pero podr√≠a ser un problema a gran escala.

La validaci√≥n walk-forward es el est√°ndar de oro para la evaluaci√≥n de modelos de series temporales y se recomienda para tus propios proyectos.

1. Primero, crea un punto de datos de prueba para cada paso del HORIZON.

    ```python
    test_shifted = test.copy()

    for t in range(1, HORIZON+1):
        test_shifted['load+'+str(t)] = test_shifted['load'].shift(-t, freq='H')

    test_shifted = test_shifted.dropna(how='any')
    test_shifted.head(5)
    ```

    |            |          | load | load+1 | load+2 |
    | ---------- | -------- | ---- | ------ | ------ |
    | 2014-12-30 | 00:00:00 | 0.33 | 0.29   | 0.27   |
    | 2014-12-30 | 01:00:00 | 0.29 | 0.27   | 0.27   |
    | 2014-12-30 | 02:00:00 | 0.27 | 0.27   | 0.30   |
    | 2014-12-30 | 03:00:00 | 0.27 | 0.30   | 0.41   |
    | 2014-12-30 | 04:00:00 | 0.30 | 0.41   | 0.57   |

    Los datos se desplazan horizontalmente seg√∫n su punto de horizonte.

1. Realiza predicciones en tus datos de prueba utilizando este enfoque de ventana deslizante en un bucle del tama√±o de la longitud de los datos de prueba:

    ```python
    %%time
    training_window = 720 # dedicate 30 days (720 hours) for training

    train_ts = train['load']
    test_ts = test_shifted

    history = [x for x in train_ts]
    history = history[(-training_window):]

    predictions = list()

    order = (2, 1, 0)
    seasonal_order = (1, 1, 0, 24)

    for t in range(test_ts.shape[0]):
        model = SARIMAX(endog=history, order=order, seasonal_order=seasonal_order)
        model_fit = model.fit()
        yhat = model_fit.forecast(steps = HORIZON)
        predictions.append(yhat)
        obs = list(test_ts.iloc[t])
        # move the training window
        history.append(obs[0])
        history.pop(0)
        print(test_ts.index[t])
        print(t+1, ': predicted =', yhat, 'expected =', obs)
    ```

    Puedes observar el entrenamiento en curso:

    ```output
    2014-12-30 00:00:00
    1 : predicted = [0.32 0.29 0.28] expected = [0.32945389435989236, 0.2900626678603402, 0.2739480752014323]

    2014-12-30 01:00:00
    2 : predicted = [0.3  0.29 0.3 ] expected = [0.2900626678603402, 0.2739480752014323, 0.26812891674127126]

    2014-12-30 02:00:00
    3 : predicted = [0.27 0.28 0.32] expected = [0.2739480752014323, 0.26812891674127126, 0.3025962399283795]
    ```

1. Compara las predicciones con la carga real:

    ```python
    eval_df = pd.DataFrame(predictions, columns=['t+'+str(t) for t in range(1, HORIZON+1)])
    eval_df['timestamp'] = test.index[0:len(test.index)-HORIZON+1]
    eval_df = pd.melt(eval_df, id_vars='timestamp', value_name='prediction', var_name='h')
    eval_df['actual'] = np.array(np.transpose(test_ts)).ravel()
    eval_df[['prediction', 'actual']] = scaler.inverse_transform(eval_df[['prediction', 'actual']])
    eval_df.head()
    ```

    Salida  
    |     |            | timestamp | h   | prediction | actual   |
    | --- | ---------- | --------- | --- | ---------- | -------- |
    | 0   | 2014-12-30 | 00:00:00  | t+1 | 3,008.74   | 3,023.00 |
    | 1   | 2014-12-30 | 01:00:00  | t+1 | 2,955.53   | 2,935.00 |
    | 2   | 2014-12-30 | 02:00:00  | t+1 | 2,900.17   | 2,899.00 |
    | 3   | 2014-12-30 | 03:00:00  | t+1 | 2,917.69   | 2,886.00 |
    | 4   | 2014-12-30 | 04:00:00  | t+1 | 2,946.99   | 2,963.00 |

    Observa la predicci√≥n de los datos horarios en comparaci√≥n con la carga real. ¬øQu√© tan precisa es?

### Verificar la precisi√≥n del modelo

Verifica la precisi√≥n de tu modelo probando su error porcentual absoluto medio (MAPE) en todas las predicciones.
> **üßÆ Mu√©strame las matem√°ticas**
>
> ![MAPE](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/2-ARIMA/images/mape.png)
>
> [MAPE](https://www.linkedin.com/pulse/what-mape-mad-msd-time-series-allameh-statistics/) se utiliza para mostrar la precisi√≥n de las predicciones como una proporci√≥n definida por la f√≥rmula anterior. La diferencia entre el valor real y el valor predicho se divide por el valor real.  
> "El valor absoluto en este c√°lculo se suma para cada punto pronosticado en el tiempo y se divide por el n√∫mero de puntos ajustados n." [wikipedia](https://wikipedia.org/wiki/Mean_absolute_percentage_error)
1. Expresar la ecuaci√≥n en c√≥digo:

    ```python
    if(HORIZON > 1):
        eval_df['APE'] = (eval_df['prediction'] - eval_df['actual']).abs() / eval_df['actual']
        print(eval_df.groupby('h')['APE'].mean())
    ```

1. Calcular el MAPE de un paso:

    ```python
    print('One step forecast MAPE: ', (mape(eval_df[eval_df['h'] == 't+1']['prediction'], eval_df[eval_df['h'] == 't+1']['actual']))*100, '%')
    ```

    MAPE de pron√≥stico de un paso:  0.5570581332313952 %

1. Imprimir el MAPE del pron√≥stico de m√∫ltiples pasos:

    ```python
    print('Multi-step forecast MAPE: ', mape(eval_df['prediction'], eval_df['actual'])*100, '%')
    ```

    ```output
    Multi-step forecast MAPE:  1.1460048657704118 %
    ```

    Un n√∫mero bajo es lo mejor: considera que un pron√≥stico con un MAPE de 10 est√° desviado en un 10%.

1. Pero como siempre, es m√°s f√°cil ver este tipo de medici√≥n de precisi√≥n de forma visual, as√≠ que vamos a graficarlo:

    ```python
     if(HORIZON == 1):
        ## Plotting single step forecast
        eval_df.plot(x='timestamp', y=['actual', 'prediction'], style=['r', 'b'], figsize=(15, 8))

    else:
        ## Plotting multi step forecast
        plot_df = eval_df[(eval_df.h=='t+1')][['timestamp', 'actual']]
        for t in range(1, HORIZON+1):
            plot_df['t+'+str(t)] = eval_df[(eval_df.h=='t+'+str(t))]['prediction'].values

        fig = plt.figure(figsize=(15, 8))
        ax = plt.plot(plot_df['timestamp'], plot_df['actual'], color='red', linewidth=4.0)
        ax = fig.add_subplot(111)
        for t in range(1, HORIZON+1):
            x = plot_df['timestamp'][(t-1):]
            y = plot_df['t+'+str(t)][0:len(x)]
            ax.plot(x, y, color='blue', linewidth=4*math.pow(.9,t), alpha=math.pow(0.8,t))

        ax.legend(loc='best')

    plt.xlabel('timestamp', fontsize=12)
    plt.ylabel('load', fontsize=12)
    plt.show()
    ```

    ![un modelo de series temporales](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/2-ARIMA/images/accuracy.png)

üèÜ Un gr√°fico muy bueno, mostrando un modelo con buena precisi√≥n. ¬°Bien hecho!

---

## üöÄDesaf√≠o

Investiga las formas de probar la precisi√≥n de un modelo de series temporales. En esta lecci√≥n hablamos sobre el MAPE, pero ¬øhay otros m√©todos que podr√≠as usar? Invest√≠galos y an√≥talos. Un documento √∫til se puede encontrar [aqu√≠](https://otexts.com/fpp2/accuracy.html)

## [Cuestionario posterior a la lecci√≥n](https://ff-quizzes.netlify.app/en/ml/)

## Revisi√≥n y autoestudio

Esta lecci√≥n solo toca los conceptos b√°sicos de la predicci√≥n de series temporales con ARIMA. T√≥mate un tiempo para profundizar en tu conocimiento explorando [este repositorio](https://microsoft.github.io/forecasting/) y sus diversos tipos de modelos para aprender otras formas de construir modelos de series temporales.

## Tarea

[Un nuevo modelo ARIMA](assignment.md)

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por lograr precisi√≥n, tenga en cuenta que las traducciones autom√°ticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err√≥neas que puedan surgir del uso de esta traducci√≥n.

---

<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "482bccabe1df958496ea71a3667995cd",
  "translation_date": "2025-09-04T22:16:07+00:00",
  "source_file": "7-TimeSeries/3-SVR/README.md",
  "language_code": "es"
}
-->
# Pron√≥stico de Series Temporales con Support Vector Regressor

En la lecci√≥n anterior, aprendiste a usar el modelo ARIMA para realizar predicciones de series temporales. Ahora explorar√°s el modelo Support Vector Regressor, que es un modelo de regresi√≥n utilizado para predecir datos continuos.

## [Cuestionario previo a la lecci√≥n](https://ff-quizzes.netlify.app/en/ml/) 

## Introducci√≥n

En esta lecci√≥n, descubrir√°s una forma espec√≠fica de construir modelos con [**SVM**: **S**upport **V**ector **M**achine](https://en.wikipedia.org/wiki/Support-vector_machine) para regresi√≥n, o **SVR: Support Vector Regressor**. 

### SVR en el contexto de series temporales [^1]

Antes de entender la importancia de SVR en la predicci√≥n de series temporales, aqu√≠ tienes algunos conceptos importantes que necesitas conocer:

- **Regresi√≥n:** T√©cnica de aprendizaje supervisado para predecir valores continuos a partir de un conjunto de entradas. La idea es ajustar una curva (o l√≠nea) en el espacio de caracter√≠sticas que tenga el mayor n√∫mero de puntos de datos. [Haz clic aqu√≠](https://en.wikipedia.org/wiki/Regression_analysis) para m√°s informaci√≥n.
- **Support Vector Machine (SVM):** Un tipo de modelo de aprendizaje supervisado utilizado para clasificaci√≥n, regresi√≥n y detecci√≥n de valores at√≠picos. El modelo es un hiperplano en el espacio de caracter√≠sticas, que en el caso de clasificaci√≥n act√∫a como un l√≠mite, y en el caso de regresi√≥n act√∫a como la l√≠nea de mejor ajuste. En SVM, generalmente se utiliza una funci√≥n Kernel para transformar el conjunto de datos a un espacio de mayor n√∫mero de dimensiones, de modo que puedan ser f√°cilmente separables. [Haz clic aqu√≠](https://en.wikipedia.org/wiki/Support-vector_machine) para m√°s informaci√≥n sobre SVMs.
- **Support Vector Regressor (SVR):** Un tipo de SVM, que encuentra la l√≠nea de mejor ajuste (que en el caso de SVM es un hiperplano) que tiene el mayor n√∫mero de puntos de datos.

### ¬øPor qu√© SVR? [^1]

En la √∫ltima lecci√≥n aprendiste sobre ARIMA, que es un m√©todo estad√≠stico lineal muy exitoso para pronosticar datos de series temporales. Sin embargo, en muchos casos, los datos de series temporales tienen *no linealidad*, que no puede ser modelada por modelos lineales. En tales casos, la capacidad de SVM para considerar la no linealidad en los datos para tareas de regresi√≥n hace que SVR sea exitoso en el pron√≥stico de series temporales.

## Ejercicio - construir un modelo SVR

Los primeros pasos para la preparaci√≥n de datos son los mismos que en la lecci√≥n anterior sobre [ARIMA](https://github.com/microsoft/ML-For-Beginners/tree/main/7-TimeSeries/2-ARIMA). 

Abre la carpeta [_/working_](https://github.com/microsoft/ML-For-Beginners/tree/main/7-TimeSeries/3-SVR/working) en esta lecci√≥n y encuentra el archivo [_notebook.ipynb_](https://github.com/microsoft/ML-For-Beginners/blob/main/7-TimeSeries/3-SVR/working/notebook.ipynb). [^2]

1. Ejecuta el notebook e importa las bibliotecas necesarias: [^2]

   ```python
   import sys
   sys.path.append('../../')
   ```

   ```python
   import os
   import warnings
   import matplotlib.pyplot as plt
   import numpy as np
   import pandas as pd
   import datetime as dt
   import math
   
   from sklearn.svm import SVR
   from sklearn.preprocessing import MinMaxScaler
   from common.utils import load_data, mape
   ```

2. Carga los datos del archivo `/data/energy.csv` en un dataframe de Pandas y √©chales un vistazo: [^2]

   ```python
   energy = load_data('../../data')[['load']]
   ```

3. Grafica todos los datos de energ√≠a disponibles desde enero de 2012 hasta diciembre de 2014: [^2]

   ```python
   energy.plot(y='load', subplots=True, figsize=(15, 8), fontsize=12)
   plt.xlabel('timestamp', fontsize=12)
   plt.ylabel('load', fontsize=12)
   plt.show()
   ```

   ![datos completos](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/3-SVR/images/full-data.png)

   Ahora, construyamos nuestro modelo SVR.

### Crear conjuntos de entrenamiento y prueba

Ahora que tus datos est√°n cargados, puedes separarlos en conjuntos de entrenamiento y prueba. Luego, reformatear√°s los datos para crear un conjunto de datos basado en pasos de tiempo, que ser√° necesario para el SVR. Entrenar√°s tu modelo en el conjunto de entrenamiento. Una vez que el modelo haya terminado de entrenarse, evaluar√°s su precisi√≥n en el conjunto de entrenamiento, el conjunto de prueba y luego en el conjunto de datos completo para ver el rendimiento general. Debes asegurarte de que el conjunto de prueba cubra un per√≠odo posterior en el tiempo al conjunto de entrenamiento para garantizar que el modelo no obtenga informaci√≥n de per√≠odos futuros [^2] (una situaci√≥n conocida como *sobreajuste*).

1. Asigna un per√≠odo de dos meses, del 1 de septiembre al 31 de octubre de 2014, al conjunto de entrenamiento. El conjunto de prueba incluir√° el per√≠odo de dos meses del 1 de noviembre al 31 de diciembre de 2014: [^2]

   ```python
   train_start_dt = '2014-11-01 00:00:00'
   test_start_dt = '2014-12-30 00:00:00'
   ```

2. Visualiza las diferencias: [^2]

   ```python
   energy[(energy.index < test_start_dt) & (energy.index >= train_start_dt)][['load']].rename(columns={'load':'train'}) \
       .join(energy[test_start_dt:][['load']].rename(columns={'load':'test'}), how='outer') \
       .plot(y=['train', 'test'], figsize=(15, 8), fontsize=12)
   plt.xlabel('timestamp', fontsize=12)
   plt.ylabel('load', fontsize=12)
   plt.show()
   ```

   ![datos de entrenamiento y prueba](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/3-SVR/images/train-test.png)

### Preparar los datos para el entrenamiento

Ahora necesitas preparar los datos para el entrenamiento realizando un filtrado y escalado de tus datos. Filtra tu conjunto de datos para incluir solo los per√≠odos de tiempo y columnas necesarios, y escala los datos para asegurarte de que est√©n proyectados en el intervalo 0,1.

1. Filtra el conjunto de datos original para incluir solo los per√≠odos de tiempo mencionados anteriormente por conjunto e incluyendo √∫nicamente la columna 'load' m√°s la fecha: [^2]

   ```python
   train = energy.copy()[(energy.index >= train_start_dt) & (energy.index < test_start_dt)][['load']]
   test = energy.copy()[energy.index >= test_start_dt][['load']]
   
   print('Training data shape: ', train.shape)
   print('Test data shape: ', test.shape)
   ```

   ```output
   Training data shape:  (1416, 1)
   Test data shape:  (48, 1)
   ```
   
2. Escala los datos de entrenamiento para que est√©n en el rango (0, 1): [^2]

   ```python
   scaler = MinMaxScaler()
   train['load'] = scaler.fit_transform(train)
   ```
   
4. Ahora, escala los datos de prueba: [^2]

   ```python
   test['load'] = scaler.transform(test)
   ```

### Crear datos con pasos de tiempo [^1]

Para el SVR, transformas los datos de entrada para que tengan la forma `[batch, timesteps]`. Por lo tanto, reformateas los `train_data` y `test_data` existentes de manera que haya una nueva dimensi√≥n que se refiera a los pasos de tiempo. 

```python
# Converting to numpy arrays
train_data = train.values
test_data = test.values
```

Para este ejemplo, tomamos `timesteps = 5`. Por lo tanto, las entradas al modelo son los datos de los primeros 4 pasos de tiempo, y la salida ser√°n los datos del quinto paso de tiempo.

```python
timesteps=5
```

Convirtiendo los datos de entrenamiento a un tensor 2D usando comprensi√≥n de listas anidadas:

```python
train_data_timesteps=np.array([[j for j in train_data[i:i+timesteps]] for i in range(0,len(train_data)-timesteps+1)])[:,:,0]
train_data_timesteps.shape
```

```output
(1412, 5)
```

Convirtiendo los datos de prueba a un tensor 2D:

```python
test_data_timesteps=np.array([[j for j in test_data[i:i+timesteps]] for i in range(0,len(test_data)-timesteps+1)])[:,:,0]
test_data_timesteps.shape
```

```output
(44, 5)
```

Seleccionando entradas y salidas de los datos de entrenamiento y prueba:

```python
x_train, y_train = train_data_timesteps[:,:timesteps-1],train_data_timesteps[:,[timesteps-1]]
x_test, y_test = test_data_timesteps[:,:timesteps-1],test_data_timesteps[:,[timesteps-1]]

print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)
```

```output
(1412, 4) (1412, 1)
(44, 4) (44, 1)
```

### Implementar SVR [^1]

Ahora es momento de implementar SVR. Para leer m√°s sobre esta implementaci√≥n, puedes consultar [esta documentaci√≥n](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html). Para nuestra implementaci√≥n, seguimos estos pasos:

  1. Define el modelo llamando a `SVR()` y pasando los hiperpar√°metros del modelo: kernel, gamma, c y epsilon.
  2. Prepara el modelo para los datos de entrenamiento llamando a la funci√≥n `fit()`.
  3. Realiza predicciones llamando a la funci√≥n `predict()`.

Ahora creamos un modelo SVR. Aqu√≠ usamos el [kernel RBF](https://scikit-learn.org/stable/modules/svm.html#parameters-of-the-rbf-kernel), y configuramos los hiperpar√°metros gamma, C y epsilon como 0.5, 10 y 0.05 respectivamente.

```python
model = SVR(kernel='rbf',gamma=0.5, C=10, epsilon = 0.05)
```

#### Ajustar el modelo a los datos de entrenamiento [^1]

```python
model.fit(x_train, y_train[:,0])
```

```output
SVR(C=10, cache_size=200, coef0=0.0, degree=3, epsilon=0.05, gamma=0.5,
    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)
```

#### Realizar predicciones con el modelo [^1]

```python
y_train_pred = model.predict(x_train).reshape(-1,1)
y_test_pred = model.predict(x_test).reshape(-1,1)

print(y_train_pred.shape, y_test_pred.shape)
```

```output
(1412, 1) (44, 1)
```

¬°Has construido tu SVR! Ahora necesitamos evaluarlo.

### Evaluar tu modelo [^1]

Para la evaluaci√≥n, primero escalaremos los datos de vuelta a nuestra escala original. Luego, para verificar el rendimiento, graficaremos la serie temporal original y la predicha, y tambi√©n imprimiremos el resultado de MAPE.

Escala la salida predicha y la original:

```python
# Scaling the predictions
y_train_pred = scaler.inverse_transform(y_train_pred)
y_test_pred = scaler.inverse_transform(y_test_pred)

print(len(y_train_pred), len(y_test_pred))
```

```python
# Scaling the original values
y_train = scaler.inverse_transform(y_train)
y_test = scaler.inverse_transform(y_test)

print(len(y_train), len(y_test))
```

#### Verificar el rendimiento del modelo en los datos de entrenamiento y prueba [^1]

Extraemos las marcas de tiempo del conjunto de datos para mostrarlas en el eje x de nuestro gr√°fico. Ten en cuenta que estamos utilizando los primeros ```timesteps-1``` valores como entrada para la primera salida, por lo que las marcas de tiempo para la salida comenzar√°n despu√©s de eso.

```python
train_timestamps = energy[(energy.index < test_start_dt) & (energy.index >= train_start_dt)].index[timesteps-1:]
test_timestamps = energy[test_start_dt:].index[timesteps-1:]

print(len(train_timestamps), len(test_timestamps))
```

```output
1412 44
```

Grafica las predicciones para los datos de entrenamiento:

```python
plt.figure(figsize=(25,6))
plt.plot(train_timestamps, y_train, color = 'red', linewidth=2.0, alpha = 0.6)
plt.plot(train_timestamps, y_train_pred, color = 'blue', linewidth=0.8)
plt.legend(['Actual','Predicted'])
plt.xlabel('Timestamp')
plt.title("Training data prediction")
plt.show()
```

![predicci√≥n de datos de entrenamiento](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/3-SVR/images/train-data-predict.png)

Imprime el MAPE para los datos de entrenamiento:

```python
print('MAPE for training data: ', mape(y_train_pred, y_train)*100, '%')
```

```output
MAPE for training data: 1.7195710200875551 %
```

Grafica las predicciones para los datos de prueba:

```python
plt.figure(figsize=(10,3))
plt.plot(test_timestamps, y_test, color = 'red', linewidth=2.0, alpha = 0.6)
plt.plot(test_timestamps, y_test_pred, color = 'blue', linewidth=0.8)
plt.legend(['Actual','Predicted'])
plt.xlabel('Timestamp')
plt.show()
```

![predicci√≥n de datos de prueba](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/3-SVR/images/test-data-predict.png)

Imprime el MAPE para los datos de prueba:

```python
print('MAPE for testing data: ', mape(y_test_pred, y_test)*100, '%')
```

```output
MAPE for testing data:  1.2623790187854018 %
```

üèÜ ¬°Tienes un muy buen resultado en el conjunto de datos de prueba!

### Verificar el rendimiento del modelo en el conjunto de datos completo [^1]

```python
# Extracting load values as numpy array
data = energy.copy().values

# Scaling
data = scaler.transform(data)

# Transforming to 2D tensor as per model input requirement
data_timesteps=np.array([[j for j in data[i:i+timesteps]] for i in range(0,len(data)-timesteps+1)])[:,:,0]
print("Tensor shape: ", data_timesteps.shape)

# Selecting inputs and outputs from data
X, Y = data_timesteps[:,:timesteps-1],data_timesteps[:,[timesteps-1]]
print("X shape: ", X.shape,"\nY shape: ", Y.shape)
```

```output
Tensor shape:  (26300, 5)
X shape:  (26300, 4) 
Y shape:  (26300, 1)
```

```python
# Make model predictions
Y_pred = model.predict(X).reshape(-1,1)

# Inverse scale and reshape
Y_pred = scaler.inverse_transform(Y_pred)
Y = scaler.inverse_transform(Y)
```

```python
plt.figure(figsize=(30,8))
plt.plot(Y, color = 'red', linewidth=2.0, alpha = 0.6)
plt.plot(Y_pred, color = 'blue', linewidth=0.8)
plt.legend(['Actual','Predicted'])
plt.xlabel('Timestamp')
plt.show()
```

![predicci√≥n de datos completos](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/3-SVR/images/full-data-predict.png)

```python
print('MAPE: ', mape(Y_pred, Y)*100, '%')
```

```output
MAPE:  2.0572089029888656 %
```

üèÜ Muy buenos gr√°ficos, mostrando un modelo con buena precisi√≥n. ¬°Bien hecho!

---

## üöÄDesaf√≠o

- Intenta ajustar los hiperpar√°metros (gamma, C, epsilon) al crear el modelo y eval√∫alo en los datos para ver qu√© conjunto de hiperpar√°metros da los mejores resultados en los datos de prueba. Para saber m√°s sobre estos hiperpar√°metros, puedes consultar el documento [aqu√≠](https://scikit-learn.org/stable/modules/svm.html#parameters-of-the-rbf-kernel). 
- Intenta usar diferentes funciones kernel para el modelo y analiza su rendimiento en el conjunto de datos. Un documento √∫til se encuentra [aqu√≠](https://scikit-learn.org/stable/modules/svm.html#kernel-functions).
- Intenta usar diferentes valores para `timesteps` para que el modelo mire hacia atr√°s y haga predicciones.

## [Cuestionario posterior a la lecci√≥n](https://ff-quizzes.netlify.app/en/ml/)

## Revisi√≥n y Autoestudio

Esta lecci√≥n fue para introducir la aplicaci√≥n de SVR para el pron√≥stico de series temporales. Para leer m√°s sobre SVR, puedes consultar [este blog](https://www.analyticsvidhya.com/blog/2020/03/support-vector-regression-tutorial-for-machine-learning/). Esta [documentaci√≥n en scikit-learn](https://scikit-learn.org/stable/modules/svm.html) proporciona una explicaci√≥n m√°s completa sobre SVMs en general, [SVRs](https://scikit-learn.org/stable/modules/svm.html#regression) y tambi√©n otros detalles de implementaci√≥n como las diferentes [funciones kernel](https://scikit-learn.org/stable/modules/svm.html#kernel-functions) que se pueden usar y sus par√°metros.

## Tarea

[Un nuevo modelo SVR](assignment.md)

## Cr√©ditos

[^1]: El texto, c√≥digo y salida en esta secci√≥n fueron contribuidos por [@AnirbanMukherjeeXD](https://github.com/AnirbanMukherjeeXD)
[^2]: El texto, c√≥digo y salida en esta secci√≥n fueron tomados de [ARIMA](https://github.com/microsoft/ML-For-Beginners/tree/main/7-TimeSeries/2-ARIMA)

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por lograr precisi√≥n, tenga en cuenta que las traducciones autom√°ticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err√≥neas que puedan surgir del uso de esta traducci√≥n.

---


# Aprendizaje por Refuerzo

<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "20ca019012b1725de956681d036d8b18",
  "translation_date": "2025-09-04T00:14:18+00:00",
  "source_file": "8-Reinforcement/README.md",
  "language_code": "es"
}
-->
# Introducci√≥n al aprendizaje por refuerzo

El aprendizaje por refuerzo, RL, se considera uno de los paradigmas b√°sicos del aprendizaje autom√°tico, junto con el aprendizaje supervisado y el aprendizaje no supervisado. RL trata sobre decisiones: tomar las decisiones correctas o, al menos, aprender de ellas.

Imagina que tienes un entorno simulado como el mercado de valores. ¬øQu√© sucede si impones una regulaci√≥n espec√≠fica? ¬øTiene un efecto positivo o negativo? Si ocurre algo negativo, necesitas tomar este _refuerzo negativo_, aprender de ello y cambiar de rumbo. Si el resultado es positivo, necesitas construir sobre ese _refuerzo positivo_.

![Pedro y el lobo](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/translated_images/es/peter.779730f9ba3a8a8d.webp)

> ¬°Pedro y sus amigos necesitan escapar del lobo hambriento! Imagen por [Jen Looper](https://twitter.com/jenlooper)

## Tema regional: Pedro y el Lobo (Rusia)

[Pedro y el Lobo](https://es.wikipedia.org/wiki/Pedro_y_el_lobo) es un cuento musical escrito por el compositor ruso [Sergei Prokofiev](https://es.wikipedia.org/wiki/Sergu%C3%A9i_Prok%C3%B3fiev). Es una historia sobre el joven pionero Pedro, quien valientemente sale de su casa hacia el claro del bosque para perseguir al lobo. En esta secci√≥n, entrenaremos algoritmos de aprendizaje autom√°tico que ayudar√°n a Pedro:

- **Explorar** el √°rea circundante y construir un mapa de navegaci√≥n √≥ptimo.
- **Aprender** a usar un monopat√≠n y mantener el equilibrio en √©l, para moverse m√°s r√°pido.

[![Pedro y el Lobo](https://img.youtube.com/vi/Fmi5zHg4QSM/0.jpg)](https://www.youtube.com/watch?v=Fmi5zHg4QSM)

> üé• Haz clic en la imagen de arriba para escuchar Pedro y el Lobo de Prokofiev

## Aprendizaje por refuerzo

En secciones anteriores, has visto dos ejemplos de problemas de aprendizaje autom√°tico:

- **Supervisado**, donde tenemos conjuntos de datos que sugieren soluciones de muestra para el problema que queremos resolver. [Clasificaci√≥n](../4-Classification/README.md) y [regresi√≥n](../2-Regression/README.md) son tareas de aprendizaje supervisado.
- **No supervisado**, en el que no tenemos datos de entrenamiento etiquetados. El principal ejemplo de aprendizaje no supervisado es [Agrupamiento](../5-Clustering/README.md).

En esta secci√≥n, te presentaremos un nuevo tipo de problema de aprendizaje que no requiere datos de entrenamiento etiquetados. Hay varios tipos de problemas de este tipo:

- **[Aprendizaje semisupervisado](https://es.wikipedia.org/wiki/Aprendizaje_semisupervisado)**, donde tenemos una gran cantidad de datos no etiquetados que pueden usarse para preentrenar el modelo.
- **[Aprendizaje por refuerzo](https://es.wikipedia.org/wiki/Aprendizaje_por_refuerzo)**, en el que un agente aprende c√≥mo comportarse realizando experimentos en alg√∫n entorno simulado.

### Ejemplo - videojuego

Supongamos que quieres ense√±ar a una computadora a jugar un juego, como ajedrez o [Super Mario](https://es.wikipedia.org/wiki/Super_Mario). Para que la computadora juegue, necesitamos que prediga qu√© movimiento realizar en cada estado del juego. Aunque esto pueda parecer un problema de clasificaci√≥n, no lo es, porque no tenemos un conjunto de datos con estados y acciones correspondientes. Aunque podr√≠amos tener algunos datos como partidas de ajedrez existentes o grabaciones de jugadores jugando Super Mario, es probable que esos datos no cubran suficientemente una gran cantidad de estados posibles.

En lugar de buscar datos existentes del juego, el **Aprendizaje por Refuerzo** (RL) se basa en la idea de *hacer que la computadora juegue* muchas veces y observar el resultado. Por lo tanto, para aplicar el Aprendizaje por Refuerzo, necesitamos dos cosas:

- **Un entorno** y **un simulador** que nos permitan jugar muchas veces. Este simulador definir√≠a todas las reglas del juego, as√≠ como los posibles estados y acciones.

- **Una funci√≥n de recompensa**, que nos indique qu√© tan bien lo hicimos durante cada movimiento o partida.

La principal diferencia entre otros tipos de aprendizaje autom√°tico y RL es que en RL t√≠picamente no sabemos si ganamos o perdemos hasta que terminamos el juego. Por lo tanto, no podemos decir si un movimiento en particular es bueno o no: solo recibimos una recompensa al final del juego. Y nuestro objetivo es dise√±ar algoritmos que nos permitan entrenar un modelo bajo condiciones inciertas. Aprenderemos sobre un algoritmo de RL llamado **Q-learning**.

## Lecciones

1. [Introducci√≥n al aprendizaje por refuerzo y Q-Learning](1-QLearning/README.md)
2. [Uso de un entorno de simulaci√≥n gym](2-Gym/README.md)

## Cr√©ditos

"Introducci√≥n al Aprendizaje por Refuerzo" fue escrito con ‚ô•Ô∏è por [Dmitry Soshnikov](http://soshnikov.com)

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por lograr precisi√≥n, tenga en cuenta que las traducciones autom√°ticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err√≥neas que puedan surgir del uso de esta traducci√≥n.

---

<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "911efd5e595089000cb3c16fce1beab8",
  "translation_date": "2025-09-04T22:25:29+00:00",
  "source_file": "8-Reinforcement/1-QLearning/README.md",
  "language_code": "es"
}
-->
# Introducci√≥n al Aprendizaje por Refuerzo y Q-Learning

![Resumen del refuerzo en el aprendizaje autom√°tico en un sketchnote](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/sketchnotes/ml-reinforcement.png)
> Sketchnote por [Tomomi Imura](https://www.twitter.com/girlie_mac)

El aprendizaje por refuerzo implica tres conceptos importantes: el agente, algunos estados y un conjunto de acciones por estado. Al ejecutar una acci√≥n en un estado espec√≠fico, el agente recibe una recompensa. Imagina nuevamente el videojuego Super Mario. T√∫ eres Mario, est√°s en un nivel del juego, parado junto al borde de un acantilado. Sobre ti hay una moneda. T√∫, siendo Mario, en un nivel del juego, en una posici√≥n espec√≠fica... ese es tu estado. Moverte un paso hacia la derecha (una acci√≥n) te llevar√° al borde y te dar√° una puntuaci√≥n num√©rica baja. Sin embargo, presionar el bot√≥n de salto te permitir√° ganar un punto y seguir vivo. Ese es un resultado positivo y deber√≠a otorgarte una puntuaci√≥n num√©rica positiva.

Usando aprendizaje por refuerzo y un simulador (el juego), puedes aprender a jugar para maximizar la recompensa, que es permanecer vivo y obtener la mayor cantidad de puntos posible.

[![Introducci√≥n al Aprendizaje por Refuerzo](https://img.youtube.com/vi/lDq_en8RNOo/0.jpg)](https://www.youtube.com/watch?v=lDq_en8RNOo)

> üé• Haz clic en la imagen de arriba para escuchar a Dmitry hablar sobre el Aprendizaje por Refuerzo

## [Cuestionario previo a la lecci√≥n](https://ff-quizzes.netlify.app/en/ml/)

## Prerrequisitos y Configuraci√≥n

En esta lecci√≥n, experimentaremos con algo de c√≥digo en Python. Deber√≠as poder ejecutar el c√≥digo del Jupyter Notebook de esta lecci√≥n, ya sea en tu computadora o en la nube.

Puedes abrir [el notebook de la lecci√≥n](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/notebook.ipynb) y seguir esta lecci√≥n para construir.

> **Nota:** Si est√°s abriendo este c√≥digo desde la nube, tambi√©n necesitas obtener el archivo [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), que se utiliza en el c√≥digo del notebook. Agr√©galo al mismo directorio que el notebook.

## Introducci√≥n

En esta lecci√≥n, exploraremos el mundo de **[Pedro y el Lobo](https://en.wikipedia.org/wiki/Peter_and_the_Wolf)**, inspirado en un cuento musical de hadas de un compositor ruso, [Sergei Prokofiev](https://en.wikipedia.org/wiki/Sergei_Prokofiev). Usaremos **Aprendizaje por Refuerzo** para permitir que Pedro explore su entorno, recoja manzanas deliciosas y evite encontrarse con el lobo.

El **Aprendizaje por Refuerzo** (RL) es una t√©cnica de aprendizaje que nos permite aprender un comportamiento √≥ptimo de un **agente** en alg√∫n **entorno** realizando muchos experimentos. Un agente en este entorno debe tener alg√∫n **objetivo**, definido por una **funci√≥n de recompensa**.

## El entorno

Para simplificar, consideremos el mundo de Pedro como un tablero cuadrado de tama√±o `ancho` x `alto`, como este:

![Entorno de Pedro](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/8-Reinforcement/1-QLearning/images/environment.png)

Cada celda en este tablero puede ser:

* **suelo**, sobre el cual Pedro y otras criaturas pueden caminar.
* **agua**, sobre la cual obviamente no puedes caminar.
* un **√°rbol** o **hierba**, un lugar donde puedes descansar.
* una **manzana**, que representa algo que Pedro estar√≠a encantado de encontrar para alimentarse.
* un **lobo**, que es peligroso y debe evitarse.

Hay un m√≥dulo de Python separado, [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), que contiene el c√≥digo para trabajar con este entorno. Dado que este c√≥digo no es importante para entender nuestros conceptos, importaremos el m√≥dulo y lo usaremos para crear el tablero de muestra (bloque de c√≥digo 1):

```python
from rlboard import *

width, height = 8,8
m = Board(width,height)
m.randomize(seed=13)
m.plot()
```

Este c√≥digo deber√≠a imprimir una imagen del entorno similar a la anterior.

## Acciones y pol√≠tica

En nuestro ejemplo, el objetivo de Pedro ser√≠a encontrar una manzana, mientras evita al lobo y otros obst√°culos. Para lograr esto, esencialmente puede caminar por el tablero hasta encontrar una manzana.

Por lo tanto, en cualquier posici√≥n, puede elegir entre una de las siguientes acciones: arriba, abajo, izquierda y derecha.

Definiremos esas acciones como un diccionario y las mapearemos a pares de cambios de coordenadas correspondientes. Por ejemplo, moverse a la derecha (`R`) corresponder√≠a a un par `(1,0)`. (bloque de c√≥digo 2):

```python
actions = { "U" : (0,-1), "D" : (0,1), "L" : (-1,0), "R" : (1,0) }
action_idx = { a : i for i,a in enumerate(actions.keys()) }
```

En resumen, la estrategia y el objetivo de este escenario son los siguientes:

- **La estrategia** de nuestro agente (Pedro) est√° definida por una llamada **pol√≠tica**. Una pol√≠tica es una funci√≥n que devuelve la acci√≥n en cualquier estado dado. En nuestro caso, el estado del problema est√° representado por el tablero, incluida la posici√≥n actual del jugador.

- **El objetivo** del aprendizaje por refuerzo es eventualmente aprender una buena pol√≠tica que nos permita resolver el problema de manera eficiente. Sin embargo, como l√≠nea base, consideremos la pol√≠tica m√°s simple llamada **camino aleatorio**.

## Camino aleatorio

Primero resolvamos nuestro problema implementando una estrategia de camino aleatorio. Con el camino aleatorio, elegiremos aleatoriamente la siguiente acci√≥n de las acciones permitidas, hasta que lleguemos a la manzana (bloque de c√≥digo 3).

1. Implementa el camino aleatorio con el siguiente c√≥digo:

    ```python
    def random_policy(m):
        return random.choice(list(actions))
    
    def walk(m,policy,start_position=None):
        n = 0 # number of steps
        # set initial position
        if start_position:
            m.human = start_position 
        else:
            m.random_start()
        while True:
            if m.at() == Board.Cell.apple:
                return n # success!
            if m.at() in [Board.Cell.wolf, Board.Cell.water]:
                return -1 # eaten by wolf or drowned
            while True:
                a = actions[policy(m)]
                new_pos = m.move_pos(m.human,a)
                if m.is_valid(new_pos) and m.at(new_pos)!=Board.Cell.water:
                    m.move(a) # do the actual move
                    break
            n+=1
    
    walk(m,random_policy)
    ```

    La llamada a `walk` deber√≠a devolver la longitud del camino correspondiente, que puede variar de una ejecuci√≥n a otra.

1. Ejecuta el experimento de camino varias veces (digamos, 100) y muestra las estad√≠sticas resultantes (bloque de c√≥digo 4):

    ```python
    def print_statistics(policy):
        s,w,n = 0,0,0
        for _ in range(100):
            z = walk(m,policy)
            if z<0:
                w+=1
            else:
                s += z
                n += 1
        print(f"Average path length = {s/n}, eaten by wolf: {w} times")
    
    print_statistics(random_policy)
    ```

    Nota que la longitud promedio de un camino es de alrededor de 30-40 pasos, lo cual es bastante, dado que la distancia promedio a la manzana m√°s cercana es de alrededor de 5-6 pasos.

    Tambi√©n puedes ver c√≥mo se mueve Pedro durante el camino aleatorio:

    ![Camino aleatorio de Pedro](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/8-Reinforcement/1-QLearning/images/random_walk.gif)

## Funci√≥n de recompensa

Para hacer nuestra pol√≠tica m√°s inteligente, necesitamos entender qu√© movimientos son "mejores" que otros. Para hacer esto, necesitamos definir nuestro objetivo.

El objetivo puede definirse en t√©rminos de una **funci√≥n de recompensa**, que devolver√° alg√∫n valor de puntuaci√≥n para cada estado. Cuanto mayor sea el n√∫mero, mejor ser√° la funci√≥n de recompensa. (bloque de c√≥digo 5)

```python
move_reward = -0.1
goal_reward = 10
end_reward = -10

def reward(m,pos=None):
    pos = pos or m.human
    if not m.is_valid(pos):
        return end_reward
    x = m.at(pos)
    if x==Board.Cell.water or x == Board.Cell.wolf:
        return end_reward
    if x==Board.Cell.apple:
        return goal_reward
    return move_reward
```

Lo interesante de las funciones de recompensa es que en la mayor√≠a de los casos, *solo se nos da una recompensa sustancial al final del juego*. Esto significa que nuestro algoritmo deber√≠a recordar los pasos "buenos" que conducen a una recompensa positiva al final y aumentar su importancia. De manera similar, todos los movimientos que conducen a malos resultados deber√≠an desalentarse.

## Q-Learning

El algoritmo que discutiremos aqu√≠ se llama **Q-Learning**. En este algoritmo, la pol√≠tica est√° definida por una funci√≥n (o una estructura de datos) llamada **Q-Table**. Registra la "bondad" de cada una de las acciones en un estado dado.

Se llama Q-Table porque a menudo es conveniente representarla como una tabla o un arreglo multidimensional. Dado que nuestro tablero tiene dimensiones `ancho` x `alto`, podemos representar la Q-Table usando un arreglo numpy con forma `ancho` x `alto` x `len(actions)`: (bloque de c√≥digo 6)

```python
Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)
```

Nota que inicializamos todos los valores de la Q-Table con un valor igual, en nuestro caso - 0.25. Esto corresponde a la pol√≠tica de "camino aleatorio", porque todos los movimientos en cada estado son igualmente buenos. Podemos pasar la Q-Table a la funci√≥n `plot` para visualizar la tabla en el tablero: `m.plot(Q)`.

![Entorno de Pedro](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/8-Reinforcement/1-QLearning/images/env_init.png)

En el centro de cada celda hay una "flecha" que indica la direcci√≥n preferida de movimiento. Dado que todas las direcciones son iguales, se muestra un punto.

Ahora necesitamos ejecutar la simulaci√≥n, explorar nuestro entorno y aprender una mejor distribuci√≥n de valores de la Q-Table, lo que nos permitir√° encontrar el camino hacia la manzana mucho m√°s r√°pido.

## Esencia del Q-Learning: Ecuaci√≥n de Bellman

Una vez que comenzamos a movernos, cada acci√≥n tendr√° una recompensa correspondiente, es decir, te√≥ricamente podemos seleccionar la siguiente acci√≥n basada en la recompensa inmediata m√°s alta. Sin embargo, en la mayor√≠a de los estados, el movimiento no lograr√° nuestro objetivo de alcanzar la manzana, y por lo tanto no podemos decidir inmediatamente qu√© direcci√≥n es mejor.

> Recuerda que no importa el resultado inmediato, sino el resultado final, que obtendremos al final de la simulaci√≥n.

Para tener en cuenta esta recompensa diferida, necesitamos usar los principios de la **[programaci√≥n din√°mica](https://en.wikipedia.org/wiki/Dynamic_programming)**, que nos permiten pensar en nuestro problema de manera recursiva.

Supongamos que ahora estamos en el estado *s*, y queremos movernos al siguiente estado *s'*. Al hacerlo, recibiremos la recompensa inmediata *r(s,a)*, definida por la funci√≥n de recompensa, m√°s alguna recompensa futura. Si suponemos que nuestra Q-Table refleja correctamente la "atractividad" de cada acci√≥n, entonces en el estado *s'* elegiremos una acci√≥n *a'* que corresponda al valor m√°ximo de *Q(s',a')*. As√≠, la mejor recompensa futura posible que podr√≠amos obtener en el estado *s* se definir√° como `max`

## Comprobando la pol√≠tica

Dado que la Q-Table enumera la "atractividad" de cada acci√≥n en cada estado, es bastante f√°cil usarla para definir la navegaci√≥n eficiente en nuestro mundo. En el caso m√°s simple, podemos seleccionar la acci√≥n correspondiente al valor m√°s alto de la Q-Table: (bloque de c√≥digo 9)

```python
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

> Si pruebas el c√≥digo anterior varias veces, puede que notes que a veces se "queda colgado" y necesitas presionar el bot√≥n STOP en el notebook para interrumpirlo. Esto ocurre porque podr√≠a haber situaciones en las que dos estados "apuntan" el uno al otro en t√©rminos de valor √≥ptimo de Q-Value, en cuyo caso el agente termina movi√©ndose entre esos estados indefinidamente.

## üöÄDesaf√≠o

> **Tarea 1:** Modifica la funci√≥n `walk` para limitar la longitud m√°xima del camino a un cierto n√∫mero de pasos (por ejemplo, 100), y observa c√≥mo el c√≥digo anterior devuelve este valor de vez en cuando.

> **Tarea 2:** Modifica la funci√≥n `walk` para que no regrese a los lugares donde ya ha estado previamente. Esto evitar√° que `walk` entre en bucles, sin embargo, el agente a√∫n puede terminar "atrapado" en una ubicaci√≥n de la que no puede escapar.

## Navegaci√≥n

Una pol√≠tica de navegaci√≥n mejor ser√≠a la que usamos durante el entrenamiento, que combina explotaci√≥n y exploraci√≥n. En esta pol√≠tica, seleccionaremos cada acci√≥n con cierta probabilidad, proporcional a los valores en la Q-Table. Esta estrategia a√∫n puede hacer que el agente regrese a una posici√≥n que ya ha explorado, pero, como puedes ver en el c√≥digo a continuaci√≥n, resulta en un camino promedio muy corto hacia la ubicaci√≥n deseada (recuerda que `print_statistics` ejecuta la simulaci√≥n 100 veces): (bloque de c√≥digo 10)

```python
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

Despu√©s de ejecutar este c√≥digo, deber√≠as obtener una longitud promedio de camino mucho m√°s peque√±a que antes, en el rango de 3-6.

## Investigando el proceso de aprendizaje

Como hemos mencionado, el proceso de aprendizaje es un equilibrio entre la exploraci√≥n y la explotaci√≥n del conocimiento adquirido sobre la estructura del espacio del problema. Hemos visto que los resultados del aprendizaje (la capacidad de ayudar a un agente a encontrar un camino corto hacia el objetivo) han mejorado, pero tambi√©n es interesante observar c√≥mo se comporta la longitud promedio del camino durante el proceso de aprendizaje:

## Los aprendizajes pueden resumirse como:

- **La longitud promedio del camino aumenta**. Lo que vemos aqu√≠ es que al principio, la longitud promedio del camino aumenta. Esto probablemente se debe al hecho de que cuando no sabemos nada sobre el entorno, es probable que quedemos atrapados en estados desfavorables, como agua o lobos. A medida que aprendemos m√°s y comenzamos a usar este conocimiento, podemos explorar el entorno por m√°s tiempo, pero a√∫n no sabemos muy bien d√≥nde est√°n las manzanas.

- **La longitud del camino disminuye a medida que aprendemos m√°s**. Una vez que aprendemos lo suficiente, se vuelve m√°s f√°cil para el agente alcanzar el objetivo, y la longitud del camino comienza a disminuir. Sin embargo, a√∫n estamos abiertos a la exploraci√≥n, por lo que a menudo nos desviamos del mejor camino y exploramos nuevas opciones, haciendo que el camino sea m√°s largo de lo √≥ptimo.

- **La longitud aumenta abruptamente**. Lo que tambi√©n observamos en este gr√°fico es que en alg√∫n momento, la longitud aument√≥ abruptamente. Esto indica la naturaleza estoc√°stica del proceso, y que en alg√∫n momento podemos "estropear" los coeficientes de la Q-Table sobrescribi√©ndolos con nuevos valores. Esto idealmente deber√≠a minimizarse disminuyendo la tasa de aprendizaje (por ejemplo, hacia el final del entrenamiento, solo ajustamos los valores de la Q-Table por un peque√±o valor).

En general, es importante recordar que el √©xito y la calidad del proceso de aprendizaje dependen significativamente de par√°metros como la tasa de aprendizaje, la disminuci√≥n de la tasa de aprendizaje y el factor de descuento. A menudo se les llama **hiperpar√°metros**, para distinguirlos de los **par√°metros**, que optimizamos durante el entrenamiento (por ejemplo, los coeficientes de la Q-Table). El proceso de encontrar los mejores valores de hiperpar√°metros se llama **optimizaci√≥n de hiperpar√°metros**, y merece un tema aparte.

## [Cuestionario post-clase](https://ff-quizzes.netlify.app/en/ml/)

## Tarea 
[Un Mundo M√°s Realista](assignment.md)

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por garantizar la precisi√≥n, tenga en cuenta que las traducciones automatizadas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err√≥neas que puedan surgir del uso de esta traducci√≥n.

---

<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "107d5bb29da8a562e7ae72262d251a75",
  "translation_date": "2025-09-04T22:26:14+00:00",
  "source_file": "8-Reinforcement/2-Gym/README.md",
  "language_code": "es"
}
-->
# CartPole Patinaje

El problema que resolvimos en la lecci√≥n anterior puede parecer un problema de juguete, sin mucha aplicaci√≥n en escenarios de la vida real. Sin embargo, este no es el caso, ya que muchos problemas del mundo real comparten caracter√≠sticas similares, como jugar al ajedrez o al Go. Son similares porque tambi√©n tenemos un tablero con reglas definidas y un **estado discreto**.

## [Cuestionario previo a la lecci√≥n](https://ff-quizzes.netlify.app/en/ml/)

## Introducci√≥n

En esta lecci√≥n aplicaremos los mismos principios de Q-Learning a un problema con un **estado continuo**, es decir, un estado definido por uno o m√°s n√∫meros reales. Abordaremos el siguiente problema:

> **Problema**: Si Pedro quiere escapar del lobo, necesita aprender a moverse m√°s r√°pido. Veremos c√≥mo Pedro puede aprender a patinar, en particular, a mantener el equilibrio, utilizando Q-Learning.

![¬°La gran escapada!](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/8-Reinforcement/2-Gym/images/escape.png)

> ¬°Pedro y sus amigos se ponen creativos para escapar del lobo! Imagen de [Jen Looper](https://twitter.com/jenlooper)

Usaremos una versi√≥n simplificada del equilibrio conocida como el problema del **CartPole**. En el mundo del CartPole, tenemos un deslizador horizontal que puede moverse a la izquierda o a la derecha, y el objetivo es equilibrar un palo vertical sobre el deslizador.

## Prerrequisitos

En esta lecci√≥n, utilizaremos una biblioteca llamada **OpenAI Gym** para simular diferentes **entornos**. Puedes ejecutar el c√≥digo de esta lecci√≥n localmente (por ejemplo, desde Visual Studio Code), en cuyo caso la simulaci√≥n se abrir√° en una nueva ventana. Si ejecutas el c√≥digo en l√≠nea, es posible que necesites hacer algunos ajustes, como se describe [aqu√≠](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).

## OpenAI Gym

En la lecci√≥n anterior, las reglas del juego y el estado estaban definidos por la clase `Board` que creamos nosotros mismos. Aqu√≠ utilizaremos un **entorno de simulaci√≥n** especial, que simular√° la f√≠sica detr√°s del palo en equilibrio. Uno de los entornos de simulaci√≥n m√°s populares para entrenar algoritmos de aprendizaje por refuerzo se llama [Gym](https://gym.openai.com/), mantenido por [OpenAI](https://openai.com/). Usando este Gym, podemos crear diferentes **entornos**, desde una simulaci√≥n de CartPole hasta juegos de Atari.

> **Nota**: Puedes ver otros entornos disponibles en OpenAI Gym [aqu√≠](https://gym.openai.com/envs/#classic_control).

Primero, instalemos Gym e importemos las bibliotecas necesarias (bloque de c√≥digo 1):

```python
import sys
!{sys.executable} -m pip install gym 

import gym
import matplotlib.pyplot as plt
import numpy as np
import random
```

## Ejercicio - inicializar un entorno de CartPole

Para trabajar con el problema de equilibrio de CartPole, necesitamos inicializar el entorno correspondiente. Cada entorno est√° asociado con:

- Un **espacio de observaci√≥n** que define la estructura de la informaci√≥n que recibimos del entorno. Para el problema de CartPole, recibimos la posici√≥n del palo, la velocidad y otros valores.

- Un **espacio de acci√≥n** que define las acciones posibles. En nuestro caso, el espacio de acci√≥n es discreto y consta de dos acciones: **izquierda** y **derecha**. (bloque de c√≥digo 2)

1. Para inicializar, escribe el siguiente c√≥digo:

    ```python
    env = gym.make("CartPole-v1")
    print(env.action_space)
    print(env.observation_space)
    print(env.action_space.sample())
    ```

Para ver c√≥mo funciona el entorno, ejecutemos una simulaci√≥n corta de 100 pasos. En cada paso, proporcionamos una de las acciones a realizar; en esta simulaci√≥n simplemente seleccionamos una acci√≥n aleatoriamente del `action_space`.

1. Ejecuta el siguiente c√≥digo y observa el resultado.

    ‚úÖ Recuerda que es preferible ejecutar este c√≥digo en una instalaci√≥n local de Python. (bloque de c√≥digo 3)

    ```python
    env.reset()
    
    for i in range(100):
       env.render()
       env.step(env.action_space.sample())
    env.close()
    ```

    Deber√≠as ver algo similar a esta imagen:

    ![CartPole sin equilibrio](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/8-Reinforcement/2-Gym/images/cartpole-nobalance.gif)

1. Durante la simulaci√≥n, necesitamos obtener observaciones para decidir c√≥mo actuar. De hecho, la funci√≥n `step` devuelve las observaciones actuales, una funci√≥n de recompensa y una bandera `done` que indica si tiene sentido continuar la simulaci√≥n o no: (bloque de c√≥digo 4)

    ```python
    env.reset()
    
    done = False
    while not done:
       env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       print(f"{obs} -> {rew}")
    env.close()
    ```

    Ver√°s algo como esto en la salida del notebook:

    ```text
    [ 0.03403272 -0.24301182  0.02669811  0.2895829 ] -> 1.0
    [ 0.02917248 -0.04828055  0.03248977  0.00543839] -> 1.0
    [ 0.02820687  0.14636075  0.03259854 -0.27681916] -> 1.0
    [ 0.03113408  0.34100283  0.02706215 -0.55904489] -> 1.0
    [ 0.03795414  0.53573468  0.01588125 -0.84308041] -> 1.0
    ...
    [ 0.17299878  0.15868546 -0.20754175 -0.55975453] -> 1.0
    [ 0.17617249  0.35602306 -0.21873684 -0.90998894] -> 1.0
    ```

    El vector de observaci√≥n que se devuelve en cada paso de la simulaci√≥n contiene los siguientes valores:
    - Posici√≥n del carrito
    - Velocidad del carrito
    - √Ångulo del palo
    - Tasa de rotaci√≥n del palo

1. Obt√©n el valor m√≠nimo y m√°ximo de esos n√∫meros: (bloque de c√≥digo 5)

    ```python
    print(env.observation_space.low)
    print(env.observation_space.high)
    ```

    Tambi√©n notar√°s que el valor de recompensa en cada paso de la simulaci√≥n siempre es 1. Esto se debe a que nuestro objetivo es sobrevivir el mayor tiempo posible, es decir, mantener el palo en una posici√≥n razonablemente vertical durante el mayor tiempo posible.

    ‚úÖ De hecho, la simulaci√≥n de CartPole se considera resuelta si logramos obtener una recompensa promedio de 195 en 100 ensayos consecutivos.

## Discretizaci√≥n del estado

En Q-Learning, necesitamos construir una Q-Table que defina qu√© hacer en cada estado. Para poder hacer esto, el estado debe ser **discreto**, m√°s precisamente, debe contener un n√∫mero finito de valores discretos. Por lo tanto, necesitamos de alguna manera **discretizar** nuestras observaciones, mape√°ndolas a un conjunto finito de estados.

Hay algunas formas de hacer esto:

- **Dividir en intervalos**. Si conocemos el intervalo de un valor determinado, podemos dividir este intervalo en un n√∫mero de **intervalos**, y luego reemplazar el valor por el n√∫mero del intervalo al que pertenece. Esto se puede hacer utilizando el m√©todo [`digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html) de numpy. En este caso, conoceremos con precisi√≥n el tama√±o del estado, ya que depender√° del n√∫mero de intervalos que seleccionemos para la digitalizaci√≥n.

‚úÖ Podemos usar interpolaci√≥n lineal para llevar los valores a un intervalo finito (por ejemplo, de -20 a 20), y luego convertir los n√∫meros a enteros redonde√°ndolos. Esto nos da un poco menos de control sobre el tama√±o del estado, especialmente si no conocemos los rangos exactos de los valores de entrada. Por ejemplo, en nuestro caso, 2 de los 4 valores no tienen l√≠mites superiores/inferiores, lo que puede resultar en un n√∫mero infinito de estados.

En nuestro ejemplo, utilizaremos el segundo enfoque. Como notar√°s m√°s adelante, a pesar de los l√≠mites superiores/inferiores indefinidos, esos valores rara vez toman valores fuera de ciertos intervalos finitos, por lo que esos estados con valores extremos ser√°n muy raros.

1. Aqu√≠ est√° la funci√≥n que tomar√° la observaci√≥n de nuestro modelo y producir√° una tupla de 4 valores enteros: (bloque de c√≥digo 6)

    ```python
    def discretize(x):
        return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
    ```

1. Exploremos tambi√©n otro m√©todo de discretizaci√≥n utilizando intervalos: (bloque de c√≥digo 7)

    ```python
    def create_bins(i,num):
        return np.arange(num+1)*(i[1]-i[0])/num+i[0]
    
    print("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))
    
    ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter
    nbins = [20,20,10,10] # number of bins for each parameter
    bins = [create_bins(ints[i],nbins[i]) for i in range(4)]
    
    def discretize_bins(x):
        return tuple(np.digitize(x[i],bins[i]) for i in range(4))
    ```

1. Ahora ejecutemos una simulaci√≥n corta y observemos esos valores discretos del entorno. Si√©ntete libre de probar tanto `discretize` como `discretize_bins` y observa si hay alguna diferencia.

    ‚úÖ `discretize_bins` devuelve el n√∫mero del intervalo, que comienza en 0. Por lo tanto, para valores de la variable de entrada cercanos a 0, devuelve el n√∫mero del medio del intervalo (10). En `discretize`, no nos preocupamos por el rango de los valores de salida, permitiendo que sean negativos, por lo que los valores del estado no est√°n desplazados, y 0 corresponde a 0. (bloque de c√≥digo 8)

    ```python
    env.reset()
    
    done = False
    while not done:
       #env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       #print(discretize_bins(obs))
       print(discretize(obs))
    env.close()
    ```

    ‚úÖ Descomenta la l√≠nea que comienza con `env.render` si deseas ver c√≥mo se ejecuta el entorno. De lo contrario, puedes ejecutarlo en segundo plano, lo cual es m√°s r√°pido. Usaremos esta ejecuci√≥n "invisible" durante nuestro proceso de Q-Learning.

## La estructura de la Q-Table

En nuestra lecci√≥n anterior, el estado era un simple par de n√∫meros del 0 al 8, por lo que era conveniente representar la Q-Table con un tensor de numpy con una forma de 8x8x2. Si usamos la discretizaci√≥n por intervalos, el tama√±o de nuestro vector de estado tambi√©n es conocido, por lo que podemos usar el mismo enfoque y representar el estado con un array de forma 20x20x10x10x2 (aqu√≠ 2 es la dimensi√≥n del espacio de acci√≥n, y las primeras dimensiones corresponden al n√∫mero de intervalos que seleccionamos para cada uno de los par√°metros en el espacio de observaci√≥n).

Sin embargo, a veces las dimensiones precisas del espacio de observaci√≥n no son conocidas. En el caso de la funci√≥n `discretize`, nunca podemos estar seguros de que nuestro estado se mantenga dentro de ciertos l√≠mites, ya que algunos de los valores originales no est√°n acotados. Por lo tanto, utilizaremos un enfoque ligeramente diferente y representaremos la Q-Table con un diccionario.

1. Usa el par *(estado, acci√≥n)* como clave del diccionario, y el valor corresponder√° al valor de la entrada en la Q-Table. (bloque de c√≥digo 9)

    ```python
    Q = {}
    actions = (0,1)
    
    def qvalues(state):
        return [Q.get((state,a),0) for a in actions]
    ```

    Aqu√≠ tambi√©n definimos una funci√≥n `qvalues()`, que devuelve una lista de valores de la Q-Table para un estado dado que corresponde a todas las acciones posibles. Si la entrada no est√° presente en la Q-Table, devolveremos 0 como valor predeterminado.

## ¬°Comencemos con Q-Learning!

Ahora estamos listos para ense√±ar a Pedro a mantener el equilibrio.

1. Primero, definamos algunos hiperpar√°metros: (bloque de c√≥digo 10)

    ```python
    # hyperparameters
    alpha = 0.3
    gamma = 0.9
    epsilon = 0.90
    ```

    Aqu√≠, `alpha` es la **tasa de aprendizaje** que define en qu√© medida debemos ajustar los valores actuales de la Q-Table en cada paso. En la lecci√≥n anterior comenzamos con 1 y luego disminuimos `alpha` a valores m√°s bajos durante el entrenamiento. En este ejemplo lo mantendremos constante por simplicidad, pero puedes experimentar ajustando los valores de `alpha` m√°s adelante.

    `gamma` es el **factor de descuento** que muestra en qu√© medida debemos priorizar la recompensa futura sobre la recompensa actual.

    `epsilon` es el **factor de exploraci√≥n/explotaci√≥n** que determina si debemos preferir la exploraci√≥n o la explotaci√≥n. En nuestro algoritmo, en un porcentaje `epsilon` de los casos seleccionaremos la siguiente acci√≥n seg√∫n los valores de la Q-Table, y en el porcentaje restante ejecutaremos una acci√≥n aleatoria. Esto nos permitir√° explorar √°reas del espacio de b√∫squeda que nunca hemos visto antes.

    ‚úÖ En t√©rminos de equilibrio, elegir una acci√≥n aleatoria (exploraci√≥n) actuar√≠a como un empuj√≥n aleatorio en la direcci√≥n equivocada, y el palo tendr√≠a que aprender a recuperar el equilibrio de esos "errores".

### Mejorar el algoritmo

Podemos hacer dos mejoras a nuestro algoritmo de la lecci√≥n anterior:

- **Calcular la recompensa acumulativa promedio**, durante un n√∫mero de simulaciones. Imprimiremos el progreso cada 5000 iteraciones, y promediaremos nuestra recompensa acumulativa durante ese per√≠odo de tiempo. Esto significa que si obtenemos m√°s de 195 puntos, podemos considerar el problema resuelto, con una calidad incluso mayor a la requerida.

- **Calcular el resultado acumulativo promedio m√°ximo**, `Qmax`, y almacenaremos la Q-Table correspondiente a ese resultado. Cuando ejecutes el entrenamiento, notar√°s que a veces el resultado acumulativo promedio comienza a disminuir, y queremos conservar los valores de la Q-Table que corresponden al mejor modelo observado durante el entrenamiento.

1. Recopila todas las recompensas acumulativas en cada simulaci√≥n en el vector `rewards` para su posterior representaci√≥n gr√°fica. (bloque de c√≥digo 11)

    ```python
    def probs(v,eps=1e-4):
        v = v-v.min()+eps
        v = v/v.sum()
        return v
    
    Qmax = 0
    cum_rewards = []
    rewards = []
    for epoch in range(100000):
        obs = env.reset()
        done = False
        cum_reward=0
        # == do the simulation ==
        while not done:
            s = discretize(obs)
            if random.random()<epsilon:
                # exploitation - chose the action according to Q-Table probabilities
                v = probs(np.array(qvalues(s)))
                a = random.choices(actions,weights=v)[0]
            else:
                # exploration - randomly chose the action
                a = np.random.randint(env.action_space.n)
    
            obs, rew, done, info = env.step(a)
            cum_reward+=rew
            ns = discretize(obs)
            Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
        cum_rewards.append(cum_reward)
        rewards.append(cum_reward)
        # == Periodically print results and calculate average reward ==
        if epoch%5000==0:
            print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
            if np.average(cum_rewards) > Qmax:
                Qmax = np.average(cum_rewards)
                Qbest = Q
            cum_rewards=[]
    ```

Lo que puedes notar de estos resultados:

- **Cerca de nuestro objetivo**. Estamos muy cerca de alcanzar el objetivo de obtener 195 recompensas acumulativas en m√°s de 100 ejecuciones consecutivas de la simulaci√≥n, ¬°o incluso podemos haberlo logrado! Incluso si obtenemos n√∫meros m√°s bajos, no lo sabremos con certeza, ya que promediamos sobre 5000 ejecuciones, y solo se requieren 100 ejecuciones seg√∫n el criterio formal.

- **La recompensa comienza a disminuir**. A veces la recompensa comienza a disminuir, lo que significa que podemos "destruir" valores ya aprendidos en la Q-Table con otros que empeoran la situaci√≥n.

Esta observaci√≥n es m√°s clara si graficamos el progreso del entrenamiento.

## Graficar el progreso del entrenamiento

Durante el entrenamiento, hemos recopilado el valor de la recompensa acumulativa en cada una de las iteraciones en el vector `rewards`. As√≠ es como se ve cuando lo graficamos contra el n√∫mero de iteraci√≥n:

```python
plt.plot(rewards)
```

![progreso sin procesar](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/8-Reinforcement/2-Gym/images/train_progress_raw.png)

En este gr√°fico no es posible sacar conclusiones, ya que debido a la naturaleza del proceso de entrenamiento estoc√°stico, la duraci√≥n de las sesiones de entrenamiento var√≠a mucho. Para darle m√°s sentido a este gr√°fico, podemos calcular el **promedio m√≥vil** sobre una serie de experimentos, digamos 100. Esto se puede hacer convenientemente usando `np.convolve`: (bloque de c√≥digo 12)

```python
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

![progreso del entrenamiento](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/8-Reinforcement/2-Gym/images/train_progress_runav.png)

## Variar los hiperpar√°metros

Para hacer el aprendizaje m√°s estable, tiene sentido ajustar algunos de nuestros hiperpar√°metros durante el entrenamiento. En particular:

- **Para la tasa de aprendizaje**, `alpha`, podemos comenzar con valores cercanos a 1 y luego ir disminuyendo el par√°metro. Con el tiempo, obtendremos buenos valores de probabilidad en la Q-Table, y por lo tanto deber√≠amos ajustarlos ligeramente, y no sobrescribirlos completamente con nuevos valores.

- **Aumentar epsilon**. Podr√≠amos querer aumentar lentamente el valor de `epsilon`, para explorar menos y explotar m√°s. Probablemente tenga sentido comenzar con un valor bajo de `epsilon` y aumentarlo hasta casi 1.
> **Tarea 1**: Prueba con diferentes valores de hiperpar√°metros y observa si puedes lograr una recompensa acumulativa m√°s alta. ¬øEst√°s obteniendo m√°s de 195?
> **Tarea 2**: Para resolver formalmente el problema, necesitas alcanzar un promedio de recompensa de 195 a lo largo de 100 ejecuciones consecutivas. Mide eso durante el entrenamiento y aseg√∫rate de que has resuelto el problema formalmente.

## Ver el resultado en acci√≥n

Ser√≠a interesante ver c√≥mo se comporta el modelo entrenado. Vamos a ejecutar la simulaci√≥n y seguir la misma estrategia de selecci√≥n de acciones que durante el entrenamiento, muestreando seg√∫n la distribuci√≥n de probabilidad en la Q-Table: (bloque de c√≥digo 13)

```python
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

Deber√≠as ver algo como esto:

![un carrito equilibrando](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/8-Reinforcement/2-Gym/images/cartpole-balance.gif)

---

## üöÄDesaf√≠o

> **Tarea 3**: Aqu√≠ estuvimos utilizando la copia final de la Q-Table, que puede no ser la mejor. Recuerda que hemos almacenado la Q-Table con mejor rendimiento en la variable `Qbest`. ¬°Prueba el mismo ejemplo con la Q-Table de mejor rendimiento copiando `Qbest` sobre `Q` y observa si notas alguna diferencia!

> **Tarea 4**: Aqu√≠ no est√°bamos seleccionando la mejor acci√≥n en cada paso, sino muestreando con la distribuci√≥n de probabilidad correspondiente. ¬øTendr√≠a m√°s sentido seleccionar siempre la mejor acci√≥n, con el valor m√°s alto en la Q-Table? Esto se puede hacer utilizando la funci√≥n `np.argmax` para encontrar el n√∫mero de acci√≥n correspondiente al valor m√°s alto en la Q-Table. Implementa esta estrategia y observa si mejora el equilibrio.

## [Cuestionario post-clase](https://ff-quizzes.netlify.app/en/ml/)

## Asignaci√≥n
[Entrena un Mountain Car](assignment.md)

## Conclusi√≥n

Ahora hemos aprendido c√≥mo entrenar agentes para lograr buenos resultados simplemente proporcionando una funci√≥n de recompensa que define el estado deseado del juego y d√°ndoles la oportunidad de explorar inteligentemente el espacio de b√∫squeda. Hemos aplicado con √©xito el algoritmo de Q-Learning en casos de entornos discretos y continuos, pero con acciones discretas.

Es importante tambi√©n estudiar situaciones donde el estado de las acciones sea continuo y cuando el espacio de observaci√≥n sea mucho m√°s complejo, como la imagen de la pantalla de un juego de Atari. En esos problemas, a menudo necesitamos usar t√©cnicas de aprendizaje autom√°tico m√°s poderosas, como redes neuronales, para lograr buenos resultados. Estos temas m√°s avanzados ser√°n el enfoque de nuestro pr√≥ximo curso avanzado de IA.

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Aunque nos esforzamos por garantizar la precisi√≥n, tenga en cuenta que las traducciones automatizadas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err√≥neas que puedan surgir del uso de esta traducci√≥n.

---


# Aplicaciones del Mundo Real

<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5e069a0ac02a9606a69946c2b3c574a9",
  "translation_date": "2025-09-03T23:14:48+00:00",
  "source_file": "9-Real-World/README.md",
  "language_code": "es"
}
-->
# Posdata: Aplicaciones reales del aprendizaje autom√°tico cl√°sico

En esta secci√≥n del curr√≠culo, se te presentar√°n algunas aplicaciones reales del aprendizaje autom√°tico cl√°sico. Hemos investigado en internet para encontrar art√≠culos y documentos t√©cnicos sobre aplicaciones que han utilizado estas estrategias, evitando redes neuronales, aprendizaje profundo e inteligencia artificial tanto como sea posible. Aprende c√≥mo se utiliza el aprendizaje autom√°tico en sistemas empresariales, aplicaciones ecol√≥gicas, finanzas, arte y cultura, entre otros.

![chess](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/translated_images/es/chess.e704a268781bdad8.webp)

> Foto por <a href="https://unsplash.com/@childeye?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Alexis Fauvet</a> en <a href="https://unsplash.com/s/photos/artificial-intelligence?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Unsplash</a>
  
## Lecci√≥n

1. [Aplicaciones reales del aprendizaje autom√°tico](1-Applications/README.md)
2. [Depuraci√≥n de modelos en aprendizaje autom√°tico usando componentes del panel de Responsible AI](2-Debugging-ML-Models/README.md)

## Cr√©ditos

"Aplicaciones reales" fue escrito por un equipo de personas, incluyendo [Jen Looper](https://twitter.com/jenlooper) y [Ornella Altunyan](https://twitter.com/ornelladotcom).

"Depuraci√≥n de modelos en aprendizaje autom√°tico usando componentes del panel de Responsible AI" fue escrito por [Ruth Yakubu](https://twitter.com/ruthieyakubu)

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por lograr precisi√≥n, tenga en cuenta que las traducciones autom√°ticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err√≥neas que puedan surgir del uso de esta traducci√≥n.

---

<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "83320d6b6994909e35d830cebf214039",
  "translation_date": "2025-09-04T22:19:09+00:00",
  "source_file": "9-Real-World/1-Applications/README.md",
  "language_code": "es"
}
-->
# Posdata: Aprendizaje autom√°tico en el mundo real

![Resumen del aprendizaje autom√°tico en el mundo real en un sketchnote](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/sketchnotes/ml-realworld.png)  
> Sketchnote por [Tomomi Imura](https://www.twitter.com/girlie_mac)

En este plan de estudios, has aprendido muchas formas de preparar datos para el entrenamiento y crear modelos de aprendizaje autom√°tico. Construiste una serie de modelos cl√°sicos de regresi√≥n, agrupamiento, clasificaci√≥n, procesamiento de lenguaje natural y series temporales. ¬°Felicidades! Ahora, podr√≠as estar pregunt√°ndote para qu√© sirve todo esto... ¬øcu√°les son las aplicaciones reales de estos modelos?

Aunque la inteligencia artificial (IA), que generalmente utiliza aprendizaje profundo, ha captado mucho inter√©s en la industria, todav√≠a hay aplicaciones valiosas para los modelos cl√°sicos de aprendizaje autom√°tico. ¬°Incluso podr√≠as estar usando algunas de estas aplicaciones hoy en d√≠a! En esta lecci√≥n, explorar√°s c√≥mo ocho industrias y dominios tem√°ticos diferentes utilizan estos tipos de modelos para hacer que sus aplicaciones sean m√°s eficientes, confiables, inteligentes y valiosas para los usuarios.

## [Cuestionario previo a la lecci√≥n](https://ff-quizzes.netlify.app/en/ml/)

## üí∞ Finanzas

El sector financiero ofrece muchas oportunidades para el aprendizaje autom√°tico. Muchos problemas en esta √°rea se prestan a ser modelados y resueltos utilizando ML.

### Detecci√≥n de fraude con tarjetas de cr√©dito

Aprendimos sobre [agrupamiento k-means](../../5-Clustering/2-K-Means/README.md) anteriormente en el curso, pero ¬øc√≥mo puede usarse para resolver problemas relacionados con el fraude con tarjetas de cr√©dito?

El agrupamiento k-means es √∫til en una t√©cnica de detecci√≥n de fraude con tarjetas de cr√©dito llamada **detecci√≥n de valores at√≠picos**. Los valores at√≠picos, o desviaciones en las observaciones de un conjunto de datos, pueden indicarnos si una tarjeta de cr√©dito se est√° utilizando de manera normal o si algo inusual est√° ocurriendo. Como se muestra en el art√≠culo enlazado a continuaci√≥n, puedes clasificar los datos de tarjetas de cr√©dito utilizando un algoritmo de agrupamiento k-means y asignar cada transacci√≥n a un grupo seg√∫n qu√© tan at√≠pica parezca ser. Luego, puedes evaluar los grupos m√°s riesgosos para determinar transacciones fraudulentas frente a leg√≠timas.  
[Referencia](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.680.1195&rep=rep1&type=pdf)

### Gesti√≥n de patrimonios

En la gesti√≥n de patrimonios, una persona o empresa maneja inversiones en nombre de sus clientes. Su trabajo es mantener y hacer crecer la riqueza a largo plazo, por lo que es esencial elegir inversiones que tengan un buen desempe√±o.

Una forma de evaluar c√≥mo se desempe√±a una inversi√≥n en particular es a trav√©s de la regresi√≥n estad√≠stica. La [regresi√≥n lineal](../../2-Regression/1-Tools/README.md) es una herramienta valiosa para entender c√≥mo se desempe√±a un fondo en relaci√≥n con un punto de referencia. Tambi√©n podemos deducir si los resultados de la regresi√≥n son estad√≠sticamente significativos o cu√°nto afectar√≠an las inversiones de un cliente. Incluso podr√≠as ampliar tu an√°lisis utilizando regresi√≥n m√∫ltiple, donde se pueden tener en cuenta factores de riesgo adicionales. Para un ejemplo de c√≥mo funcionar√≠a esto para un fondo espec√≠fico, consulta el art√≠culo a continuaci√≥n sobre la evaluaci√≥n del desempe√±o de fondos utilizando regresi√≥n.  
[Referencia](http://www.brightwoodventures.com/evaluating-fund-performance-using-regression/)

## üéì Educaci√≥n

El sector educativo tambi√©n es un √°rea muy interesante donde se puede aplicar ML. Hay problemas fascinantes por resolver, como detectar trampas en ex√°menes o ensayos, o gestionar sesgos, intencionados o no, en el proceso de correcci√≥n.

### Predicci√≥n del comportamiento estudiantil

[Coursera](https://coursera.com), un proveedor de cursos abiertos en l√≠nea, tiene un excelente blog t√©cnico donde discuten muchas decisiones de ingenier√≠a. En este estudio de caso, trazaron una l√≠nea de regresi√≥n para explorar cualquier correlaci√≥n entre una baja calificaci√≥n de NPS (Net Promoter Score) y la retenci√≥n o abandono de cursos.  
[Referencia](https://medium.com/coursera-engineering/controlled-regression-quantifying-the-impact-of-course-quality-on-learner-retention-31f956bd592a)

### Mitigaci√≥n de sesgos

[Grammarly](https://grammarly.com), un asistente de escritura que revisa errores ortogr√°ficos y gramaticales, utiliza sofisticados [sistemas de procesamiento de lenguaje natural](../../6-NLP/README.md) en todos sus productos. Publicaron un interesante estudio de caso en su blog t√©cnico sobre c√≥mo abordaron el sesgo de g√©nero en el aprendizaje autom√°tico, algo que aprendiste en nuestra [lecci√≥n introductoria sobre equidad](../../1-Introduction/3-fairness/README.md).  
[Referencia](https://www.grammarly.com/blog/engineering/mitigating-gender-bias-in-autocorrect/)

## üëú Comercio minorista

El sector minorista puede beneficiarse enormemente del uso de ML, desde crear una mejor experiencia para el cliente hasta optimizar el inventario.

### Personalizaci√≥n del recorrido del cliente

En Wayfair, una empresa que vende art√≠culos para el hogar como muebles, ayudar a los clientes a encontrar los productos adecuados para sus gustos y necesidades es fundamental. En este art√≠culo, los ingenieros de la empresa describen c√≥mo utilizan ML y NLP para "mostrar los resultados correctos a los clientes". En particular, su motor de intenci√≥n de consulta se ha construido para usar extracci√≥n de entidades, entrenamiento de clasificadores, extracci√≥n de opiniones y etiquetado de sentimientos en las rese√±as de los clientes. Este es un caso cl√°sico de c√≥mo funciona el NLP en el comercio minorista en l√≠nea.  
[Referencia](https://www.aboutwayfair.com/tech-innovation/how-we-use-machine-learning-and-natural-language-processing-to-empower-search)

### Gesti√≥n de inventarios

Empresas innovadoras y √°giles como [StitchFix](https://stitchfix.com), un servicio de cajas que env√≠a ropa a los consumidores, dependen en gran medida de ML para recomendaciones y gesti√≥n de inventarios. Sus equipos de estilismo trabajan junto con sus equipos de comercializaci√≥n: "uno de nuestros cient√≠ficos de datos experiment√≥ con un algoritmo gen√©tico y lo aplic√≥ a prendas para predecir qu√© ser√≠a una pieza de ropa exitosa que a√∫n no existe. Llevamos eso al equipo de comercializaci√≥n y ahora pueden usarlo como una herramienta".  
[Referencia](https://www.zdnet.com/article/how-stitch-fix-uses-machine-learning-to-master-the-science-of-styling/)

## üè• Salud

El sector de la salud puede aprovechar ML para optimizar tareas de investigaci√≥n y tambi√©n problemas log√≠sticos como la readmisi√≥n de pacientes o la prevenci√≥n de la propagaci√≥n de enfermedades.

### Gesti√≥n de ensayos cl√≠nicos

La toxicidad en los ensayos cl√≠nicos es una gran preocupaci√≥n para los fabricantes de medicamentos. ¬øCu√°nta toxicidad es tolerable? En este estudio, el an√°lisis de varios m√©todos de ensayos cl√≠nicos llev√≥ al desarrollo de un nuevo enfoque para predecir las probabilidades de los resultados de los ensayos cl√≠nicos. Espec√≠ficamente, pudieron usar random forest para producir un [clasificador](../../4-Classification/README.md) capaz de distinguir entre grupos de medicamentos.  
[Referencia](https://www.sciencedirect.com/science/article/pii/S2451945616302914)

### Gesti√≥n de readmisiones hospitalarias

La atenci√≥n hospitalaria es costosa, especialmente cuando los pacientes deben ser readmitidos. Este art√≠culo analiza una empresa que utiliza ML para predecir el potencial de readmisi√≥n utilizando algoritmos de [agrupamiento](../../5-Clustering/README.md). Estos grupos ayudan a los analistas a "descubrir grupos de readmisiones que pueden compartir una causa com√∫n".  
[Referencia](https://healthmanagement.org/c/healthmanagement/issuearticle/hospital-readmissions-and-machine-learning)

### Gesti√≥n de enfermedades

La reciente pandemia ha puesto de relieve c√≥mo el aprendizaje autom√°tico puede ayudar a detener la propagaci√≥n de enfermedades. En este art√≠culo, reconocer√°s el uso de ARIMA, curvas log√≠sticas, regresi√≥n lineal y SARIMA. "Este trabajo es un intento de calcular la tasa de propagaci√≥n de este virus y, por lo tanto, predecir las muertes, recuperaciones y casos confirmados, para que podamos prepararnos mejor y sobrevivir".  
[Referencia](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7979218/)

## üå≤ Ecolog√≠a y tecnolog√≠a verde

La naturaleza y la ecolog√≠a consisten en muchos sistemas sensibles donde la interacci√≥n entre animales y la naturaleza es clave. Es importante medir estos sistemas con precisi√≥n y actuar adecuadamente si ocurre algo, como un incendio forestal o una disminuci√≥n en la poblaci√≥n animal.

### Gesti√≥n forestal

Aprendiste sobre [aprendizaje por refuerzo](../../8-Reinforcement/README.md) en lecciones anteriores. Puede ser muy √∫til al intentar predecir patrones en la naturaleza. En particular, puede usarse para rastrear problemas ecol√≥gicos como incendios forestales y la propagaci√≥n de especies invasoras. En Canad√°, un grupo de investigadores utiliz√≥ aprendizaje por refuerzo para construir modelos de din√°mica de incendios forestales a partir de im√°genes satelitales. Usando un innovador "proceso de propagaci√≥n espacial (SSP)", imaginaron un incendio forestal como "el agente en cualquier celda del paisaje". "El conjunto de acciones que el fuego puede tomar desde una ubicaci√≥n en cualquier momento incluye propagarse al norte, sur, este u oeste o no propagarse".  
[Referencia](https://www.frontiersin.org/articles/10.3389/fict.2018.00006/full)

### Detecci√≥n de movimiento de animales

Aunque el aprendizaje profundo ha revolucionado el seguimiento visual de movimientos de animales (puedes construir tu propio [rastreador de osos polares](https://docs.microsoft.com/learn/modules/build-ml-model-with-azure-stream-analytics/?WT.mc_id=academic-77952-leestott) aqu√≠), el ML cl√°sico todav√≠a tiene un lugar en esta tarea.

Los sensores para rastrear movimientos de animales de granja e IoT hacen uso de este tipo de procesamiento visual, pero las t√©cnicas m√°s b√°sicas de ML son √∫tiles para preprocesar datos. Por ejemplo, en este art√≠culo, se monitorearon y analizaron posturas de ovejas utilizando varios algoritmos clasificadores. Podr√≠as reconocer la curva ROC en la p√°gina 335.  
[Referencia](https://druckhaus-hofmann.de/gallery/31-wj-feb-2020.pdf)

### ‚ö°Ô∏è Gesti√≥n de energ√≠a

En nuestras lecciones sobre [pron√≥stico de series temporales](../../7-TimeSeries/README.md), invocamos el concepto de parqu√≠metros inteligentes para generar ingresos para una ciudad bas√°ndonos en la comprensi√≥n de la oferta y la demanda. Este art√≠culo analiza en detalle c√≥mo el agrupamiento, la regresi√≥n y el pron√≥stico de series temporales se combinaron para ayudar a predecir el uso futuro de energ√≠a en Irlanda, basado en medidores inteligentes.  
[Referencia](https://www-cdn.knime.com/sites/default/files/inline-images/knime_bigdata_energy_timeseries_whitepaper.pdf)

## üíº Seguros

El sector de seguros es otro sector que utiliza ML para construir y optimizar modelos financieros y actuariales viables.

### Gesti√≥n de volatilidad

MetLife, un proveedor de seguros de vida, es transparente sobre c√≥mo analizan y mitigan la volatilidad en sus modelos financieros. En este art√≠culo notar√°s visualizaciones de clasificaci√≥n binaria y ordinal. Tambi√©n descubrir√°s visualizaciones de pron√≥sticos.  
[Referencia](https://investments.metlife.com/content/dam/metlifecom/us/investments/insights/research-topics/macro-strategy/pdf/MetLifeInvestmentManagement_MachineLearnedRanking_070920.pdf)

## üé® Artes, cultura y literatura

En las artes, por ejemplo en el periodismo, hay muchos problemas interesantes. Detectar noticias falsas es un gran desaf√≠o, ya que se ha demostrado que influyen en la opini√≥n de las personas e incluso pueden desestabilizar democracias. Los museos tambi√©n pueden beneficiarse del uso de ML en todo, desde encontrar v√≠nculos entre artefactos hasta la planificaci√≥n de recursos.

### Detecci√≥n de noticias falsas

Detectar noticias falsas se ha convertido en un juego del gato y el rat√≥n en los medios de comunicaci√≥n actuales. En este art√≠culo, los investigadores sugieren que un sistema que combine varias de las t√©cnicas de ML que hemos estudiado puede probarse y desplegarse el mejor modelo: "Este sistema se basa en el procesamiento de lenguaje natural para extraer caracter√≠sticas de los datos y luego estas caracter√≠sticas se utilizan para entrenar clasificadores de aprendizaje autom√°tico como Naive Bayes, Support Vector Machine (SVM), Random Forest (RF), Stochastic Gradient Descent (SGD) y Logistic Regression (LR)".  
[Referencia](https://www.irjet.net/archives/V7/i6/IRJET-V7I6688.pdf)

Este art√≠culo muestra c√≥mo combinar diferentes dominios de ML puede producir resultados interesantes que ayuden a detener la propagaci√≥n de noticias falsas y evitar da√±os reales; en este caso, el impulso fue la propagaci√≥n de rumores sobre tratamientos para el COVID que incitaron a la violencia.

### ML en museos

Los museos est√°n al borde de una revoluci√≥n de IA en la que catalogar y digitalizar colecciones y encontrar v√≠nculos entre artefactos se est√° volviendo m√°s f√°cil a medida que avanza la tecnolog√≠a. Proyectos como [In Codice Ratio](https://www.sciencedirect.com/science/article/abs/pii/S0306457321001035#:~:text=1.,studies%20over%20large%20historical%20sources.) est√°n ayudando a desbloquear los misterios de colecciones inaccesibles como los Archivos Vaticanos. Pero el aspecto comercial de los museos tambi√©n se beneficia de los modelos de ML.

Por ejemplo, el Instituto de Arte de Chicago construy√≥ modelos para predecir qu√© interesa a las audiencias y cu√°ndo asistir√°n a exposiciones. El objetivo es crear experiencias de visitante individualizadas y optimizadas cada vez que el usuario visite el museo. "Durante el a√±o fiscal 2017, el modelo predijo la asistencia y las admisiones con un 1% de precisi√≥n, dice Andrew Simnick, vicepresidente senior del Instituto de Arte".  
[Referencia](https://www.chicagobusiness.com/article/20180518/ISSUE01/180519840/art-institute-of-chicago-uses-data-to-make-exhibit-choices)

## üè∑ Marketing

### Segmentaci√≥n de clientes

Las estrategias de marketing m√°s efectivas apuntan a los clientes de diferentes maneras seg√∫n varios grupos. En este art√≠culo, se discuten los usos de los algoritmos de agrupamiento para apoyar el marketing diferenciado. El marketing diferenciado ayuda a las empresas a mejorar el reconocimiento de marca, llegar a m√°s clientes y ganar m√°s dinero.  
[Referencia](https://ai.inqline.com/machine-learning-for-marketing-customer-segmentation/)

## üöÄ Desaf√≠o

Identifica otro sector que se beneficie de algunas de las t√©cnicas que aprendiste en este plan de estudios y descubre c√≥mo utiliza ML.
## [Cuestionario posterior a la clase](https://ff-quizzes.netlify.app/en/ml/)

## Revisi√≥n y Autoestudio

El equipo de ciencia de datos de Wayfair tiene varios videos interesantes sobre c√≥mo utilizan el aprendizaje autom√°tico en su empresa. Vale la pena [echarles un vistazo](https://www.youtube.com/channel/UCe2PjkQXqOuwkW1gw6Ameuw/videos).

## Tarea

[Una b√∫squeda del tesoro de aprendizaje autom√°tico](assignment.md)

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por lograr precisi√≥n, tenga en cuenta que las traducciones autom√°ticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err√≥neas que puedan surgir del uso de esta traducci√≥n.

---

<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df2b538e8fbb3e91cf0419ae2f858675",
  "translation_date": "2025-09-04T22:19:52+00:00",
  "source_file": "9-Real-World/2-Debugging-ML-Models/README.md",
  "language_code": "es"
}
-->
# Posdata: Depuraci√≥n de modelos de aprendizaje autom√°tico utilizando componentes del panel de IA Responsable

## [Cuestionario previo a la clase](https://ff-quizzes.netlify.app/en/ml/)

## Introducci√≥n

El aprendizaje autom√°tico impacta nuestra vida cotidiana. La IA est√° encontrando su lugar en algunos de los sistemas m√°s importantes que nos afectan como individuos y como sociedad, desde la atenci√≥n m√©dica, las finanzas, la educaci√≥n y el empleo. Por ejemplo, los sistemas y modelos est√°n involucrados en tareas de toma de decisiones diarias, como diagn√≥sticos m√©dicos o detecci√≥n de fraudes. En consecuencia, los avances en IA junto con su adopci√≥n acelerada est√°n siendo recibidos con expectativas sociales en evoluci√≥n y una creciente regulaci√≥n en respuesta. Constantemente vemos √°reas donde los sistemas de IA no cumplen con las expectativas; exponen nuevos desaf√≠os; y los gobiernos est√°n comenzando a regular las soluciones de IA. Por lo tanto, es importante que estos modelos sean analizados para proporcionar resultados justos, confiables, inclusivos, transparentes y responsables para todos.

En este plan de estudios, exploraremos herramientas pr√°cticas que pueden utilizarse para evaluar si un modelo tiene problemas relacionados con la IA Responsable. Las t√©cnicas tradicionales de depuraci√≥n de aprendizaje autom√°tico tienden a basarse en c√°lculos cuantitativos como la precisi√≥n agregada o la p√©rdida promedio de error. Imagina lo que puede suceder cuando los datos que est√°s utilizando para construir estos modelos carecen de ciertos grupos demogr√°ficos, como raza, g√©nero, visi√≥n pol√≠tica, religi√≥n, o representan desproporcionadamente dichos grupos demogr√°ficos. ¬øQu√© sucede cuando la salida del modelo se interpreta como favorable para alg√∫n grupo demogr√°fico? Esto puede introducir una representaci√≥n excesiva o insuficiente de estos grupos sensibles, lo que resulta en problemas de equidad, inclusi√≥n o confiabilidad del modelo. Otro factor es que los modelos de aprendizaje autom√°tico son considerados cajas negras, lo que dificulta entender y explicar qu√© impulsa las predicciones de un modelo. Todos estos son desaf√≠os que enfrentan los cient√≠ficos de datos y desarrolladores de IA cuando no tienen herramientas adecuadas para depurar y evaluar la equidad o confiabilidad de un modelo.

En esta lecci√≥n, aprender√°s a depurar tus modelos utilizando:

- **An√°lisis de errores**: identificar d√≥nde en la distribuci√≥n de tus datos el modelo tiene altas tasas de error.
- **Visi√≥n general del modelo**: realizar an√°lisis comparativos entre diferentes cohortes de datos para descubrir disparidades en las m√©tricas de rendimiento de tu modelo.
- **An√°lisis de datos**: investigar d√≥nde podr√≠a haber una representaci√≥n excesiva o insuficiente de tus datos que pueda sesgar tu modelo para favorecer un grupo demogr√°fico sobre otro.
- **Importancia de las caracter√≠sticas**: comprender qu√© caracter√≠sticas est√°n impulsando las predicciones de tu modelo a nivel global o local.

## Prerrequisito

Como prerrequisito, revisa [Herramientas de IA Responsable para desarrolladores](https://www.microsoft.com/ai/ai-lab-responsible-ai-dashboard)

> ![Gif sobre herramientas de IA Responsable](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/9-Real-World/2-Debugging-ML-Models/images/rai-overview.gif)

## An√°lisis de errores

Las m√©tricas tradicionales de rendimiento de modelos utilizadas para medir la precisi√≥n son principalmente c√°lculos basados en predicciones correctas frente a incorrectas. Por ejemplo, determinar que un modelo es preciso el 89% del tiempo con una p√©rdida de error de 0.001 puede considerarse un buen rendimiento. Los errores a menudo no se distribuyen uniformemente en tu conjunto de datos subyacente. Puedes obtener una puntuaci√≥n de precisi√≥n del modelo del 89%, pero descubrir que hay diferentes regiones de tus datos en las que el modelo falla el 42% del tiempo. La consecuencia de estos patrones de falla con ciertos grupos de datos puede llevar a problemas de equidad o confiabilidad. Es esencial comprender las √°reas donde el modelo est√° funcionando bien o no. Las regiones de datos donde hay un alto n√∫mero de inexactitudes en tu modelo pueden resultar ser un grupo demogr√°fico importante.

![Analizar y depurar errores del modelo](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/9-Real-World/2-Debugging-ML-Models/images/ea-error-distribution.png)

El componente de An√°lisis de Errores en el panel de IA Responsable ilustra c√≥mo se distribuyen las fallas del modelo entre varios cohortes con una visualizaci√≥n en forma de √°rbol. Esto es √∫til para identificar caracter√≠sticas o √°reas donde hay una alta tasa de error en tu conjunto de datos. Al observar de d√≥nde provienen la mayor√≠a de las inexactitudes del modelo, puedes comenzar a investigar la causa ra√≠z. Tambi√©n puedes crear cohortes de datos para realizar an√°lisis. Estos cohortes de datos ayudan en el proceso de depuraci√≥n para determinar por qu√© el rendimiento del modelo es bueno en un cohorte pero err√≥neo en otro.

![An√°lisis de errores](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/9-Real-World/2-Debugging-ML-Models/images/ea-error-cohort.png)

Los indicadores visuales en el mapa del √°rbol ayudan a localizar las √°reas problem√°ticas m√°s r√°pidamente. Por ejemplo, cuanto m√°s oscuro sea el tono de rojo en un nodo del √°rbol, mayor ser√° la tasa de error.

El mapa de calor es otra funcionalidad de visualizaci√≥n que los usuarios pueden utilizar para investigar la tasa de error utilizando una o dos caracter√≠sticas para encontrar un contribuyente a los errores del modelo en todo el conjunto de datos o cohortes.

![Mapa de calor de an√°lisis de errores](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/9-Real-World/2-Debugging-ML-Models/images/ea-heatmap.png)

Utiliza el an√°lisis de errores cuando necesites:

* Obtener una comprensi√≥n profunda de c√≥mo se distribuyen las fallas del modelo en un conjunto de datos y en varias dimensiones de entrada y caracter√≠sticas.
* Desglosar las m√©tricas de rendimiento agregadas para descubrir autom√°ticamente cohortes err√≥neos que informen tus pasos de mitigaci√≥n espec√≠ficos.

## Visi√≥n general del modelo

Evaluar el rendimiento de un modelo de aprendizaje autom√°tico requiere obtener una comprensi√≥n hol√≠stica de su comportamiento. Esto puede lograrse revisando m√°s de una m√©trica, como tasa de error, precisi√≥n, recall, precisi√≥n o MAE (Error Absoluto Medio) para encontrar disparidades entre las m√©tricas de rendimiento. Una m√©trica de rendimiento puede parecer excelente, pero las inexactitudes pueden exponerse en otra m√©trica. Adem√°s, comparar las m√©tricas para encontrar disparidades en todo el conjunto de datos o cohortes ayuda a arrojar luz sobre d√≥nde el modelo est√° funcionando bien o no. Esto es especialmente importante para observar el rendimiento del modelo entre caracter√≠sticas sensibles frente a insensibles (por ejemplo, raza, g√©nero o edad del paciente) para descubrir posibles problemas de equidad que pueda tener el modelo. Por ejemplo, descubrir que el modelo es m√°s err√≥neo en un cohorte que tiene caracter√≠sticas sensibles puede revelar posibles problemas de equidad en el modelo.

El componente de Visi√≥n General del Modelo del panel de IA Responsable ayuda no solo a analizar las m√©tricas de rendimiento de la representaci√≥n de datos en un cohorte, sino que tambi√©n brinda a los usuarios la capacidad de comparar el comportamiento del modelo entre diferentes cohortes.

![Cohortes de conjuntos de datos - visi√≥n general del modelo en el panel de IA Responsable](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/9-Real-World/2-Debugging-ML-Models/images/model-overview-dataset-cohorts.png)

La funcionalidad de an√°lisis basada en caracter√≠sticas del componente permite a los usuarios reducir subgrupos de datos dentro de una caracter√≠stica particular para identificar anomal√≠as a nivel granular. Por ejemplo, el panel tiene inteligencia integrada para generar autom√°ticamente cohortes para una caracter√≠stica seleccionada por el usuario (por ejemplo, *"time_in_hospital < 3"* o *"time_in_hospital >= 7"*). Esto permite al usuario aislar una caracter√≠stica particular de un grupo de datos m√°s grande para ver si es un factor clave en los resultados err√≥neos del modelo.

![Cohortes de caracter√≠sticas - visi√≥n general del modelo en el panel de IA Responsable](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/9-Real-World/2-Debugging-ML-Models/images/model-overview-feature-cohorts.png)

El componente de Visi√≥n General del Modelo admite dos clases de m√©tricas de disparidad:

**Disparidad en el rendimiento del modelo**: Este conjunto de m√©tricas calcula la disparidad (diferencia) en los valores de la m√©trica de rendimiento seleccionada entre subgrupos de datos. Aqu√≠ hay algunos ejemplos:

* Disparidad en la tasa de precisi√≥n
* Disparidad en la tasa de error
* Disparidad en la precisi√≥n
* Disparidad en el recall
* Disparidad en el error absoluto medio (MAE)

**Disparidad en la tasa de selecci√≥n**: Esta m√©trica contiene la diferencia en la tasa de selecci√≥n (predicci√≥n favorable) entre subgrupos. Un ejemplo de esto es la disparidad en las tasas de aprobaci√≥n de pr√©stamos. La tasa de selecci√≥n significa la fracci√≥n de puntos de datos en cada clase clasificados como 1 (en clasificaci√≥n binaria) o la distribuci√≥n de valores de predicci√≥n (en regresi√≥n).

## An√°lisis de datos

> "Si torturas los datos lo suficiente, confesar√°n cualquier cosa" - Ronald Coase

Esta afirmaci√≥n suena extrema, pero es cierto que los datos pueden manipularse para respaldar cualquier conclusi√≥n. Tal manipulaci√≥n a veces puede ocurrir de manera no intencional. Como humanos, todos tenemos sesgos, y a menudo es dif√≠cil saber conscientemente cu√°ndo est√°s introduciendo sesgos en los datos. Garantizar la equidad en la IA y el aprendizaje autom√°tico sigue siendo un desaf√≠o complejo.

Los datos son un gran punto ciego para las m√©tricas tradicionales de rendimiento de modelos. Puedes tener puntuaciones de precisi√≥n altas, pero esto no siempre refleja el sesgo subyacente en los datos que podr√≠a estar en tu conjunto de datos. Por ejemplo, si un conjunto de datos de empleados tiene un 27% de mujeres en puestos ejecutivos en una empresa y un 73% de hombres en el mismo nivel, un modelo de IA para publicidad de empleo entrenado con estos datos puede dirigirse principalmente a una audiencia masculina para puestos de alto nivel. Tener este desequilibrio en los datos sesg√≥ la predicci√≥n del modelo para favorecer un g√©nero. Esto revela un problema de equidad donde hay un sesgo de g√©nero en el modelo de IA.

El componente de An√°lisis de Datos en el panel de IA Responsable ayuda a identificar √°reas donde hay una representaci√≥n excesiva o insuficiente en el conjunto de datos. Ayuda a los usuarios a diagnosticar la causa ra√≠z de errores y problemas de equidad introducidos por desequilibrios en los datos o la falta de representaci√≥n de un grupo de datos particular. Esto brinda a los usuarios la capacidad de visualizar conjuntos de datos basados en resultados predichos y reales, grupos de errores y caracter√≠sticas espec√≠ficas. A veces, descubrir un grupo de datos subrepresentado tambi√©n puede revelar que el modelo no est√° aprendiendo bien, de ah√≠ las altas inexactitudes. Tener un modelo con sesgo en los datos no solo es un problema de equidad, sino que muestra que el modelo no es inclusivo ni confiable.

![Componente de an√°lisis de datos en el panel de IA Responsable](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/9-Real-World/2-Debugging-ML-Models/images/dataanalysis-cover.png)

Utiliza el an√°lisis de datos cuando necesites:

* Explorar las estad√≠sticas de tu conjunto de datos seleccionando diferentes filtros para dividir tus datos en diferentes dimensiones (tambi√©n conocidas como cohortes).
* Comprender la distribuci√≥n de tu conjunto de datos entre diferentes cohortes y grupos de caracter√≠sticas.
* Determinar si tus hallazgos relacionados con equidad, an√°lisis de errores y causalidad (derivados de otros componentes del panel) son resultado de la distribuci√≥n de tu conjunto de datos.
* Decidir en qu√© √°reas recolectar m√°s datos para mitigar errores que provienen de problemas de representaci√≥n, ruido en las etiquetas, ruido en las caracter√≠sticas, sesgo en las etiquetas y factores similares.

## Interpretabilidad del modelo

Los modelos de aprendizaje autom√°tico tienden a ser cajas negras. Comprender qu√© caracter√≠sticas clave de los datos impulsan la predicci√≥n de un modelo puede ser un desaf√≠o. Es importante proporcionar transparencia sobre por qu√© un modelo hace una determinada predicci√≥n. Por ejemplo, si un sistema de IA predice que un paciente diab√©tico est√° en riesgo de ser readmitido en un hospital en menos de 30 d√≠as, deber√≠a poder proporcionar datos de apoyo que llevaron a su predicci√≥n. Tener indicadores de datos de apoyo aporta transparencia para ayudar a los m√©dicos o hospitales a tomar decisiones bien informadas. Adem√°s, poder explicar por qu√© un modelo hizo una predicci√≥n para un paciente individual permite responsabilidad con las regulaciones de salud. Cuando utilizas modelos de aprendizaje autom√°tico de maneras que afectan la vida de las personas, es crucial comprender y explicar qu√© influye en el comportamiento de un modelo. La explicabilidad e interpretabilidad del modelo ayuda a responder preguntas en escenarios como:

* Depuraci√≥n del modelo: ¬øPor qu√© mi modelo cometi√≥ este error? ¬øC√≥mo puedo mejorar mi modelo?
* Colaboraci√≥n humano-IA: ¬øC√≥mo puedo entender y confiar en las decisiones del modelo?
* Cumplimiento normativo: ¬øCumple mi modelo con los requisitos legales?

El componente de Importancia de las Caracter√≠sticas del panel de IA Responsable te ayuda a depurar y obtener una comprensi√≥n integral de c√≥mo un modelo hace predicciones. Tambi√©n es una herramienta √∫til para profesionales de aprendizaje autom√°tico y tomadores de decisiones para explicar y mostrar evidencia de las caracter√≠sticas que influyen en el comportamiento de un modelo para el cumplimiento normativo. Los usuarios pueden explorar explicaciones globales y locales para validar qu√© caracter√≠sticas impulsan la predicci√≥n de un modelo. Las explicaciones globales enumeran las principales caracter√≠sticas que afectaron la predicci√≥n general de un modelo. Las explicaciones locales muestran qu√© caracter√≠sticas llevaron a la predicci√≥n de un modelo para un caso individual. La capacidad de evaluar explicaciones locales tambi√©n es √∫til para depurar o auditar un caso espec√≠fico para comprender mejor y explicar por qu√© un modelo hizo una predicci√≥n precisa o inexacta.

![Componente de importancia de caracter√≠sticas en el panel de IA Responsable](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/9-Real-World/2-Debugging-ML-Models/images/9-feature-importance.png)

* Explicaciones globales: Por ejemplo, ¬øqu√© caracter√≠sticas afectan el comportamiento general de un modelo de readmisi√≥n hospitalaria para pacientes diab√©ticos?
* Explicaciones locales: Por ejemplo, ¬øpor qu√© se predijo que un paciente diab√©tico mayor de 60 a√±os con hospitalizaciones previas ser√≠a readmitido o no readmitido dentro de los 30 d√≠as en un hospital?

En el proceso de depuraci√≥n al examinar el rendimiento de un modelo entre diferentes cohortes, la Importancia de las Caracter√≠sticas muestra qu√© nivel de impacto tiene una caracter√≠stica en los cohortes. Ayuda a revelar anomal√≠as al comparar el nivel de influencia que tiene la caracter√≠stica en impulsar las predicciones err√≥neas de un modelo. El componente de Importancia de las Caracter√≠sticas puede mostrar qu√© valores en una caracter√≠stica influyeron positiva o negativamente en el resultado del modelo. Por ejemplo, si un modelo hizo una predicci√≥n inexacta, el componente te da la capacidad de profundizar y se√±alar qu√© caracter√≠sticas o valores de caracter√≠sticas impulsaron la predicci√≥n. Este nivel de detalle no solo ayuda en la depuraci√≥n, sino que proporciona transparencia y responsabilidad en situaciones de auditor√≠a. Finalmente, el componente puede ayudarte a identificar problemas de equidad. Para ilustrar, si una caracter√≠stica sensible como etnia o g√©nero tiene una alta influencia en impulsar la predicci√≥n de un modelo, esto podr√≠a ser un indicio de sesgo racial o de g√©nero en el modelo.

![Importancia de caracter√≠sticas](https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/9-Real-World/2-Debugging-ML-Models/images/9-features-influence.png)

Utiliza la interpretabilidad cuando necesites:

* Determinar qu√© tan confiables son las predicciones de tu sistema de IA al comprender qu√© caracter√≠sticas son m√°s importantes para las predicciones.
* Abordar la depuraci√≥n de tu modelo al comprenderlo primero e identificar si el modelo est√° utilizando caracter√≠sticas saludables o simplemente correlaciones falsas.
* Descubrir posibles fuentes de inequidad al comprender si el modelo est√° basando sus predicciones en caracter√≠sticas sensibles o en caracter√≠sticas altamente correlacionadas con ellas.
* Generar confianza en las decisiones de tu modelo al generar explicaciones locales para ilustrar sus resultados.
* Completar una auditor√≠a regulatoria de un sistema de IA para validar modelos y monitorear el impacto de las decisiones del modelo en las personas.

## Conclusi√≥n

Todos los componentes del panel de IA Responsable son herramientas pr√°cticas para ayudarte a construir modelos de aprendizaje autom√°tico que sean menos perjudiciales y m√°s confiables para la sociedad. Mejoran la prevenci√≥n de amenazas a los derechos humanos; la discriminaci√≥n o exclusi√≥n de ciertos grupos de oportunidades de vida; y el riesgo de da√±o f√≠sico o psicol√≥gico. Tambi√©n ayudan a generar confianza en las decisiones de tu modelo al generar explicaciones locales para ilustrar sus resultados. Algunos de los posibles da√±os pueden clasificarse como:

- **Asignaci√≥n**, si un g√©nero o etnia, por ejemplo, es favorecido sobre otro.
- **Calidad del servicio**. Si entrenas los datos para un escenario espec√≠fico pero la realidad es mucho m√°s compleja, esto lleva a un servicio de bajo rendimiento.
- **Estereotipos**. Asociar un grupo dado con atributos preasignados.
- **Denigraci√≥n**. Criticar injustamente y etiquetar algo o alguien.
- **Sobre- o sub-representaci√≥n**. La idea es que un cierto grupo no se vea en una determinada profesi√≥n, y cualquier servicio o funci√≥n que siga promoviendo eso est√° contribuyendo al da√±o.

### Azure RAI dashboard

[Azure RAI dashboard](https://learn.microsoft.com/en-us/azure/machine-learning/concept-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu) est√° construido sobre herramientas de c√≥digo abierto desarrolladas por instituciones acad√©micas y organizaciones l√≠deres, incluyendo Microsoft. Estas herramientas son fundamentales para que los cient√≠ficos de datos y desarrolladores de IA comprendan mejor el comportamiento de los modelos, descubran y mitiguen problemas indeseables en los modelos de IA.

- Aprende c√≥mo usar los diferentes componentes consultando la [documentaci√≥n del RAI dashboard.](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu)

- Consulta algunos [notebooks de ejemplo del RAI dashboard](https://github.com/Azure/RAI-vNext-Preview/tree/main/examples/notebooks) para depurar escenarios de IA m√°s responsables en Azure Machine Learning.

---
## üöÄ Desaf√≠o

Para evitar que se introduzcan sesgos estad√≠sticos o de datos desde el principio, deber√≠amos:

- contar con una diversidad de antecedentes y perspectivas entre las personas que trabajan en los sistemas
- invertir en conjuntos de datos que reflejen la diversidad de nuestra sociedad
- desarrollar mejores m√©todos para detectar y corregir sesgos cuando ocurren

Piensa en escenarios de la vida real donde la injusticia sea evidente en la construcci√≥n y uso de modelos. ¬øQu√© m√°s deber√≠amos considerar?

## [Cuestionario posterior a la clase](https://ff-quizzes.netlify.app/en/ml/)
## Revisi√≥n y Autoestudio

En esta lecci√≥n, has aprendido algunas herramientas pr√°cticas para incorporar IA responsable en el aprendizaje autom√°tico.

Mira este taller para profundizar en los temas:

- Responsible AI Dashboard: Una soluci√≥n integral para operacionalizar la IA responsable en la pr√°ctica por Besmira Nushi y Mehrnoosh Sameki

[![Responsible AI Dashboard: Una soluci√≥n integral para operacionalizar la IA responsable en la pr√°ctica](https://img.youtube.com/vi/f1oaDNl3djg/0.jpg)](https://www.youtube.com/watch?v=f1oaDNl3djg "Responsible AI Dashboard: Una soluci√≥n integral para operacionalizar la IA responsable en la pr√°ctica")


> üé• Haz clic en la imagen de arriba para ver el video: Responsible AI Dashboard: Una soluci√≥n integral para operacionalizar la IA responsable en la pr√°ctica por Besmira Nushi y Mehrnoosh Sameki

Consulta los siguientes materiales para aprender m√°s sobre IA responsable y c√≥mo construir modelos m√°s confiables:

- Herramientas del RAI dashboard de Microsoft para depurar modelos de aprendizaje autom√°tico: [Recursos de herramientas de IA responsable](https://aka.ms/rai-dashboard)

- Explora el kit de herramientas de IA responsable: [Github](https://github.com/microsoft/responsible-ai-toolbox)

- Centro de recursos de IA responsable de Microsoft: [Recursos de IA Responsable ‚Äì Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- Grupo de investigaci√≥n FATE de Microsoft: [FATE: Equidad, Responsabilidad, Transparencia y √âtica en IA - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

## Tarea

[Explora el RAI dashboard](assignment.md)

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por garantizar la precisi√≥n, tenga en cuenta que las traducciones automatizadas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de ning√∫n malentendido o interpretaci√≥n err√≥nea que surja del uso de esta traducci√≥n.

---
