# Curso de NLP de Hugging Face ü§ó
## Parte 1: Fundamentos

**Cap√≠tulos 0-4:** Configuraci√≥n, Modelos de Transformadores, Usando Transformers, Ajuste fino, Compartiendo modelos

---

# 0. Configuraci√≥n

# Introducci√≥n

Bienvenido al curso de Hugging Face. Esta introducci√≥n te guiar√° en la configuraci√≥n de un entorno de trabajo. Si acabas de empezar el curso, te recomendamos que primero eches un vistazo al [Cap√≠tulo 1](/course/chapter1), y luego vuelvas y configures tu entorno para poder probar el c√≥digo por ti mismo.

Todas las librer√≠as que usaremos en este curso est√°n disponibles como paquetes de Python, as√≠ que aqu√≠ te mostraremos c√≥mo configurar un entorno de Python e instalar las librer√≠as espec√≠ficas que necesitar√°s.

Cubriremos dos formas de configurar tu entorno de trabajo, utilizando un cuaderno Colab o un entorno virtual Python. Si√©ntete libre de elegir la que m√°s te convenga. Para los principiantes, recomendamos encarecidamente que comiencen utilizando un cuaderno Colab.

Tenga en cuenta que no vamos a cubrir el sistema Windows. Si est√° utilizando Windows, le recomendamos que siga utilizando un cuaderno Colab. Si est√° utilizando una distribuci√≥n de Linux o macOS, puede utilizar cualquiera de los enfoques descritos aqu√≠.

La mayor parte del curso depende de que tengas una cuenta de Hugging Face. Te recomendamos que crees una ahora: [crear una cuenta](https://huggingface.co/join).

## Uso de un cuaderno Google Colab

Utilizar un cuaderno Colab es la configuraci√≥n m√°s sencilla posible; ¬°arranca un cuaderno en tu navegador y ponte a codificar directamente! 

Si no est√°s familiarizado con Colab, te recomendamos que empieces siguiendo la [introducci√≥n](https://colab.research.google.com/notebooks/intro.ipynb). Colab te permite utilizar alg√∫n hardware de aceleraci√≥n, como GPUs o TPUs, y es gratuito para cargas de trabajo peque√±as.

Una vez que te sientas c√≥modo movi√©ndote en Colab, crea un nuevo notebook y comienza con la configuraci√≥n:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter0/new_colab.png" alt="An empty colab notebook" width="80%"/>
</div>

El siguiente paso es instalar las librer√≠as que usaremos en este curso. Usaremos `pip` para la instalaci√≥n, que es el gestor de paquetes para Python. En los cuadernos, puedes ejecutar comandos del sistema precedi√©ndolos con el car√°cter `!`, as√≠ que puedes instalar la librer√≠a ü§ó Transformers de la siguiente manera:

```
!pip install transformers
```

Puede asegurarse de que el paquete se ha instalado correctamente import√°ndolo en su tiempo de ejecuci√≥n de Python:

```
import transformers
```

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter0/install.gif" alt="A gif showing the result of the two commands above: installation and import" width="80%"/>
</div>

Esto instala una versi√≥n muy ligera de ü§ó Transformers. En particular, no se instalan frameworks espec√≠ficos de deep learning (como PyTorch o TensorFlow). Dado que vamos a utilizar un mont√≥n de caracter√≠sticas diferentes de la librer√≠a, se recomienda instalar la versi√≥n de desarrollo, que viene con todas las dependencias necesarias para casi cualquier caso de uso imaginable:

```
!pip install transformers[sentencepiece]
```

Esto te llevar√° un poco de tiempo, pero luego estar√°s listo para el resto del curso.

## Usar un entorno virtual de Python

Si prefieres utilizar un entorno virtual de Python, el primer paso es instalar Python en tu sistema. Recomendamos seguir [esta gu√≠a](https://realpython.com/installing-python/) para empezar.

Una vez que tengas Python instalado, deber√≠as poder ejecutar comandos de Python en tu terminal. Puedes empezar ejecutando el siguiente comando para asegurarte de que est√° correctamente instalado antes de proceder a los siguientes pasos: `python --version`. Esto deber√≠a imprimir la versi√≥n de Python disponible en tu sistema.

Cuando ejecutes un comando de Python en tu terminal, como `python --version`, debes pensar en el programa que ejecuta tu comando como el Python "principal" de tu sistema. Recomendamos mantener esta instalaci√≥n principal libre de paquetes, y usarla para crear entornos separados para cada aplicaci√≥n en la que trabajes - de esta manera, cada aplicaci√≥n puede tener sus propias dependencias y paquetes, y no tendr√°s que preocuparte por posibles problemas de compatibilidad con otras aplicaciones.

En Python esto se hace con [*entornos virtuales*](https://docs.python.org/3/tutorial/venv.html), que son √°rboles de directorios autocontenidos que contienen cada uno una instalaci√≥n de Python con una versi√≥n particular de Python junto con todos los paquetes que la aplicaci√≥n necesita. La creaci√≥n de un entorno virtual de este tipo puede hacerse con varias herramientas diferentes, pero nosotros utilizaremos el paquete oficial de Python para este fin, que se llama [`venv`](https://docs.python.org/3/library/venv.html#module-venv).

En primer lugar, crea el directorio en el que te gustar√≠a que viviera tu aplicaci√≥n - por ejemplo, podr√≠as crear un nuevo directorio llamado *transformers-course* en la ra√≠z de tu directorio personal:

```
mkdir ~/transformers-course
cd ~/transformers-course
```

Desde este directorio, crea un entorno virtual utilizando el m√≥dulo `venv` de Python:

```
python -m venv .env
```

Ahora deber√≠a tener un directorio llamado *.env* en su carpeta, por lo dem√°s vac√≠a:

```
ls -a
```

```out
.      ..    .env
```

Puedes entrar y salir de tu entorno virtual con los scripts `activate` y `deactivate`:

```
# Activa el entorno virtual
source .env/bin/activate

# Desactiva el entorno virtual
deactivate
```

Puedes asegurarte de que el entorno est√° activado ejecutando el comando `which python`: si apunta al entorno virtual, entonces lo has activado con √©xito.

```
which python
```

```out
/home/<user>/transformers-course/.env/bin/python
```

### Instalaci√≥n de dependencias

Al igual que en la secci√≥n anterior sobre el uso de las instancias de Google Colab, ahora necesitar√°s instalar los paquetes necesarios para continuar. De nuevo, puedes instalar la versi√≥n de desarrollo de ü§ó Transformers utilizando el gestor de paquetes `pip`:

```
pip install "transformers[sentencepiece]"
```

Ya est√° todo preparado y listo para funcionar.


---

# 1. Modelos de Transformadores

# Introducci√≥n


## ¬°Te damos la bienvenida al curso de ü§ó!


**Video:** [Ver en YouTube](https://youtu.be/00GKzGyWFEs)


Este curso te ense√±ar√° sobre procesamiento de lenguaje natural (PLN) usando librer√≠as del ecosistema [Hugging Face](https://huggingface.co/) - [ü§ó Transformers](https://github.com/huggingface/transformers), [ü§ó Datasets](https://github.com/huggingface/datasets), [ü§ó Tokenizers](https://github.com/huggingface/tokenizers) y [ü§ó Accelerate](https://github.com/huggingface/accelerate) ‚Äî as√≠ como el [Hub de Hugging Face](https://huggingface.co/models). El curso es completamente gratuito y sin anuncios.

## ¬øQu√© esperar?

Esta es una peque√±a descripci√≥n del curso:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Brief overview of the chapters of the course.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Brief overview of the chapters of the course.">
</div>

- Los cap√≠tulos 1 a 4 ofrecen una introducci√≥n a los conceptos principales de la librer√≠a ü§ó Transformers. Al final de esta secci√≥n del curso, estar√°s familiarizado con la manera en que trabajan los Transformadores y sabr√°s c√≥mo usar un modelo del [Hub de Hugging Face](https://huggingface.co/models), ajustarlo a tu conjunto de datos y compartir tus resultados en el Hub.
- Los cap√≠tulos 5 a 8 ense√±an lo b√°sico de ü§ó Datasets y ü§ó Tokenizers antes de entrar en tareas cl√°sicas de PLN. Al final de esta secci√≥n, podr√°s abordar por ti mismo los problemas m√°s comunes de PLN.
- Los cap√≠tulos 9 al 12 van m√°s all√° del PLN y exploran c√≥mo los Transformadores pueden abordar tareas de procesamiento del habla y visi√≥n por computador. A lo largo del camino, aprender√°s a construir y compartir demos de tus modelos, as√≠ como optimizarlos para entornos de producci√≥n. Al final de esta secci√≥n, estar√°s listo para aplicar ü§ó Transformers a (casi) cualquier problema de Machine Learning.

Este curso:

* Requiere amplio conocimiento de Python
* Deber√≠a ser tomado despu√©s de un curso de introducci√≥n a deep learning, como [Practical Deep Learning for Coders](https://course.fast.ai/) de [fast.ai's](https://www.fast.ai/) o alguno de los programas desarrollados por [DeepLearning.AI](https://www.deeplearning.ai/)
* No necesita conocimiento previo de [PyTorch](https://pytorch.org/) o [TensorFlow](https://www.tensorflow.org/), aunque un nivel de familiaridad con alguno de los dos podr√≠a ser √∫til

Despu√©s de que hayas completado este curso, te recomendamos revisar la [Especializaci√≥n en Procesamiento de Lenguaje Natural](https://www.coursera.org/specializations/natural-language-processing?utm_source=deeplearning-ai&utm_medium=institutions&utm_campaign=20211011-PLN-2-hugging_face-page-PLN-refresh) de DeepLearning.AI, que cubre un gran n√∫mero de modelos tradicionales de PLN como Naive Bayes y LSTMs.

## ¬øQui√©nes somos?

Acerca de los autores:

**Matthew Carrigan** es Ingeniero de Machine Learning en Hugging Face. Vive en Dublin, Irlanda y anteriormente trabaj√≥ como Ingeniero ML en Parse.ly y como investigador post-doctoral en Trinity College Dublin. No cree que vamos a alcanzar una Inteligencia Artificial General escalando arquitecturas existentes, pero en todo caso tiene grandes expectativas sobre la inmortalidad rob√≥tica.

**Lysandre Debut** es Ingeniero de Machine Learning en Hugging Face y ha trabajado en la librer√≠a ü§ó Transformers desde sus etapas de desarrollo m√°s tempranas. Su objetivo es hacer que el PLN sea accesible para todos a trav√©s del desarrollo de herramientas con una API muy simple.

**Sylvain Gugger** es Ingeniero de Investigaci√≥n en Hugging Face y uno de los principales mantenedores de la librer√≠a ü§ó Transformers. Anteriormente fue Cient√≠fico de Investigaci√≥n en fast.ai y escribi√≥ _[Deep Learning for Coders with fastai and PyTorch](https://learning.oreilly.com/library/view/deep-learning-for/9781492045519/)_ junto con Jeremy Howard. El foco principal de su investigaci√≥n es hacer el deep learning m√°s accesible, al dise√±ar y mejorar t√©cnicas que permiten un entrenamiento r√°pido de modelos con recursos limitados.

**Merve Noyan** es Promotora de Desarrolladores en Hugging Face, trabaja en el desarrollo de herramientas y construcci√≥n de contenido relacionado, con el f√≠n de democratizar el machine learning para todos.

**Lucile Saulnier** es Ingeniera de Machine Learning en Hugging Face, donde desarrolla y apoya el uso de herramientas de c√≥digo abierto. Ella est√° activamente involucrada en varios proyectos de investigaci√≥n en el campo del Procesamiento de Lenguaje Natural como entrenamiento colaborativo y BigScience.

**Lewis Tunstall**  es Ingeniero de Machine Learning en Hugging Face, enfocado en desarrollar herramientas de c√≥digo abierto y hacerlas accesibles a la comunidad en general. Tambi√©n es coautor de un pr√≥ximo [libro de O'Reilly sobre Transformadores](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/).

**Leandro von Werra**  es Ingeniero de Machine Learning en el equipo de c√≥digo abierto en Hugging Face y coautor de un pr√≥ximo [libro de O'Reilly sobre Transformadores](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/). Tiene varios a√±os de experiencia en la industria llevando modelos de PLN a producci√≥n, trabajando a lo largo de todo el entorno de Machine Learning. 

¬øEst√°s listo para comenzar? En este cap√≠tulo vas a aprender:
* C√≥mo usar la funci√≥n `pipeline()` para resolver tareas de PLN como la generaci√≥n y clasificaci√≥n de texto
* Sobre la arquitectura de los Transformadores
* C√≥mo distinguir entre las arquitecturas de codificador, decodificador y codificador-decofidicador, adem√°s de sus casos de uso

---

# Procesamiento de Lenguaje Natural


Antes de ver los Transformadores, hagamos una revisi√≥n r√°pida de qu√© es el procesamiento de lenguaje natural y por qu√© nos interesa.

## ¬øQu√© es PLN?

El PLN es un campo de la ling√º√≠stica y el machine learning enfocado en entender todo lo relacionado con el lenguaje humano. El objetivo de las tareas de PLN no s√≥lo es entender palabras de manera individual, sino tambi√©n entender su contexto.

Esta es una lista de tareas comunes de PLN, con algunos ejemplos:

- **Clasificar oraciones enteras**: Obtener el sentimiento de una rese√±a, detectar si un correo electr√≥nico es spam, determinar si una oraci√≥n es gramaticalmente correcta o si dos oraciones est√°n l√≥gicamente relacionadas entre si
- **Clasificar cada palabra en una oraci√≥n**: Identificar los componentes gramaticales de una oraci√≥n (sustantivo, verbo, adjetivo) o las entidades nombradas (persona, ubicaci√≥n, organizaci√≥n)
- **Generar texto**: Completar una indicaci√≥n con texto generado autom√°ticamente, completar los espacios en blanco en un texto con palabras ocultas
- **Extraer una respuesta de un texto**: Dada una pregunta y un contexto, extraer la respuesta a la pregunta en funci√≥n de la informaci√≥n proporcionada en el contexto
- **Generar una nueva oraci√≥n de un texto de entrada**: Traducir un texto a otro idioma, resumir un texto

No obstante, el PLN no se limita a texto escrito. Tambi√©n aborda desaf√≠os complejos en reconocimiento del habla y visi√≥n por computador, como generar la transcripci√≥n de una muestra de audio o la descripci√≥n de una imagen.

## ¬øPor qu√© es retador?

Los computadores no procesan la informaci√≥n en la misma forma que los humanos. Por ejemplo, cuando leemos la oraci√≥n "tengo hambre", podemos identificar f√°cilmente su significado. De manera similar, dadas dos oraciones como "tengo hambre" y "estoy triste", podemos determinar f√°cilmente qu√© tan parecidas son. Para los modelos de Machine Learning (ML) estas tareas son m√°s dif√≠ciles. El texto necesita ser procesado de tal forma que le permita al modelo aprender de √©l. Y como el lenguaje es complejo, necesitamos pensar con cuidado c√≥mo debe ser este procesamiento. Hay gran cantidad de investigaci√≥n sobre c√≥mo representar texto y vamos a ver algunos m√©todos en el siguiente cap√≠tulo.


---

# Transformadores, ¬øqu√© pueden hacer?


En esta secci√≥n, veremos qu√© pueden hacer los Transformadores y usaremos nuestra primera herramienta de la librer√≠a ü§ó Transformers: la funci√≥n `pipeline()`.

> [!TIP]
> üëÄ Ves el bot√≥n <em>Open in Colab</em> en la parte superior derecha? Haz clic en √©l para abrir un cuaderno de Google Colab con todos los ejemplos de c√≥digo de esta secci√≥n. Este bot√≥n aparecer√° en cualquier secci√≥n que tenga ejemplos de c√≥digo.
>
> Si quieres ejecutar los ejemplos localmente, te recomendamos revisar la <a href="/course/chapter0">configuraci√≥n</a>.

## ¬°Los Transformadores est√°n en todas partes!

Los Transformadores se usan para resolver todo tipo de tareas de PLN, como las mencionadas en la secci√≥n anterior. Aqu√≠ te mostramos algunas de las compa√±√≠as y organizaciones que usan Hugging Face y Transformadores, que tambi√©n contribuyen de vuelta a la comunidad al compartir sus modelos:

<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/companies.PNG" alt="Companies using Hugging Face" width="100%">

La [librer√≠a ü§ó Transformers](https://github.com/huggingface/transformers) provee la funcionalidad de crear y usar estos modelos compartidos. El [Hub de Modelos](https://huggingface.co/models) contiene miles de modelos preentrenados que cualquiera puede descargar y usar. ¬°T√∫ tambi√©n puedes subir tus propios modelos al Hub!

> [!TIP]
> ‚ö†Ô∏è El Hub de Hugging Face no se limita a Transformadores. ¬°Cualquiera puede compartir los tipos de modelos o conjuntos de datos que quiera! ¬°<a href="https://huggingface.co/join">Crea una cuenta de huggingface.co</a> para beneficiarte de todas las funciones disponibles!

Antes de ver c√≥mo funcionan internamente los Transformadores, veamos un par de ejemplos sobre c√≥mo pueden ser usados para resolver tareas de PLN. 

## Trabajando con pipelines


**Video:** [Ver en YouTube](https://youtu.be/tiZFewofSLM)


El objeto m√°s b√°sico en la librer√≠a ü§ó Transformers es la funci√≥n `pipeline()`. Esta funci√≥n conecta un modelo con los pasos necesarios para su preprocesamiento y posprocesamiento, permiti√©ndonos introducir de manera directa cualquier texto y obtener una respuesta inteligible:

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier("I've been waiting for a HuggingFace course my whole life.")
```

```python out
[{'label': 'POSITIVE', 'score': 0.9598047137260437}]
```

¬°Incluso podemos pasar varias oraciones!

```python
classifier(
    ["I've been waiting for a HuggingFace course my whole life.", "I hate this so much!"]
)
```

```python out
[{'label': 'POSITIVE', 'score': 0.9598047137260437},
 {'label': 'NEGATIVE', 'score': 0.9994558095932007}]
```

Por defecto, este pipeline selecciona un modelo particular preentrenado que ha sido ajustado para el an√°lisis de sentimientos en Ingl√©s. El modelo se descarga y se almacena en el cach√© cuando creas el objeto `classifier`. Si vuelves a ejecutar el comando, se usar√° el modelo almacenado en cach√© y no habr√° necesidad de descargarlo de nuevo.

Hay tres pasos principales que ocurren cuando pasas un texto a un pipeline:

1. El texto es preprocesado en un formato que el modelo puede entender.
2. La entrada preprocesada se pasa al modelo.
3. Las predicciones del modelo son posprocesadas, de tal manera que las puedas entender.

Algunos de los [pipelines disponibles](https://huggingface.co/transformers/main_classes/pipelines.html) son:

- `feature-extraction` (obtener la representaci√≥n vectorial de un texto)
- `fill-mask`
- `ner` (reconocimiento de entidades nombradas)
- `question-answering`
- `sentiment-analysis`
- `summarization`
- `text-generation`
- `translation`
- `zero-shot-classification`

¬°Veamos algunas de ellas!

## Clasificaci√≥n *zero-shot*

Empezaremos abordando una tarea m√°s compleja, en la que necesitamos clasificar textos que no han sido etiquetados. Este es un escenario com√∫n en proyectos de la vida real porque anotar texto usualmente requiere mucho tiempo y dominio del tema. Para este caso de uso, el pipeline `zero-shot-classification` es muy poderoso: permite que especifiques qu√© etiquetas usar para la clasificaci√≥n, para que no dependas de las etiquetas del modelo preentrenado. Ya viste c√≥mo el modelo puede clasificar una oraci√≥n como positiva o negativa usando esas dos etiquetas ‚Äî pero tambi√©n puede clasificar el texto usando cualquier otro conjunto de etiquetas que definas.

```python
from transformers import pipeline

classifier = pipeline("zero-shot-classification")
classifier(
    "This is a course about the Transformers library",
    candidate_labels=["education", "politics", "business"],
)
```

```python out
{'sequence': 'This is a course about the Transformers library',
 'labels': ['education', 'business', 'politics'],
 'scores': [0.8445963859558105, 0.111976258456707, 0.043427448719739914]}
```

Este pipeline se llama _zero-shot_ porque no necesitas ajustar el modelo con tus datos para usarlo. ¬°Puede devolver directamente puntajes de probabilidad para cualquier lista de de etiquetas que escojas!

> [!TIP]
> ‚úèÔ∏è **¬°Pru√©balo!** Juega con tus propias secuencias y etiquetas, y observa c√≥mo se comporta el modelo.


## Generaci√≥n de texto

Ahora veamos c√≥mo usar un pipeline para generar texto. La idea es que proporciones una indicaci√≥n (*prompt*) y el modelo la va a completar autom√°ticamente al generar el texto restante. Esto es parecido a la funci√≥n de texto predictivo que est√° presente en muchos tel√©fonos. La generaci√≥n de texto involucra aleatoriedad, por lo que es normal que no obtengas el mismo resultado que se muestra abajo.

```python
from transformers import pipeline

generator = pipeline("text-generation")
generator("In this course, we will teach you how to")
```

```python out
[{'generated_text': 'In this course, we will teach you how to understand and use '
                    'data flow and data interchange when handling user data. We '
                    'will be working with one or more of the most commonly used '
                    'data flows ‚Äî data flows of various types, as seen by the '
                    'HTTP'}]
```

Puedes controlar cu√°ntas secuencias diferentes se generan con el argumento `num_return_sequences` y la longitud total del texto de salida con el argumento `max_length`.

> [!TIP]
> ‚úèÔ∏è **¬°Pru√©balo!** Usa los argumentos `num_return_sequences` y `max_length` para generar dos oraciones de 15 palabras cada una.


## Usa cualquier modelo del Hub en un pipeline

Los ejemplos anteriores usaban el modelo por defecto para cada tarea, pero tambi√©n puedes escoger un modelo particular del Hub y usarlo en un pipeline para una tarea espec√≠fica - por ejemplo, la generaci√≥n de texto. Ve al [Hub de Modelos](https://huggingface.co/models) y haz clic en la etiqueta correspondiente en la parte izquierda para mostrar √∫nicamente los modelos soportados para esa tarea. Deber√≠as ver una p√°gina [como esta](https://huggingface.co/models?pipeline_tag=text-generation).

¬°Intentemos con el modelo [`distilgpt2`](https://huggingface.co/distilgpt2)! Puedes cargarlo en el mismo pipeline de la siguiente manera:

```python
from transformers import pipeline

generator = pipeline("text-generation", model="distilgpt2")
generator(
    "In this course, we will teach you how to",
    max_length=30,
    num_return_sequences=2,
)
```

```python out
[{'generated_text': 'In this course, we will teach you how to manipulate the world and '
                    'move your mental and physical capabilities to your advantage.'},
 {'generated_text': 'In this course, we will teach you how to become an expert and '
                    'practice realtime, and with a hands on experience on both real '
                    'time and real'}]
```

Puedes refinar tu b√∫squeda de un modelo haciendo clic en las etiquetas de idioma y escoger uno que genere textos en otro idioma. El Hub de Modelos tambi√©n contiene puntos de control (*checkpoints*) para modelos que soportan m√∫ltiples lenguajes.

Una vez has seleccionado un modelo haciendo clic en √©l, ver√°s que hay un widget que te permite probarlo directamente en l√≠nea. De esta manera puedes probar r√°pidamente las capacidades del modelo antes de descargarlo.

> [!TIP]
> ‚úèÔ∏è **¬°Pru√©balo!** Usa los filtros para encontrar un modelo de generaci√≥n de texto para un idioma diferente. ¬°Si√©ntete libre de jugar con el widget y √∫salo en un pipeline!


### La API de Inferencia

Todos los modelos pueden ser probados directamente en tu navegador usando la API de Inferencia, que est√° disponible en el [sitio web](https://huggingface.co/) de Hugging Face. Puedes jugar con el modelo directamente en esta p√°gina al pasar tu texto personalizado como entrada y ver c√≥mo lo procesa.

La API de Inferencia que hace funcionar al widget tambi√©n est√° disponible como un producto pago, algo √∫til si lo necesitas para tus flujos de trabajo. Dir√≠gete a la [p√°gina de precios](https://huggingface.co/pricing) para m√°s detalles.

## Llenado de ocultos (*Mask filling*)

El siguiente pipeline con el que vas a trabajar es `fill-mask`. La idea de esta tarea es llenar los espacios en blanco de un texto dado:

```python
from transformers import pipeline

unmasker = pipeline("fill-mask")
unmasker("This course will teach you all about <mask> models.", top_k=2)
```

```python out
[{'sequence': 'This course will teach you all about mathematical models.',
  'score': 0.19619831442832947,
  'token': 30412,
  'token_str': ' mathematical'},
 {'sequence': 'This course will teach you all about computational models.',
  'score': 0.04052725434303284,
  'token': 38163,
  'token_str': ' computational'}]
```

El argumento `top_k` controla el n√∫mero de posibilidades que se van a mostrar. Nota que en este caso el modelo llena la palabra especial `<mask>`, que se denomina com√∫nmente como *mask token*. Otros modelos pueden tener diferentes tokens, por lo que es una buena idea verificar la palabra especial adecuada cuando est√©s explorando diferentes modelos. Una manera de confirmar es revisar la palabra usada en el widget.

> [!TIP]
> ‚úèÔ∏è **¬°Pru√©balo!** Busca el modelo `bert-base-cased` en el Hub e identifica su *mask token* en el widget de la API de Inferencia. ¬øQu√© predice este modelo para la oraci√≥n que est√° en el ejemplo de `pipeline` anterior?

## Reconocimiento de entidades nombradas

El reconocimiento de entidades nombradas (REN) es una tarea en la que el modelo tiene que encontrar cu√°les partes del texto introducido corresponden a entidades como personas, ubicaciones u organizaciones. Veamos un ejemplo:

```python
from transformers import pipeline

ner = pipeline("ner", grouped_entities=True)
ner("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity_group': 'PER', 'score': 0.99816, 'word': 'Sylvain', 'start': 11, 'end': 18}, 
 {'entity_group': 'ORG', 'score': 0.97960, 'word': 'Hugging Face', 'start': 33, 'end': 45}, 
 {'entity_group': 'LOC', 'score': 0.99321, 'word': 'Brooklyn', 'start': 49, 'end': 57}
]
```

En este caso el modelo identific√≥ correctamente que Sylvain es una persona (PER), Hugging Face una organizaci√≥n (ORG) y Brooklyn una ubicaci√≥n (LOC).

Pasamos la opci√≥n `grouped_entities=True` en la funci√≥n de creaci√≥n del pipeline para decirle que agrupe las partes de la oraci√≥n que corresponden a la misma entidad: Aqu√≠ el modelo agrup√≥ correctamente "Hugging" y "Face" como una sola organizaci√≥n, a pesar de que su nombre est√° compuesto de varias palabras. De hecho, como veremos en el siguiente cap√≠tulo, el preprocesamiento puede incluso dividir palabras en partes m√°s peque√±as. Por ejemplo, 'Sylvain' se separa en cuatro piezas: `S`, `##yl`, `##va` y`##in`. En el paso de prosprocesamiento, el pipeline reagrupa de manera exitosa dichas piezas.

> [!TIP]
> ‚úèÔ∏è **¬°Pru√©balo!** Busca en el Model Hub un modelo capaz de hacer etiquetado *part-of-speech* (que se abrevia usualmente como POS) en Ingl√©s. ¬øQu√© predice este modelo para la oraci√≥n en el ejemplo de arriba?

## Responder preguntas

El pipeline `question-answering` responde preguntas usando informaci√≥n de un contexto dado:

```python
from transformers import pipeline

question_answerer = pipeline("question-answering")
question_answerer(
    question="Where do I work?",
    context="My name is Sylvain and I work at Hugging Face in Brooklyn",
)
```

```python out
{'score': 0.6385916471481323, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}
```

Observa que este pipeline funciona extrayendo informaci√≥n del contexto ofrecido; m√°s no genera la respuesta.

## Resumir (*Summarization*)

Resumir es la tarea de reducir un texto en uno m√°s corto, conservando todos (o la mayor parte de) los aspectos importantes mencionados. Aqu√≠ va un ejemplo: 

```python
from transformers import pipeline

summarizer = pipeline("summarization")
summarizer(
    """
    America has changed dramatically during recent years. Not only has the number of 
    graduates in traditional engineering disciplines such as mechanical, civil, 
    electrical, chemical, and aeronautical engineering declined, but in most of 
    the premier American universities engineering curricula now concentrate on 
    and encourage largely the study of engineering science. As a result, there 
    are declining offerings in engineering subjects dealing with infrastructure, 
    the environment, and related issues, and greater concentration on high 
    technology subjects, largely supporting increasingly complex scientific 
    developments. While the latter is important, it should not be at the expense 
    of more traditional engineering.

    Rapidly developing economies such as China and India, as well as other 
    industrial countries in Europe and Asia, continue to encourage and advance 
    the teaching of engineering. Both China and India, respectively, graduate 
    six and eight times as many traditional engineers as does the United States. 
    Other industrial countries at minimum maintain their output, while America 
    suffers an increasingly serious decline in the number of engineering graduates 
    and a lack of well-educated engineers.
"""
)
```

```python out
[{'summary_text': ' America has changed dramatically during recent years . The '
                  'number of engineering graduates in the U.S. has declined in '
                  'traditional engineering disciplines such as mechanical, civil '
                  ', electrical, chemical, and aeronautical engineering . Rapidly '
                  'developing economies such as China and India, as well as other '
                  'industrial countries in Europe and Asia, continue to encourage '
                  'and advance engineering .'}]
```

Similar a la generaci√≥n de textos, puedes especificar los argumentos `max-length` o `min_length` para definir la longitud del resultado.


## Traducci√≥n

Para la traducci√≥n, puedes usar el modelo por defecto si indicas una pareja de idiomas en el nombre de la tarea (como `"translation_en_to_fr"`), pero la forma m√°s sencilla es escoger el modelo que quieres usar en el [Hub de Modelos](https://huggingface.co/models). Aqu√≠ intentaremos traducir de Franc√©s a Ingl√©s:

```python
from transformers import pipeline

translator = pipeline("translation", model="Helsinki-NLP/opus-mt-fr-en")
translator("Ce cours est produit par Hugging Face.")
```

```python out
[{'translation_text': 'This course is produced by Hugging Face.'}]
```

Al igual que los pipelines de generaci√≥n de textos y resumen, puedes especificar una longitud m√°xima (`max_length`) o m√≠nima (`min_length`) para el resultado.

> [!TIP]
> ‚úèÔ∏è **¬°Pru√©balo!** Busca modelos de traducci√≥n en otros idiomas e intenta traducir la oraci√≥n anterior en varios de ellos.

Los pipelines vistos hasta el momento son principalmente para fines demostrativos. Fueron programados para tareas espec√≠ficas y no pueden desarrollar variaciones de ellas. En el siguiente cap√≠tulo, aprender√°s qu√© est√° detr√°s de una funci√≥n `pipeline()` y c√≥mo personalizar su comportamiento.

---

# ¬øC√≥mo funcionan los Transformadores?


En esta secci√≥n, daremos una mirada de alto nivel a la arquitectura de los Transformadores.

## Un poco de historia sobre los Transformadores

Estos son algunos hitos en la (corta) historia de los Transformadores:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono.svg" alt="A brief chronology of Transformers models.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono-dark.svg" alt="A brief chronology of Transformers models.">
</div>

La [arquitectura de los Transformadores](https://arxiv.org/abs/1706.03762) fue presentada por primera vez en junio de 2017. El trabajo original se enfocaba en tareas de traducci√≥n. A esto le sigui√≥ la introducci√≥n de numerosos modelos influyentes, que incluyen:

- **Junio de 2018**: [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), el primer modelo de Transformadores preentrenados, que fue usado para ajustar varias tareas de PLN y obtuvo resultados de vanguardia

- **Octubre de 2018**: [BERT](https://arxiv.org/abs/1810.04805), otro gran modelo preentrenado, dise√±ado para producir mejores res√∫menes de oraciones (¬°m√°s sobre esto en el siguiente cap√≠tulo!)

- **Febrero de 2019**: [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), una versi√≥n mejorada (y m√°s grande) de GPT, que no se liber√≥ inmediatamente al p√∫blico por consideraciones √©ticas

- **Octubre de 2019**: [DistilBERT](https://arxiv.org/abs/1910.01108), una versi√≥n destilada de BERT que es 60% m√°s r√°pida, 40% m√°s ligera en memoria y que retiene el 97% del desempe√±o de BERT

- **Octubre de 2019**: [BART](https://arxiv.org/abs/1910.13461) y [T5](https://arxiv.org/abs/1910.10683), dos grandes modelos preentrenados usando la misma arquitectura del modelo original de Transformador (los primeros en hacerlo)

- **Mayo de 2020**, [GPT-3](https://arxiv.org/abs/2005.14165), una versi√≥n a√∫n m√°s grande de GPT-2 con buen desempe√±o en una gran variedad de tareas sin la necesidad de ajustes (llamado _zero-shot learning_)

Esta lista est√° lejos de ser exhaustiva y solo pretende resaltar algunos de los diferentes modelos de Transformadores. De manera general, estos pueden agruparse en tres categor√≠as:
- Parecidos a GPT (tambi√©n llamados modelos _auto-regressive_)
- Parecidos a BERT (tambi√©n llamados modelos _auto-encoding_)
- Parecidos a BART/T5 (tambi√©n llamados modelos _sequence-to-sequence_)

Vamos a entrar en estas familias de modelos a profundidad m√°s adelante.

## Los Transformadores son modelos de lenguaje

Todos los modelos de Transformadores mencionados con anterioridad (GPT, BERT, BART, T5, etc.) han sido entrenados como *modelos de lenguaje*. Esto significa que han sido entrenados con grandes cantidades de texto crudo de una manera auto-supervisada. El aprendizaje auto-supervisado es un tipo de entrenamiento en el que el objetivo se computa autom√°ticamente de las entradas del modelo. ¬°Esto significa que no necesitan humanos que etiqueten los datos!

Este tipo de modelos desarrolla un entendimiento estad√≠stico del lenguaje sobre el que fue entrenado, pero no es muy √∫til para tareas pr√°cticas espec√≠ficas. Por lo anterior, el modelo general preentrenado pasa por un proceso llamado *transferencia de aprendizaje* (o *transfer learning* en Ingl√©s). Durante este proceso, el modelo se ajusta de una forma supervisada -- esto es, usando etiquetas hechas por humanos -- para una tarea dada.

Un ejemplo de una tarea es predecir la palabra siguiente en una oraci√≥n con base en las *n* palabras previas. Esto se denomina *modelado de lenguaje causal* porque la salida depende de las entradas pasadas y presentes, pero no en las futuras.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling.svg" alt="Example of causal language modeling in which the next word from a sentence is predicted.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling-dark.svg" alt="Example of causal language modeling in which the next word from a sentence is predicted.">
</div>

Otro ejemplo es el *modelado de lenguaje oculto*, en el que el modelo predice una palabra oculta en la oraci√≥n.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling.svg" alt="Example of masked language modeling in which a masked word from a sentence is predicted.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling-dark.svg" alt="Example of masked language modeling in which a masked word from a sentence is predicted.">
</div>

## Los Transformadores son modelos grandes

Excepto algunos casos at√≠picos (como DistilBERT), la estrategia general para mejorar el desempe√±o es incrementar el tama√±o de los modelos, as√≠ como la cantidad de datos con los que est√°n preentrenados.

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png" alt="Number of parameters of recent Transformers models" width="90%">
</div>

Desafortunadamente, entrenar un modelo, especialmente uno grande, requiere de grandes cantidades de datos. Esto se vuelve muy costoso en t√©rminos de tiempo y recursos de computaci√≥n, que se traduce incluso en impacto ambiental, como se puede ver en la siguiente gr√°fica.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint.svg" alt="The carbon footprint of a large language model.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint-dark.svg" alt="The carbon footprint of a large language model.">
</div>


**Video:** [Ver en YouTube](https://youtu.be/ftWlj4FBHTg)


Esto es ilustrativo para un proyecto que busca un modelo (muy grande), liderado por un equipo que intenta de manera consciente reducir el impacto ambiental del preentrenamiento. La huella de ejecutar muchas pruebas para encontrar los mejores hiperpar√°metros es a√∫n mayor.

Ahora imag√≠nate si cada vez que un equipo de investigaci√≥n, una organizaci√≥n estudiantil o una compa√±√≠a intentaran entrenar un modelo, tuvieran que hacerlo desde cero. ¬°Esto implicar√≠a costos globales enormes e innecesarios!

Esta es la raz√≥n por la que compartir modelos de lenguaje es fundamental: compartir los pesos entrenados y construir sobre los existentes reduce el costo general y la huella de carbono de la comunidad.

## Transferencia de aprendizaje (*Transfer learning*)


**Video:** [Ver en YouTube](https://youtu.be/BqqfQnyjmgg)


El *preentrenamiento* es el acto de entrenar un modelo desde cero: los pesos se inicializan de manera aleatoria y el entrenamiento empieza sin un conocimiento previo.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining.svg" alt="The pretraining of a language model is costly in both time and money.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining-dark.svg" alt="The pretraining of a language model is costly in both time and money.">
</div>

Este preentrenamiento se hace usualmente sobre grandes cantidades de datos. Por lo anterior, requiere un gran corpus de datos y el entrenamiento puede tomar varias semanas.

Por su parte, el ajuste (o *fine-tuning*) es el entrenamiento realizado **despu√©s** de que el modelo ha sido preentrenado. Para hacer el ajuste, comienzas con un modelo de lenguaje preentrenado y luego realizas un aprendizaje adicional con un conjunto de datos espec√≠ficos para tu tarea. Pero entonces -- ¬øpor qu√© no entrenar directamente para la tarea final? Hay un par de razones:

* El modelo preentrenado ya est√° entrenado con un conjunto de datos parecido al conjunto de datos de ajuste. De esta manera, el proceso de ajuste puede hacer uso del conocimiento adquirido por el modelo inicial durante el preentrenamiento (por ejemplo, para problemas de PLN, el modelo preentrenado tendr√° alg√∫n tipo de entendimiento estad√≠stico del idioma que est√°s usando para tu tarea).
* Dado que el modelo preentrenado fue entrenado con muchos datos, el ajuste requerir√° menos datos para tener resultados decentes.
* Por la misma raz√≥n, la cantidad de tiempo y recursos necesarios para tener buenos resultados es mucho menor.

Por ejemplo, se podr√≠a aprovechar un modelo preentrenado en Ingl√©s y despu√©s ajustarlo con un corpus arXiv, teniendo como resultado un modelo basado en investigaci√≥n cient√≠fica. El ajuste solo requerir√° una cantidad limitada de datos: el conocimiento que el modelo preentrenado ha adquirido se "transfiere", de ah√≠ el t√©rmino *transferencia de aprendizaje*.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning.svg" alt="The fine-tuning of a language model is cheaper than pretraining in both time and money.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning-dark.svg" alt="The fine-tuning of a language model is cheaper than pretraining in both time and money.">
</div>

De ese modo, el ajuste de un modelo tendr√° menos costos de tiempo, de datos, financieros y ambientales. Adem√°s es m√°s r√°pido y m√°s f√°cil de iterar en diferentes esquemas de ajuste, dado que el entrenamiento es menos restrictivo que un preentrenamiento completo.

Este proceso tambi√©n conseguir√° mejores resultados que entrenar desde cero (a menos que tengas una gran cantidad de datos), raz√≥n por la cual siempre deber√≠as intentar aprovechar un modelo preentrenado -- uno que est√© tan cerca como sea posible a la tarea respectiva -- y ajustarlo.

## Arquitectura general

En esta secci√≥n, revisaremos la arquitectura general del Transformador. No te preocupes si no entiendes algunos de los conceptos; hay secciones detalladas m√°s adelante para cada uno de los componentes.


**Video:** [Ver en YouTube](https://youtu.be/H39Z_720T5s)


## Introducci√≥n

El modelo est√° compuesto por dos bloques:

* **Codificador (izquierda)**: El codificador recibe una entrada y construye una representaci√≥n de √©sta (sus caracter√≠sticas). Esto significa que el modelo est√° optimizado para conseguir un entendimiento a partir de la entrada.
* **Decodificador (derecha)**: El decodificador usa la representaci√≥n del codificador (caracter√≠sticas) junto con otras entradas para generar una secuencia objetivo. Esto significa que el modelo est√° optimizado para generar salidas.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks.svg" alt="Architecture of a Transformers models">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks-dark.svg" alt="Architecture of a Transformers models">
</div>

Cada una de estas partes puede ser usada de manera independiente, dependiendo de la tarea:

* **Modelos con solo codificadores**: Buenos para las tareas que requieren el entendimiento de la entrada, como la clasificaci√≥n de oraciones y reconocimiento de entidades nombradas.
* **Modelos con solo decodificadores**: Buenos para tareas generativas como la generaci√≥n de textos.
* **Modelos con codificadores y decodificadores** o **Modelos secuencia a secuencia**: Buenos para tareas generativas que requieren una entrada, como la traducci√≥n o resumen.

Vamos a abordar estas arquitecturas de manera independiente en secciones posteriores.

## Capas de atenci√≥n

Una caracter√≠stica clave de los Transformadores es que est√°n construidos con capas especiales llamadas *capas de atenci√≥n*. De hecho, el t√≠tulo del trabajo que introdujo la arquitectura de los Transformadores fue ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762). Vamos a explorar los detalles de las capas de atenci√≥n m√°s adelante en el curso; por ahora, todo lo que tienes que saber es que esta capa va a indicarle al modelo que tiene que prestar especial atenci√≥n a ciertas partes de la oraci√≥n que le pasaste (y m√°s o menos ignorar las dem√°s), cuando trabaje con la representaci√≥n de cada palabra.

Para poner esto en contexto, piensa en la tarea de traducir texto de Ingl√©s a Franc√©s. Dada la entrada "You like this course", un modelo de traducci√≥n necesitar√° tener en cuenta la palabra adyacente "You" para obtener la traducci√≥n correcta de la palabra "like", porque en Franc√©s el verbo "like" se conjuga de manera distinta dependiendo del sujeto. Sin embargo, el resto de la oraci√≥n no es √∫til para la traducci√≥n de esa palabra. En la misma l√≠nea, al traducir "this", el modelo tambi√©n deber√° prestar atenci√≥n a la palabra "course", porque "this" se traduce de manera distinta dependiendo de si el nombre asociado es masculino o femenino. De nuevo, las otras palabras en la oraci√≥n no van a importar para la traducci√≥n de "this". Con oraciones (y reglas gramaticales) m√°s complejas, el modelo deber√° prestar especial atenci√≥n a palabras que pueden aparecer m√°s lejos en la oraci√≥n para traducir correctamente cada palabra.

El mismo concepto aplica para cualquier tarea asociada con lenguaje natural: una palabra por si misma tiene un significado, pero ese significado est√° afectado profundamente por el contexto, que puede ser cualquier palabra (o palabras) antes o despu√©s de la palabra que est√° siendo estudiada.

Ahora que tienes una idea de qu√© son las capas de atenci√≥n, echemos un vistazo m√°s de cerca a la arquitectura del Transformador.

## La arquitectura original

La arquitectura del Transformador fue dise√±ada originalmente para traducci√≥n. Durante el entrenamiento, el codificador recibe entradas (oraciones) en un idioma dado, mientras que el decodificador recibe las mismas oraciones en el idioma objetivo. En el codificador, las capas de atenci√≥n pueden usar todas las palabras en una oraci√≥n (dado que, como vimos, la traducci√≥n de una palabra dada puede ser dependiente de lo que est√° antes y despu√©s en la oraci√≥n). Por su parte, el decodificador trabaja de manera secuencial y s√≥lo le puede prestar atenci√≥n a las palabras en la oraci√≥n que ya ha traducido (es decir, s√≥lo las palabras antes de que la palabra se ha generado). Por ejemplo, cuando hemos predicho las primeras tres palabras del objetivo de traducci√≥n se las damos al decodificador, que luego usa todas las entradas del codificador para intentar predecir la cuarta palabra.

Para acelerar el entrenamiento (cuando el modelo tiene acceso a las oraciones objetivo), al decodificador se le alimenta el objetivo completo, pero no puede usar palabras futuras (si tuviera acceso a la palabra en la posici√≥n 2 cuando trata de predecir la palabra en la posici√≥n 2, ¬°el problema no ser√≠a muy dif√≠cil!). Por ejemplo, al intentar predecir la cuarta palabra, la capa de atenci√≥n s√≥lo tendr√≠a acceso a las palabras en las posiciones 1 a 3.

La arquitectura original del Transformador se ve√≠a as√≠, con el codificador a la izquierda y el decodificador a la derecha:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg" alt="Architecture of a Transformers models">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers-dark.svg" alt="Architecture of a Transformers models">
</div>

Observa que la primera capa de atenci√≥n en un bloque de decodificador presta atenci√≥n a todas las entradas (pasadas) al decodificador, mientras que la segunda capa de atenci√≥n usa la salida del codificador. De esta manera puede acceder a toda la oraci√≥n de entrada para predecir de mejor manera la palabra actual. Esto es muy √∫til dado que diferentes idiomas pueden tener reglas gramaticales que ponen las palabras en orden distinto o alg√∫n contexto que se provee despu√©s puede ser √∫til para determinar la mejor traducci√≥n de una palabra dada.

La *m√°scara de atenci√≥n* tambi√©n se puede usar en el codificador/decodificador para evitar que el modelo preste atenci√≥n a algunas palabras especiales --por ejemplo, la palabra especial de relleno que hace que todas las entradas sean de la misma longitud cuando se agrupan oraciones.

##  Arquitecturas vs. puntos de control

A medida que estudiemos a profundidad los Transformadores, ver√°s menciones a *arquitecturas*, *puntos de control* (*checkpoints*) y *modelos*. Estos t√©rminos tienen significados ligeramente diferentes:

* **Arquitecturas**: Este es el esqueleto del modelo -- la definici√≥n de cada capa y cada operaci√≥n que sucede al interior del modelo.
* **Puntos de control**: Estos son los pesos que ser√°n cargados en una arquitectura dada.
* **Modelo**: Esta es un t√©rmino sombrilla que no es tan preciso como "arquitectura" o "punto de control" y puede significar ambas cosas. Este curso especificar√° *arquitectura* o *punto de control* cuando sea relevante para evitar ambig√ºedades.

Por ejemplo, mientras que BERT es una arquitectura, `bert-base-cased` - un conjunto de pesos entrenados por el equipo de Google para la primera versi√≥n de BERT - es un punto de control. Sin embargo, se podr√≠a decir "el modelo BERT" y "el modelo `bert-base-cased`". 


---

# Modelos de codificadores


**Video:** [Ver en YouTube](https://youtu.be/MUqNwgPjJvQ)


Los modelos de codificadores usan √∫nicamente el codificador del Transformador. En cada etapa, las capas de atenci√≥n pueden acceder a todas las palabras de la oraci√≥n inicial. Estos modelos se caracterizan generalmente por tener atenci√≥n "bidireccional" y se suelen llamar modelos *auto-encoding*.

El preentrenamiento de estos modelos generalmente gira en torno a corromper de alguna manera una oraci√≥n dada (por ejemplo, ocultando aleatoriamente palabras en ella) y pidi√©ndole al modelo que encuentre o reconstruya la oraci√≥n inicial.

Los modelos de codificadores son m√°s adecuados para tareas que requieren un entendimiento de la oraci√≥n completa, como la clasificaci√≥n de oraciones, reconocimiento de entidades nombradas (y m√°s generalmente clasificaci√≥n de palabras) y respuesta extractiva a preguntas.

Los miembros de esta familia de modelos incluyen:

- [ALBERT](https://huggingface.co/transformers/model_doc/albert.html)
- [BERT](https://huggingface.co/transformers/model_doc/bert.html)
- [DistilBERT](https://huggingface.co/transformers/model_doc/distilbert.html)
- [ELECTRA](https://huggingface.co/transformers/model_doc/electra.html)
- [RoBERTa](https://huggingface.co/transformers/model_doc/roberta.html)


---

# Modelos de decodificadores


**Video:** [Ver en YouTube](https://youtu.be/d_ixlCubqQw)


Los modelos de decodificadores usan √∫nicamente el decodificador del Transformador. En cada etapa, para una palabra dada las capas de atenci√≥n pueden acceder solamente a las palabras que se ubican antes en la oraci√≥n. Estos modelos se suelen llamar modelos *auto-regressive*.

El preentrenamiento de los modelos de decodificadores generalmente gira en torno a la predicci√≥n de la siguiente palabra en la oraci√≥n.

Estos modelos son m√°s adecuados para tareas que implican la generaci√≥n de texto.

Los miembros de esta familia de modelos incluyen:

- [CTRL](https://huggingface.co/transformers/model_doc/ctrl.html)
- [GPT](https://huggingface.co/docs/transformers/model_doc/openai-gpt)
- [GPT-2](https://huggingface.co/transformers/model_doc/gpt2.html)
- [Transformer XL](https://huggingface.co/transformers/model_doc/transformerxl.html)


---

#¬†Modelos secuencia a secuencia


**Video:** [Ver en YouTube](https://youtu.be/0_4KEb08xrE)


Los modelos codificador/decodificador (tambi√©n llamados *modelos secuencia a secuencia*) usan ambas partes de la arquitectura del Transformador. En cada etapa, las capas de atenci√≥n del codificador pueden acceder a todas las palabras de la secuencia inicial, mientras que las capas de atenci√≥n del decodificador s√≥lo pueden acceder a las palabras que se ubican antes de una palabra dada en el texto de entrada.

El preentrenamiento de estos modelos se puede hacer usando los objetivos de los modelos de codificadores o decodificadores, pero usualmente implican algo m√°s complejo. Por ejemplo, [T5](https://huggingface.co/t5-base) est√° preentrenado al reemplazar segmentos aleatorios de texto (que pueden contener varias palabras) con una palabra especial que las oculta, y el objetivo es predecir el texto que esta palabra reemplaza.

Los modelos secuencia a secuencia son m√°s adecuados para tareas relacionadas con la generaci√≥n de nuevas oraciones dependiendo de una entrada dada, como resumir, traducir o responder generativamente preguntas.

Algunos miembros de esta familia de modelos son:

- [BART](https://huggingface.co/transformers/model_doc/bart.html)
- [mBART](https://huggingface.co/transformers/model_doc/mbart.html)
- [Marian](https://huggingface.co/transformers/model_doc/marian.html)
- [T5](https://huggingface.co/transformers/model_doc/t5.html)


---

# Sesgos y limitaciones


Si tu intenci√≥n es usar modelos preentrenados o una versi√≥n ajustada en producci√≥n, ten en cuenta que a pesar de ser herramientas poderosas, tienen limitaciones. La m√°s importante de ellas es que, para permitir el preentrenamiento con grandes cantidades de datos, los investigadores suelen *raspar* (*scrape*) todo el contenido que puedan encontrar, tomando lo mejor y lo peor que est√° disponible en internet.

Para dar un ejemplo r√°pido, volvamos al caso del pipeline `fill-mask` con el modelo BERT:

```python
from transformers import pipeline

unmasker = pipeline("fill-mask", model="bert-base-uncased")
result = unmasker("This man works as a [MASK].")
print([r["token_str"] for r in result])

result = unmasker("This woman works as a [MASK].")
print([r["token_str"] for r in result])
```

```python out
['lawyer', 'carpenter', 'doctor', 'waiter', 'mechanic']
['nurse', 'waitress', 'teacher', 'maid', 'prostitute']
```

Cuando se le pide llenar la palabra faltante en estas dos oraciones, el modelo devuelve solo una respuesta agn√≥stica de g√©nero (*waiter/waitress*). Las otras son ocupaciones que se suelen asociar con un g√©nero espec√≠fico -- y si, prostituta es una de las primeras 5 posibilidades que el modelo asocia con "mujer" y "trabajo". Esto sucede a pesar de que BERT es uno de los pocos modelos de Transformadores que no se construyeron basados en datos *raspados* de todo el internet, pero usando datos aparentemente neutrales (est√° entrenado con los conjuntos de datos de [Wikipedia en Ingl√©s](https://huggingface.co/datasets/wikipedia) y [BookCorpus](https://huggingface.co/datasets/bookcorpus)).

Cuando uses estas herramientas, debes tener en cuenta que el modelo original que est√°s usando puede muy f√°cilmente generar contenido sexista, racista u hom√≥fobo. Ajustar el modelo con tus datos no va a desaparecer este sesgo intr√≠nseco.


---

# Resumen


En este cap√≠tulo viste c√≥mo abordar diferentes tareas de PLN usando la funci√≥n de alto nivel `pipeline()` de ü§ó Transformers. Tambi√©n viste como buscar modelos en el Hub, as√≠ como usar la API de Inferencia para probar los modelos directamente en tu navegador.

Discutimos brevemente el funcionamiento de los Transformadores y hablamos sobre la importancia de la transferencia de aprendizaje y el ajuste. Un aspecto clave es que puedes usar la arquitectura completa o s√≥lo el codificador o decodificador, dependiendo de qu√© tipo de tarea quieres resolver. La siguiente tabla resume lo anterior:

| Modelo                    | Ejemplos                                   | Tareas                                                                                              |
|---------------------------|--------------------------------------------|-----------------------------------------------------------------------------------------------------|
| Codificador               | ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa | Clasificaci√≥n de oraciones, reconocimiento de entidades nombradas, respuesta extractiva a preguntas |
| Decodificador             | CTRL, GPT, GPT-2, Transformer XL           | Generaci√≥n de texto                                                                                 |
| Codificador-decodificador | BART, T5, Marian, mBART                    | Resumen, traducci√≥n, respuesta generativa a preguntas                                               |


---



# Quiz de final de cap√≠tulo


¬°Este cap√≠tulo cubri√≥ una gran variedad de temas! No te preocupes si no entendiste todos los detalles; los siguientes cap√≠tulos te ayudar√°n a entender c√≥mo funcionan las cosas detr√°s de c√°maras.

Por ahora, ¬°revisemos lo que aprendiste en este cap√≠tulo!

### 1. Explora el Hub y busca el punto de control `roberta-large-mnli`. ¬øQu√© tarea desarrolla?


- Resumen
- Clasificaci√≥n de texto
- Generaci√≥n de texto


### 2. ¬øQu√© devuelve el siguiente c√≥digo?

```py
from transformers import pipeline

ner = pipeline("ner", grouped_entities=True)
ner("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```


- Devuelve los puntajes de clasificaci√≥n de esta oraci√≥n, con las etiquetas \
- Devuelve un texto generado que completa esta oraci√≥n.
- Devuelve las palabras que representan personas, organizaciones o ubicaciones.


### 3. ¬øQu√© deber√≠a reemplazar ... en este ejemplo de c√≥digo?

```py
from transformers import pipeline

filler = pipeline("fill-mask", model="bert-base-cased")
result = filler("...")
```


- This &#60;mask> has been waiting for you.
- This [MASK] has been waiting for you.
- This man has been waiting for you.


### 4. ¬øPor qu√© fallar√° este c√≥digo?

```py
from transformers import pipeline

classifier = pipeline("zero-shot-classification")
result = classifier("This is a course about the Transformers library")
```


- Este pipeline necesita que se le indiquen etiquetas para clasificar el texto.
- Este pipeline requiere varias oraciones, no s√≥lo una.
- La librer√≠a ü§ó Transformers est√° da√±ada, como siempre.
- Este pipeline necesita entradas m√°s largas; esta oraci√≥n es muy corta.


### 5. ¬øQu√© significa "transferencia de aprendizaje"?


- Transferir el conocimiento de un modelo preentrenado a un nuevo modelo, al entrenarlo en el mismo conjunto de datos.
- Transferir el conocimiento de un modelo preentrenado a un nuevo modelo al inicializar un segundo modelo con los pesos del primero.
- Transferir el conocimiento de un modelo preentrenado al construir el segundo modelo con la misma arquitectura del primero.


### 6. ¬øVerdadero o falso? Un modelo de lenguaje usualmente no necesita etiquetas para su preentrenamiento.


- Verdadero
- Falso


### 7. Selecciona la oraci√≥n que describe mejor los t√©rminos "modelo", "arquitectura" y "pesos".


- Si un modelo es un edificio, su arquitectura es el plano y los pesos son las personas que viven all√≠.
- Una arquitectura es un mapa para construir un modelo y sus pesos son las ciudades representadas en el mapa.
- Una arquitectura es una sucesi√≥n de funciones matem√°ticas para construir un modelo y sus pesos son los par√°metros de dichas funciones.


### 8. ¬øCu√°l de los siguientes tipos de modelos usar√≠as para completar una indicaci√≥n con texto generado?


- Un modelo de codificadores
- Un modelo de decodificadores
- Un modelo secuencia a secuencia


### 9. ¬øCu√°l de los siguientes tipos de modelos usar√≠as para resumir textos?


- Un modelo de codificadores
- Un modelo de decodificadores
- Un modelo secuencia a secuencia


### 10. ¬øCu√°l de los siguientes tipos de modelos usar√≠as para clasificar texto de acuerdo con ciertas etiquetas?


- Un modelo de codificadores
- Un modelo de decodificadores
- Un modelo secuencia a secuencia


### 11. ¬øCu√°l puede ser una posible fuente del sesgo observado en un modelo?


- El modelo es una versi√≥n ajustada de un modelo preentrenado y tom√≥ el sesgo a partir de all√≠.
- Los datos con los que se entren√≥ el modelo est√°n sesgados.
- La m√©trica que el modelo estaba optimizando est√° sesgada.



---

# Es hora del examen!

Es hora de poner a prueba tus conocimientos! Hemos preparado un breve cuestionario para evaluar tu comprension de los conceptos cubiertos en este capitulo.

Para realizar el cuestionario, deberas seguir estos pasos:

1. Inicia sesion en tu cuenta de Hugging Face.
2. Responde las preguntas del cuestionario.
3. Envia tus respuestas.


## Cuestionario de opcion multiple

En este cuestionario, se te pedira que selecciones la respuesta correcta de una lista de opciones. Te evaluaremos sobre los fundamentos del ajuste fino supervisado.

<iframe
	src="https://huggingface-course-chapter-1-exam.hf.space"
	frameborder="0"
	width="850"
	height="450"
></iframe>


---

# 2. Usando ü§ó Transformers

# Introduccion[[introduction]]


Como viste en el [Capitulo 1](/course/chapter1), los modelos Transformer suelen ser muy grandes. Con millones a decenas de *miles de millones* de parametros, entrenar y desplegar estos modelos es una tarea complicada. Ademas, con nuevos modelos siendo lanzados casi a diario y cada uno con su propia implementacion, probarlos todos no es tarea facil.

La biblioteca ü§ó Transformers fue creada para resolver este problema. Su objetivo es proporcionar una API unica a traves de la cual cualquier modelo Transformer pueda ser cargado, entrenado y guardado. Las principales caracteristicas de la biblioteca son:

- **Facilidad de uso**: Descargar, cargar y usar un modelo de NLP de ultima generacion para inferencia se puede hacer en solo dos lineas de codigo.
- **Flexibilidad**: En su nucleo, todos los modelos son clases simples de PyTorch `nn.Module` y pueden ser manejados como cualquier otro modelo en sus respectivos frameworks de machine learning (ML).
- **Simplicidad**: Casi no se hacen abstracciones en toda la biblioteca. El concepto "Todo en un archivo" es fundamental: el forward pass de un modelo esta definido completamente en un solo archivo, de modo que el codigo mismo sea comprensible y modificable.

Esta ultima caracteristica hace que ü§ó Transformers sea bastante diferente de otras bibliotecas de ML. Los modelos no estan construidos sobre modulos que se comparten entre archivos; en su lugar, cada modelo tiene sus propias capas. Ademas de hacer los modelos mas accesibles y comprensibles, esto te permite experimentar facilmente con un modelo sin afectar a otros.

Este capitulo comenzara con un ejemplo de principio a fin donde usamos un modelo y un tokenizador juntos para replicar la funcion `pipeline()` introducida en el [Capitulo 1](/course/chapter1). A continuacion, discutiremos la API del modelo: profundizaremos en las clases de modelo y configuracion, y te mostraremos como cargar un modelo y como procesa entradas numericas para producir predicciones.

Luego veremos la API del tokenizador, que es el otro componente principal de la funcion `pipeline()`. Los tokenizadores se encargan del primer y ultimo paso del procesamiento, manejando la conversion de texto a entradas numericas para la red neuronal, y la conversion de vuelta a texto cuando es necesario. Finalmente, te mostraremos como manejar el envio de multiples oraciones a traves de un modelo en un lote preparado, y luego cerraremos con una mirada mas cercana a la funcion de alto nivel `tokenizer()`.

> [!TIP]
> ‚ö†Ô∏è Para beneficiarte de todas las funciones disponibles con el Model Hub y ü§ó Transformers, te recomendamos <a href="https://huggingface.co/join">crear una cuenta</a>.


---



# Detras del pipeline[[behind-the-pipeline]]


**Video:** [Ver en YouTube](https://youtu.be/1pedAIvTWXk)


Comencemos con un ejemplo completo, observando lo que sucedio detras de escena cuando ejecutamos el siguiente codigo en el [Capitulo 1](/course/chapter1):

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier(
    [
        "I've been waiting for a HuggingFace course my whole life.",
        "I hate this so much!",
    ]
)
```

y obtuvimos:

```python out
[{'label': 'POSITIVE', 'score': 0.9598047137260437},
 {'label': 'NEGATIVE', 'score': 0.9994558095932007}]
```

Como vimos en el [Capitulo 1](/course/chapter1), este pipeline agrupa tres pasos: preprocesamiento, pasar las entradas por el modelo y postprocesamiento:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg" alt="El pipeline completo de NLP: tokenizacion del texto, conversion a IDs, e inferencia a traves del modelo Transformer y la cabeza del modelo."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline-dark.svg" alt="El pipeline completo de NLP: tokenizacion del texto, conversion a IDs, e inferencia a traves del modelo Transformer y la cabeza del modelo."/>
</div>

Repasemos rapidamente cada uno de estos.

## Preprocesamiento con un tokenizador[[preprocessing-with-a-tokenizer]]

Como otras redes neuronales, los modelos Transformer no pueden procesar texto crudo directamente, por lo que el primer paso de nuestro pipeline es convertir las entradas de texto en numeros que el modelo pueda entender. Para hacer esto usamos un *tokenizador*, que sera responsable de:

- Dividir la entrada en palabras, subpalabras o simbolos (como signos de puntuacion) que se llaman *tokens*
- Mapear cada token a un entero
- Agregar entradas adicionales que puedan ser utiles para el modelo

Todo este preprocesamiento debe hacerse exactamente de la misma manera que cuando el modelo fue preentrenado, por lo que primero necesitamos descargar esa informacion desde el [Model Hub](https://huggingface.co/models). Para hacer esto, usamos la clase `AutoTokenizer` y su metodo `from_pretrained()`. Usando el nombre del checkpoint de nuestro modelo, automaticamente obtendra los datos asociados con el tokenizador del modelo y los almacenara en cache (para que solo se descarguen la primera vez que ejecutes el codigo a continuacion).

Como el checkpoint predeterminado del pipeline `sentiment-analysis` es `distilbert-base-uncased-finetuned-sst-2-english` (puedes ver su tarjeta de modelo [aqui](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)), ejecutamos lo siguiente:

```python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

Una vez que tenemos el tokenizador, podemos pasarle directamente nuestras oraciones y obtendremos un diccionario listo para alimentar a nuestro modelo! Lo unico que queda por hacer es convertir la lista de IDs de entrada a tensores.

Puedes usar ü§ó Transformers sin preocuparte por que framework de ML se usa como backend; podria ser PyTorch o Flax para algunos modelos. Sin embargo, los modelos Transformer solo aceptan *tensores* como entrada. Si es la primera vez que escuchas sobre tensores, puedes pensar en ellos como arreglos de NumPy. Un arreglo de NumPy puede ser un escalar (0D), un vector (1D), una matriz (2D), o tener mas dimensiones. Efectivamente es un tensor; los tensores de otros frameworks de ML se comportan de manera similar, y generalmente son tan simples de instanciar como los arreglos de NumPy.

Para especificar el tipo de tensores que queremos obtener (PyTorch o NumPy simple), usamos el argumento `return_tensors`:

```python
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)
```

No te preocupes por el padding y truncation todavia; los explicaremos mas adelante. Lo principal a recordar aqui es que puedes pasar una oracion o una lista de oraciones, asi como especificar el tipo de tensores que quieres obtener (si no se pasa ningun tipo, obtendras una lista de listas como resultado).

Asi es como se ven los resultados como tensores de PyTorch:

```python out
{
    'input_ids': tensor([
        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
    ]),
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
}
```

La salida en si es un diccionario que contiene dos claves, `input_ids` y `attention_mask`. `input_ids` contiene dos filas de enteros (una para cada oracion) que son los identificadores unicos de los tokens en cada oracion. Explicaremos que es `attention_mask` mas adelante en este capitulo.

## Pasando por el modelo[[going-through-the-model]]

Podemos descargar nuestro modelo preentrenado de la misma manera que lo hicimos con nuestro tokenizador. ü§ó Transformers proporciona una clase `AutoModel` que tambien tiene un metodo `from_pretrained()`:

```python
from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)
```

En este fragmento de codigo, hemos descargado el mismo checkpoint que usamos en nuestro pipeline antes (en realidad ya deberia estar en cache) e instanciamos un modelo con el.

Esta arquitectura contiene solo el modulo Transformer base: dadas algunas entradas, produce lo que llamaremos *estados ocultos*, tambien conocidos como *caracteristicas*. Para cada entrada del modelo, recuperaremos un vector de alta dimension que representa la **comprension contextual de esa entrada por el modelo Transformer**.

Si esto no tiene sentido, no te preocupes. Lo explicaremos todo mas adelante.

Aunque estos estados ocultos pueden ser utiles por si mismos, usualmente son entradas a otra parte del modelo, conocida como la *cabeza*. En el [Capitulo 1](/course/chapter1), las diferentes tareas podrian haber sido realizadas con la misma arquitectura, pero cada una de estas tareas tendra una cabeza diferente asociada.

### Un vector de alta dimension?[[a-high-dimensional-vector]]

El vector producido por el modulo Transformer es usualmente grande. Generalmente tiene tres dimensiones:

- **Tamano del lote**: El numero de secuencias procesadas a la vez (2 en nuestro ejemplo).
- **Longitud de secuencia**: La longitud de la representacion numerica de la secuencia (16 en nuestro ejemplo).
- **Tamano oculto**: La dimension del vector de cada entrada del modelo.

Se dice que es de "alta dimension" por el ultimo valor. El tamano oculto puede ser muy grande (768 es comun para modelos mas pequenos, y en modelos mas grandes esto puede alcanzar 3072 o mas).

Podemos ver esto si alimentamos las entradas que preprocesamos a nuestro modelo:

```python
outputs = model(**inputs)
print(outputs.last_hidden_state.shape)
```

```python out
torch.Size([2, 16, 768])
```

Nota que las salidas de los modelos ü§ó Transformers se comportan como `namedtuple`s o diccionarios. Puedes acceder a los elementos por atributos (como hicimos) o por clave (`outputs["last_hidden_state"]`), o incluso por indice si sabes exactamente donde esta lo que buscas (`outputs[0]`).

### Cabezas de modelo: Dando sentido a los numeros[[model-heads-making-sense-out-of-numbers]]

Las cabezas del modelo toman el vector de alta dimension de estados ocultos como entrada y los proyectan a una dimension diferente. Usualmente estan compuestas de una o pocas capas lineales:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg" alt="Una red Transformer junto con su cabeza."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head-dark.svg" alt="Una red Transformer junto con su cabeza."/>
</div>

La salida del modelo Transformer se envia directamente a la cabeza del modelo para ser procesada.

En este diagrama, el modelo esta representado por su capa de embeddings y las capas subsiguientes. La capa de embeddings convierte cada ID de entrada en la entrada tokenizada a un vector que representa el token asociado. Las capas subsiguientes manipulan esos vectores usando el mecanismo de atencion para producir la representacion final de las oraciones.

Hay muchas arquitecturas diferentes disponibles en ü§ó Transformers, con cada una disenada para abordar una tarea especifica. Aqui hay una lista no exhaustiva:

- `*Model` (recupera los estados ocultos)
- `*ForCausalLM`
- `*ForMaskedLM`
- `*ForMultipleChoice`
- `*ForQuestionAnswering`
- `*ForSequenceClassification`
- `*ForTokenClassification`
- y otros ü§ó

Para nuestro ejemplo, necesitaremos un modelo con una cabeza de clasificacion de secuencias (para poder clasificar las oraciones como positivas o negativas). Entonces, no usaremos realmente la clase `AutoModel`, sino `AutoModelForSequenceClassification`:

```python
from transformers import AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)
```

Ahora si miramos la forma de nuestras salidas, la dimensionalidad sera mucho menor: la cabeza del modelo toma como entrada los vectores de alta dimension que vimos antes, y produce vectores que contienen dos valores (uno por etiqueta):

```python
print(outputs.logits.shape)
```

```python out
torch.Size([2, 2])
```

Como solo tenemos dos oraciones y dos etiquetas, el resultado que obtenemos de nuestro modelo tiene forma 2 x 2.

## Postprocesamiento de la salida[[postprocessing-the-output]]

Los valores que obtenemos como salida de nuestro modelo no necesariamente tienen sentido por si mismos. Echemos un vistazo:

```python
print(outputs.logits)
```

```python out
tensor([[-1.5607,  1.6123],
        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)
```

Nuestro modelo predijo `[-1.5607, 1.6123]` para la primera oracion y `[ 4.1692, -3.3464]` para la segunda. Esos no son probabilidades sino *logits*, las puntuaciones crudas, no normalizadas, producidas por la ultima capa del modelo. Para ser convertidos a probabilidades, necesitan pasar por una capa [SoftMax](https://es.wikipedia.org/wiki/Funci%C3%B3n_SoftMax) (todos los modelos ü§ó Transformers producen logits, ya que la funcion de perdida para el entrenamiento generalmente fusionara la ultima funcion de activacion, como SoftMax, con la funcion de perdida real, como la entropia cruzada):

```py
import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)
```

```python out
tensor([[4.0195e-02, 9.5980e-01],
        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)
```

Ahora podemos ver que el modelo predijo `[0.0402, 0.9598]` para la primera oracion y `[0.9995, 0.0005]` para la segunda. Estas son puntuaciones de probabilidad reconocibles.

Para obtener las etiquetas correspondientes a cada posicion, podemos inspeccionar el atributo `id2label` de la configuracion del modelo (mas sobre esto en la siguiente seccion):

```python
model.config.id2label
```

```python out
{0: 'NEGATIVE', 1: 'POSITIVE'}
```

Ahora podemos concluir que el modelo predijo lo siguiente:

- Primera oracion: NEGATIVE: 0.0402, POSITIVE: 0.9598
- Segunda oracion: NEGATIVE: 0.9995, POSITIVE: 0.0005

Hemos reproducido exitosamente los tres pasos del pipeline: preprocesamiento con tokenizadores, pasar las entradas por el modelo y postprocesamiento! Ahora tomemos algo de tiempo para profundizar en cada uno de esos pasos.

> [!TIP]
> ‚úèÔ∏è **Pruebalo!** Elige dos (o mas) textos propios y pasalos por el pipeline de `sentiment-analysis`. Luego replica los pasos que viste aqui tu mismo y verifica que obtienes los mismos resultados!


---



# Modelos[[the-models]]


**Video:** [Ver en YouTube](https://youtu.be/AhChOFRegn4)


En esta seccion, examinaremos mas de cerca la creacion y el uso de modelos. Usaremos la clase `AutoModel`, que es practica cuando quieres instanciar cualquier modelo desde un checkpoint.

## Creando un Transformer[[creating-a-transformer]]

Comencemos examinando lo que sucede cuando instanciamos un `AutoModel`:

```py
from transformers import AutoModel

model = AutoModel.from_pretrained("bert-base-cased")
```

Similar al tokenizador, el metodo `from_pretrained()` descargara y almacenara en cache los datos del modelo desde Hugging Face Hub. Como se menciono anteriormente, el nombre del checkpoint corresponde a una arquitectura y pesos de modelo especificos, en este caso un modelo BERT con una arquitectura basica (12 capas, 768 de tamano oculto, 12 cabezas de atencion) y entradas sensibles a mayusculas/minusculas (lo que significa que la distincion entre mayusculas y minusculas es importante). Hay muchos checkpoints disponibles en el Hub ‚Äî puedes explorarlos [aqui](https://huggingface.co/models).

La clase `AutoModel` y sus asociadas son en realidad simples envoltorios disenados para obtener la arquitectura de modelo apropiada para un checkpoint dado. Es una clase "auto" que significa que adivinara la arquitectura de modelo apropiada para ti e instanciara la clase de modelo correcta. Sin embargo, si conoces el tipo de modelo que quieres usar, puedes usar la clase que define su arquitectura directamente:

```py
from transformers import BertModel

model = BertModel.from_pretrained("bert-base-cased")
```

## Cargando y guardando[[loading-and-saving]]

Guardar un modelo es tan simple como guardar un tokenizador. De hecho, los modelos tienen el mismo metodo `save_pretrained()`, que guarda los pesos y la configuracion de la arquitectura del modelo:

```py
model.save_pretrained("directory_on_my_computer")
```

Esto guardara dos archivos en tu disco:

```
ls directory_on_my_computer

config.json model.safetensors
```

Si miras dentro del archivo *config.json*, veras todos los atributos necesarios para construir la arquitectura del modelo. Este archivo tambien contiene algunos metadatos, como de donde se origino el checkpoint y que version de ü§ó Transformers estabas usando cuando guardaste el checkpoint por ultima vez.

El archivo *pytorch_model.safetensors* se conoce como el diccionario de estado; contiene todos los pesos de tu modelo. Los dos archivos trabajan juntos: el archivo de configuracion es necesario para conocer la arquitectura del modelo, mientras que los pesos del modelo son los parametros del modelo.

Para reutilizar un modelo guardado, usa el metodo `from_pretrained()` nuevamente:

```py
from transformers import AutoModel

model = AutoModel.from_pretrained("directory_on_my_computer")
```

Una caracteristica maravillosa de la biblioteca ü§ó Transformers es la capacidad de compartir facilmente modelos y tokenizadores con la comunidad. Para hacer esto, asegurate de tener una cuenta en [Hugging Face](https://huggingface.co). Si estas usando un notebook, puedes iniciar sesion facilmente con esto:

```python
from huggingface_hub import notebook_login

notebook_login()
```

De lo contrario, en tu terminal ejecuta:

```bash
huggingface-cli login
```

Luego puedes subir el modelo al Hub con el metodo `push_to_hub()`:

```py
model.push_to_hub("my-awesome-model")
```

Esto subira los archivos del modelo al Hub, en un repositorio bajo tu espacio de nombres llamado *my-awesome-model*. Entonces, cualquiera puede cargar tu modelo con el metodo `from_pretrained()`!

```py
from transformers import AutoModel

model = AutoModel.from_pretrained("your-username/my-awesome-model")
```

Puedes hacer mucho mas con la API del Hub:
- Subir un modelo desde un repositorio local
- Actualizar archivos especificos sin resubir todo
- Agregar tarjetas de modelo para documentar las capacidades, limitaciones, sesgos conocidos del modelo, etc.

Consulta [la documentacion](https://huggingface.co/docs/huggingface_hub/how-to-upstream) para un tutorial completo sobre esto, o revisa el [Capitulo 4](/course/chapter4) avanzado.

## Codificando texto[[encoding-text]]

Los modelos Transformer manejan texto convirtiendolo a numeros. Aqui veremos exactamente lo que sucede cuando tu texto es procesado por el tokenizador. Ya hemos visto en el [Capitulo 1](/course/chapter1) que los tokenizadores dividen el texto en tokens y luego convierten estos tokens en numeros. Podemos ver esta conversion a traves de un tokenizador simple:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

encoded_input = tokenizer("Hello, I'm a single sentence!")
print(encoded_input)
```

```python out
{'input_ids': [101, 8667, 117, 1000, 1045, 1005, 1049, 2235, 17662, 12172, 1012, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

Obtenemos un diccionario con los siguientes campos:
- input_ids: representaciones numericas de tus tokens
- token_type_ids: estos le dicen al modelo que parte de la entrada es la oracion A y cual es la oracion B (se discute mas en la siguiente seccion)
- attention_mask: esto indica que tokens deben ser atendidos y cuales no (se discute mas adelante)

Podemos decodificar los IDs de entrada para obtener el texto original:

```py
tokenizer.decode(encoded_input["input_ids"])
```

```python out
"[CLS] Hello, I'm a single sentence! [SEP]"
```

Notaras que el tokenizador ha agregado tokens especiales ‚Äî `[CLS]` y `[SEP]` ‚Äî requeridos por el modelo. No todos los modelos necesitan tokens especiales; se utilizan cuando un modelo fue preentrenado con ellos, en cuyo caso el tokenizador necesita agregarlos ya que el modelo los espera.

Puedes codificar multiples oraciones a la vez, ya sea agrupandolas juntas (lo discutiremos pronto) o pasando una lista:

```py
encoded_input = tokenizer("How are you?", "I'm fine, thank you!")
print(encoded_input)
```

```python out
{'input_ids': [[101, 1731, 1132, 1128, 136, 102], [101, 1045, 1005, 1049, 2503, 117, 5763, 1128, 136, 102]],
 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}
```

Nota que al pasar multiples oraciones, el tokenizador devuelve una lista para cada oracion por cada valor del diccionario. Tambien podemos pedirle al tokenizador que devuelva tensores directamente de PyTorch:

```py
encoded_input = tokenizer("How are you?", "I'm fine, thank you!", return_tensors="pt")
print(encoded_input)
```

```python out
{'input_ids': tensor([[  101,  1731,  1132,  1128,   136,   102],
         [  101,  1045,  1005,  1049,  2503,   117,  5763,  1128,   136,   102]]),
 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),
 'attention_mask': tensor([[1, 1, 1, 1, 1, 1],
         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}
```

Pero hay un problema: las dos listas no tienen la misma longitud! Los arreglos y tensores necesitan ser rectangulares, por lo que no podemos simplemente convertir estas listas a un tensor de PyTorch (o arreglo de NumPy). El tokenizador proporciona una opcion para eso: padding.

### Rellenando entradas[[padding-inputs]]

Si le pedimos al tokenizador que rellene las entradas, hara que todas las oraciones tengan la misma longitud agregando un token de relleno especial a las oraciones que son mas cortas que la mas larga:

```py
encoded_input = tokenizer(
    ["How are you?", "I'm fine, thank you!"], padding=True, return_tensors="pt"
)
print(encoded_input)
```

```python out
{'input_ids': tensor([[  101,  1731,  1132,  1128,   136,   102,     0,     0,     0,     0],
         [  101,  1045,  1005,  1049,  2503,   117,  5763,  1128,   136,   102]]),
 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),
 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0],
         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}
```

Ahora tenemos tensores rectangulares! Nota que los tokens de relleno han sido codificados en IDs de entrada con ID 0, y tienen un valor de mascara de atencion de 0 tambien. Esto es porque esos tokens de relleno no deben ser analizados por el modelo: no son parte de la oracion real.

### Truncando entradas[[truncating-inputs]]

Los tensores pueden volverse demasiado grandes para ser procesados por el modelo. Por ejemplo, BERT solo fue preentrenado con secuencias de hasta 512 tokens, por lo que no puede procesar secuencias mas largas. Si tienes secuencias mas largas de lo que el modelo puede manejar, necesitaras truncarlas con el parametro `truncation`:

```py
encoded_input = tokenizer(
    "This is a very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very long sentence.",
    truncation=True,
)
print(encoded_input["input_ids"])
```

```python out
[101, 1188, 1110, 170, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1179, 5650, 119, 102]
```

Combinando los argumentos de padding y truncation, puedes asegurarte de que tus tensores tengan el tamano exacto que necesitas:

```py
encoded_input = tokenizer(
    ["How are you?", "I'm fine, thank you!"],
    padding=True,
    truncation=True,
    max_length=5,
    return_tensors="pt",
)
print(encoded_input)
```

```python out
{'input_ids': tensor([[  101,  1731,  1132,  1128,   102],
         [  101,  1045,  1005,  1049,   102]]),
 'token_type_ids': tensor([[0, 0, 0, 0, 0],
         [0, 0, 0, 0, 0]]),
 'attention_mask': tensor([[1, 1, 1, 1, 1],
         [1, 1, 1, 1, 1]])}
```

### Agregando tokens especiales

Los tokens especiales (o al menos el concepto de ellos) son particularmente importantes para BERT y modelos derivados. Estos tokens se agregan para representar mejor los limites de las oraciones, como el comienzo de una oracion (`[CLS]`) o el separador entre oraciones (`[SEP]`). Veamos un ejemplo simple:

```py
encoded_input = tokenizer("How are you?")
print(encoded_input["input_ids"])
tokenizer.decode(encoded_input["input_ids"])
```

```python out
[101, 1731, 1132, 1128, 136, 102]
'[CLS] How are you? [SEP]'
```

Estos tokens especiales son agregados automaticamente por el tokenizador. No todos los modelos necesitan tokens especiales; se usan principalmente cuando un modelo fue preentrenado con ellos, en cuyo caso el tokenizador los agregara ya que el modelo los espera.

### Por que es necesario todo esto?

Aqui hay un ejemplo concreto. Considera estas secuencias codificadas:

```py
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
```

Una vez tokenizadas, tenemos:

```python
encoded_sequences = [
    [
        101,
        1045,
        1005,
        2310,
        2042,
        3403,
        2005,
        1037,
        17662,
        12172,
        2607,
        2026,
        2878,
        2166,
        1012,
        102,
    ],
    [101, 1045, 5223, 2023, 2061, 2172, 999, 102],
]
```

Esta es una lista de secuencias codificadas: una lista de listas. Los tensores solo aceptan formas rectangulares (piensa en matrices). Este "arreglo" ya tiene forma rectangular, por lo que convertirlo a un tensor es facil:

```py
import torch

model_inputs = torch.tensor(encoded_sequences)
```

### Usando los tensores como entradas al modelo[[using-the-tensors-as-inputs-to-the-model]]

Usar los tensores con el modelo es extremadamente simple ‚Äî solo llamamos al modelo con las entradas:

```py
output = model(model_inputs)
```

Aunque el modelo acepta muchos argumentos diferentes, solo los IDs de entrada son necesarios. Explicaremos que hacen los otros argumentos y cuando son requeridos mas adelante, pero primero necesitamos examinar mas de cerca los tokenizadores que construyen las entradas que un modelo Transformer puede entender.


---



# Tokenizadores


**Video:** [Ver en YouTube](https://youtu.be/VFp38yj8h3A)


Los tokenizadores son uno de los componentes fundamentales del pipeline en NLP. Sirven para traducir texto en datos que los modelos puedan procesar; es decir, de texto a valores num√©ricos. En esta secci√≥n veremos en qu√© se fundamenta todo el proceso de tokenizado.

En las tareas de NLP, los datos generalmente ingresan como texto crudo. Por ejemplo:

```
Jim Henson era un titiritero
```

Sin embargo, necesitamos una forma de convertir el texto crudo a valores num√©ricos para los modelos. Eso es precisamente lo que hacen los tokenizadores, y existe una variedad de formas en que puede hacerse. El objetivo final es obtener valores que sean cortos pero muy significativos para el modelo. 

Veamos algunos algoritmos de tokenizaci√≥n, e intentemos atacar algunas preguntas que puedas tener.

## Tokenizaci√≥n Word-based


**Video:** [Ver en YouTube](https://youtu.be/nhJxYji1aho)


El primer tokenizador que nos ocurre es el _word-based_ (_basado-en-palabras_). Es generalmente sencillo, con pocas normas, y generalmente da buenos resultados. Por ejemplo, en la imagen a continuaci√≥n separamos el texto en palabras y buscamos una representaci√≥n num√©rica.

<div class="flex justify-center">
  <img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization.svg" alt="Un ejemplo de tokenizador _word-based_."/>
  <img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization-dark.svg" alt="Un ejemplo de tokenizador _word-based_."/>
</div>

Existen varias formas de separar el texto. Por ejemplo, podr√≠amos usar los espacios para tokenizar usando Python y la funci√≥n `split()`. 

```py
tokenized_text = "Jim Henson era un titiritero".split()
print(tokenized_text)
```

```python out
['Jim', 'Henson', 'era', 'un', 'titiritero']
```

Tambi√©n hay variaciones de tokenizadores de palabras que tienen reglas adicionales para la puntuaci√≥n. Con este tipo de tokenizador, podemos acabar con unos "vocabularios" bastante grandes, donde un vocabulario se define por el n√∫mero total de tokens independientes que tenemos en nuestro corpus.

A cada palabra se le asigna un ID, empezando por 0 y subiendo hasta el tama√±o del vocabulario. El modelo utiliza estos ID para identificar cada palabra.

Si queremos cubrir completamente un idioma con un tokenizador basado en palabras, necesitaremos tener un identificador para cada palabra del idioma, lo que generar√° una enorme cantidad de tokens. Por ejemplo, hay m√°s de 500.000 palabras en el idioma ingl√©s, por lo que para construir un mapa de cada palabra a un identificador de entrada necesitar√≠amos hacer un seguimiento de esa cantidad de identificadores. Adem√°s, palabras como "perro" se representan de forma diferente a palabras como "perros", y el modelo no tendr√° forma de saber que "perro" y "perros" son similares: identificar√° las dos palabras como no relacionadas. Lo mismo ocurre con otras palabras similares, como "correr" y "corriendo", que el modelo no ver√° inicialmente como similares.

Por √∫ltimo, necesitamos un token personalizado para representar palabras que no est√°n en nuestro vocabulario. Esto se conoce como el token "desconocido", a menudo representado como "[UNK]" o "&lt;unk&gt;". Generalmente, si el tokenizador est√° produciendo muchos de estos tokens es una mala se√±al, ya que no fue capaz de recuperar una representaci√≥n de alguna palabra y est√° perdiendo informaci√≥n en el proceso. El objetivo al elaborar el vocabulario es hacerlo de tal manera que el tokenizador tokenice el menor n√∫mero de palabras posibles en tokens desconocidos.

Una forma de reducir la cantidad de tokens desconocidos es ir un poco m√°s all√°, utilizando un tokenizador _word-based_.

## Tokenizaci√≥n Character-based


**Video:** [Ver en YouTube](https://youtu.be/ssLq_EK2jLE)


Un tokenizador _character-based_ separa el texto en caracteres, y no en palabras. Esto conlleva dos beneficios principales:

- Obtenemos un vocabulario mucho m√°s corto.
- Habr√° muchos menos tokens por fuera del vocabulario conocido.

No obstante, pueden surgir inconvenientes por los espacios en blanco y signos de puntuaci√≥n.

<div class="flex justify-center">
  <img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization.svg" alt="Ejemplo de tokenizador basado en palabras."/>
  <img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization-dark.svg" alt="Ejemplo de tokenizador basado en palabras."/>
</div>

As√≠, este m√©todo tampoco es perfecto. Dada que la representaci√≥n se construy√≥ con caracteres, uno podr√≠a pensar intuitivamente que resulta menos significativo: Cada una de las palabras no significa mucho por separado, mientras que las palabras s√≠. Sin embargo, eso es dependiente del idioma. Por ejemplo en Chino, cada uno de los caracteres conlleva m√°s informaci√≥n que en un idioma latino.

Otro aspecto a considerar es que terminamos con una gran cantidad de tokens que el modelo debe procesar, mientras que en el caso del tokenizador _word-based_, un token representa una palabra, en la representaci√≥n de caracteres f√°cilmente puede necesitar m√°s de 10 tokens.

Para obtener lo mejor de ambos mundos, podemos usar una combinaci√≥n de las t√©cnicas: la tokenizaci√≥n por *subword tokenization*.

## Tokenizaci√≥n por Subword


**Video:** [Ver en YouTube](https://youtu.be/zHvTiHr506c)


Los algoritmos de tokenizaci√≥n de subpalabras se basan en el principio de que las palabras de uso frecuente no deben dividirse, mientras que las palabras raras deben descomponerse en subpalabras significativas.

Por ejemplo, "extra√±amente" podr√≠a considerarse una palabra rara y podr√≠a descomponerse en "extra√±a" y "mente". Es probable que ambas aparezcan con m√°s frecuencia como subpalabras independientes, mientras que al mismo tiempo el significado de "extra√±amente" se mantiene por el significado compuesto de "extra√±a" y "mente".

Este es un ejemplo que muestra c√≥mo un algoritmo de tokenizaci√≥n de subpalabras tokenizar√≠a la secuencia "Let's do tokenization!":

<div class="flex justify-center">
  <img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword.svg" alt="Un tokenizador basado en subpalabras."/>
  <img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword-dark.svg" alt="Un tokenizador basado en subpalabras."/>
</div>

Estas subpalabras terminan aportando mucho significado sem√°ntico: por ejemplo, en el ejemplo anterior, "tokenizaci√≥n" se dividi√≥ en "token" y "izaci√≥n", dos tokens que tienen un significado sem√°ntico y a la vez son eficientes en cuanto al espacio (s√≥lo se necesitan dos tokens para representar una palabra larga). Esto nos permite tener una cobertura relativamente buena con vocabularios peque√±os y casi sin tokens desconocidos.

Este enfoque es especialmente √∫til en algunos idiomas como el turco, donde se pueden formar palabras complejas (casi) arbitrariamente largas encadenando subpalabras.

### Y m√°s!

Como es l√≥gico, existen muchas m√°s t√©cnicas. Por nombrar algunas:

- Byte-level BPE (a nivel de bytes), como usa GPT-2
- WordPiece, usado por BERT
- SentencePiece or Unigram (pedazo de sentencia o unigrama), como se usa en los modelos multiling√ºes

A este punto, deber√≠as tener conocimientos suficientes sobre el funcionamiento de los tokenizadores para empezar a utilizar la API.

## Cargando y guardando

Cargar y guardar tokenizadores es tan sencillo como lo es con los modelos. En realidad, se basa en los mismos dos m√©todos: `from_pretrained()` y `save_pretrained()`. Estos m√©todos cargar√°n o guardar√°n el algoritmo utilizado por el tokenizador (un poco como la *arquitectura* del modelo) as√≠ como su vocabulario (un poco como los *pesos* del modelo).

La carga del tokenizador BERT entrenado con el mismo punto de control que BERT se realiza de la misma manera que la carga del modelo, excepto que utilizamos la clase `BertTokenizer`:

```py
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
```


**PyTorch:**

Al igual que `AutoModel`, la clase `AutoTokenizer` tomar√° la clase de tokenizador adecuada en la librer√≠a basada en el nombre del punto de control, y se puede utilizar directamente con cualquier punto de control:

**TensorFlow/Keras:**

Al igual que `TFAutoModel`, la clase `AutoTokenizer` tomar√° la clase de tokenizador adecuada en la librer√≠a basada en el nombre del punto de control, y se puede utilizar directamente con cualquier punto de control:


```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

Ahora podemos utilizar el tokenizador como se muestra en la secci√≥n anterior:

```python
tokenizer("Using a Transformer network is simple")
```

```python out
{'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

Guardar un tokenizador es id√©ntico a guardar un modelo:

```py
tokenizer.save_pretrained("directorio_en_mi_computador")
```

Hablaremos m√°s sobre `token_type_ids` en el [Cap√≠tulo 3](/course/chapter3), y explicaremos la clave `attention_mask` un poco m√°s tarde. Primero, veamos c√≥mo se generan los `input_ids`. Para ello, tendremos que ver los m√©todos intermedios del tokenizador.

## Encoding


**Video:** [Ver en YouTube](https://youtu.be/Yffk5aydLzg)


La traducci√≥n de texto a n√∫meros se conoce como _codificaci√≥n_. La codificaci√≥n se realiza en un proceso de dos pasos: la tokenizaci√≥n, seguida de la conversi√≥n a IDs de entrada.

Como hemos visto, el primer paso es dividir el texto en palabras (o partes de palabras, s√≠mbolos de puntuaci√≥n, etc.), normalmente llamadas *tokens*. Hay m√∫ltiples reglas que pueden gobernar ese proceso, por lo que necesitamos instanciar el tokenizador usando el nombre del modelo, para asegurarnos de que usamos las mismas reglas que se usaron cuando se preentren√≥ el modelo.

El segundo paso es convertir esos tokens en n√∫meros, para poder construir un tensor con ellos y alimentar el modelo. Para ello, el tokenizador tiene un *vocabulario*, que es la parte que descargamos cuando lo instanciamos con el m√©todo `from_pretrained()`. De nuevo, necesitamos usar el mismo vocabulario que se us√≥ cuando el modelo fue preentrenado.

Para entender mejor los dos pasos, los exploraremos por separado. Ten en cuenta que utilizaremos algunos m√©todos que realizan partes del proceso de tokenizaci√≥n por separado para mostrarte los resultados intermedios de esos pasos, pero en la pr√°ctica, deber√≠as llamar al tokenizador directamente en tus _inputs_ (como se muestra en la secci√≥n 2).

### Tokenization

El proceso de tokenizaci√≥n se realiza mediante el m√©todo `tokenize()` del tokenizador:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

sequence = "Using a Transformer network is simple"
tokens = tokenizer.tokenize(sequence)

print(tokens)
```

La salida de este m√©todo es una lista de cadenas, o tokens:

```python out
['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']
```

Este tokenizador es un tokenizador de subpalabras: divide las palabras hasta obtener tokens que puedan ser representados por su vocabulario. Este es el caso de `transformer`, que se divide en dos tokens: `transform` y `##er`.

### De tokens a IDs de entrada

La conversi√≥n a IDs de entrada se hace con el m√©todo del tokenizador `convert_tokens_to_ids()`:

```py
ids = tokenizer.convert_tokens_to_ids(tokens)

print(ids)
```

```python out
[7993, 170, 11303, 1200, 2443, 1110, 3014]
```

Estos resultados, una vez convertidos en el tensor del marco apropiado, pueden utilizarse como entradas de un modelo, como se ha visto anteriormente en este cap√≠tulo.

> [!TIP]
> ‚úèÔ∏è **Try it out!** Replica los dos √∫ltimos pasos (tokenizaci√≥n y conversi√≥n a IDs de entrada) en las frases de entrada que utilizamos en la secci√≥n 2 ("Llevo toda la vida esperando un curso de HuggingFace" y "¬°Odio tanto esto!"). Comprueba que obtienes los mismos ID de entrada que obtuvimos antes!

## Decodificaci√≥n

La *decodificaci√≥n* va al rev√©s: a partir de los √≠ndices del vocabulario, queremos obtener una cadena. Esto se puede hacer con el m√©todo `decode()` de la siguiente manera:

```py
decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])
print(decoded_string)
```

```python out
'Using a Transformer network is simple'
```

Notemos que el m√©todo `decode` no s√≥lo convierte los √≠ndices de nuevo en tokens, sino que tambi√©n agrupa los tokens que formaban parte de las mismas palabras para producir una frase legible. Este comportamiento ser√° extremadamente √∫til cuando utilicemos modelos que predigan texto nuevo (ya sea texto generado a partir de una indicaci√≥n, o para problemas de secuencia a secuencia como la traducci√≥n o el resumen).

A estas alturas deber√≠as entender las operaciones at√≥micas que un tokenizador puede manejar: tokenizaci√≥n, conversi√≥n a IDs, y conversi√≥n de IDs de vuelta a una cadena. Sin embargo, s√≥lo hemos rozado la punta del iceberg. En la siguiente secci√≥n, llevaremos nuestro enfoque a sus l√≠mites y echaremos un vistazo a c√≥mo superarlos.


---



# Manejando Secuencias M√∫ltiples


**PyTorch:**

**Video:** [Ver en YouTube](https://youtu.be/M6adb1j2jPI)

**TensorFlow/Keras:**

**Video:** [Ver en YouTube](https://youtu.be/ROxrFOEbsQE)


En la secci√≥n anterior, hemos explorado el caso de uso m√°s sencillo: hacer inferencia sobre una √∫nica secuencia de poca longitud. Sin embargo, surgen algunas preguntas:

- ¬øC√≥mo manejamos las secuencias m√∫ltiples?
- ¬øC√≥mo manejamos las secuencias m√∫ltiples *de diferentes longitudes*?
- ¬øSon los √≠ndices de vocabulario las √∫nicas entradas que permiten que un modelo funcione bien?
- ¬øExiste una secuencia demasiado larga?

Veamos qu√© tipo de problemas plantean estas preguntas, y c√≥mo podemos resolverlos utilizando la API de Transformers ü§ó.

## Los modelos esperan Baches de entrada 

En el ejercicio anterior has visto c√≥mo las secuencias se traducen en listas de n√∫meros. Convirtamos esta lista de n√∫meros en un tensor y envi√©moslo al modelo:


**PyTorch:**

```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(ids)
# Esta l√≠nea va a fallar:
model(input_ids)
```

```python out
IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)
```

**TensorFlow/Keras:**

```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = tf.constant(ids)
# Esta l√≠nea va a fallar:
model(input_ids)
```

```py out
InvalidArgumentError: Input to reshape is a tensor with 14 values, but the requested shape has 196 [Op:Reshape]
```

¬°Oh, no! ¬øPor qu√© ha fallado esto? "Hemos seguido los pasos de la tuber√≠a en la secci√≥n 2.

El problema es que enviamos una sola secuencia al modelo, mientras que los modelos de ü§ó Transformers esperan m√∫ltiples frases por defecto. Aqu√≠ tratamos de hacer todo lo que el tokenizador hizo detr√°s de escena cuando lo aplicamos a una `secuencia`, pero si te fijas bien, ver√°s que no s√≥lo convirti√≥ la lista de IDs de entrada en un tensor, sino que le agreg√≥ una dimensi√≥n encima:


**PyTorch:**

```py
tokenized_inputs = tokenizer(sequence, return_tensors="pt")
print(tokenized_inputs["input_ids"])
```

```python out
tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
          2607,  2026,  2878,  2166,  1012,   102]])
```

**TensorFlow/Keras:**

```py
tokenized_inputs = tokenizer(sequence, return_tensors="tf")
print(tokenized_inputs["input_ids"])
```

```py out
<tf.Tensor: shape=(1, 16), dtype=int32, numpy=
array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,
        12172,  2607,  2026,  2878,  2166,  1012,   102]], dtype=int32)>
```

Intent√©moslo de nuevo y a√±adamos una nueva dimensi√≥n encima:


**PyTorch:**

```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = torch.tensor([ids])
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)
```

**TensorFlow/Keras:**

```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = tf.constant([ids])
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)
```

Imprimimos los IDs de entrada as√≠ como los logits resultantes - aqu√≠ est√° la salida:


**PyTorch:**

```python out
Input IDs: [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]
Logits: [[-2.7276,  2.8789]]
```

**TensorFlow/Keras:**

```py out
Input IDs: tf.Tensor(
[[ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878
   2166  1012]], shape=(1, 14), dtype=int32)
Logits: tf.Tensor([[-2.7276208  2.8789377]], shape=(1, 2), dtype=float32)
```

*El "batching"* es el acto de enviar varias frases a trav√©s del modelo, todas a la vez. Si s√≥lo tienes una frase, puedes construir un lote con una sola secuencia: 

```
batched_ids = [ids, ids]
```

Se trata de un lote de dos secuencias id√©nticas.

> [!TIP]
> ‚úèÔ∏è **Try it out!** Convierte esta lista `batched_ids` en un tensor y p√°salo por tu modelo. Comprueba que obtienes los mismos logits que antes (¬°pero dos veces!).

La creaci√≥n de lotes permite que el modelo funcione cuando lo alimentas con m√∫ltiples sentencias. Utilizar varias secuencias es tan sencillo como crear un lote con una sola secuencia. Sin embargo, hay un segundo problema. Cuando se trata de agrupar dos (o m√°s) frases, √©stas pueden ser de diferente longitud. Si alguna vez ha trabajado con tensores, sabr√° que deben tener forma rectangular, por lo que no podr√° convertir la lista de IDs de entrada en un tensor directamente. Para evitar este problema, usamos el *padding* para las entradas.

## Padding a las entradas

La siguiente lista de listas no se puede convertir en un tensor:

```py no-format
batched_ids = [
    [200, 200, 200],
    [200, 200]
]
```
Para solucionar esto, utilizaremos *padding* para que nuestros tensores tengan una forma rectangular. El acolchado asegura que todas nuestras sentencias tengan la misma longitud a√±adiendo una palabra especial llamada *padding token* a las sentencias con menos valores. Por ejemplo, si tienes 10 frases con 10 palabras y 1 frase con 20 palabras, el relleno asegurar√° que todas las frases tengan 20 palabras. En nuestro ejemplo, el tensor resultante tiene este aspecto:

```py no-format
padding_id = 100

batched_ids = [
    [200, 200, 200],
    [200, 200, padding_id],
]
```
El ID del *padding token* se puede encontrar en `tokenizer.pad_token_id`. Us√©moslo y enviemos nuestras dos sentencias a trav√©s del modelo de forma individual y por lotes:


**PyTorch:**

```py no-format
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(torch.tensor(sequence1_ids)).logits)
print(model(torch.tensor(sequence2_ids)).logits)
print(model(torch.tensor(batched_ids)).logits)
```

```python out
tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)
tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
tensor([[ 1.5694, -1.3895],
        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)
```

**TensorFlow/Keras:**

```py no-format
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(tf.constant(sequence1_ids)).logits)
print(model(tf.constant(sequence2_ids)).logits)
print(model(tf.constant(batched_ids)).logits)
```

```py out
tf.Tensor([[ 1.5693678 -1.3894581]], shape=(1, 2), dtype=float32)
tf.Tensor([[ 0.5803005  -0.41252428]], shape=(1, 2), dtype=float32)
tf.Tensor(
[[ 1.5693681 -1.3894582]
 [ 1.3373486 -1.2163193]], shape=(2, 2), dtype=float32)
```


Hay un problema con los logits en nuestras predicciones por lotes: la segunda fila deber√≠a ser la misma que los logits de la segunda frase, ¬°pero tenemos valores completamente diferentes!

Esto se debe a que la caracter√≠stica clave de los modelos Transformer son las capas de atenci√≥n que *contextualizan* cada token. √âstas tendr√°n en cuenta los tokens de relleno, ya que atienden a todos los tokens de una secuencia. Para obtener el mismo resultado al pasar oraciones individuales de diferente longitud por el modelo o al pasar un lote con las mismas oraciones y el padding aplicado, tenemos que decirles a esas capas de atenci√≥n que ignoren los tokens de padding. Esto se hace utilizando una m√°scara de atenci√≥n.

## M√°scaras de atenci√≥n

*Las m√°scaras de atenci√≥n* son tensores con la misma forma que el tensor de IDs de entrada, rellenados con 0s y 1s: los 1s indican que los tokens correspondientes deben ser atendidos, y los 0s indican que los tokens correspondientes no deben ser atendidos (es decir, deben ser ignorados por las capas de atenci√≥n del modelo).


**PyTorch:**

```py no-format
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]

outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))
print(outputs.logits)
```

```python out
tensor([[ 1.5694, -1.3895],
        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
```

**TensorFlow/Keras:**

```py no-format
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]

outputs = model(tf.constant(batched_ids), attention_mask=tf.constant(attention_mask))
print(outputs.logits)
```

```py out
tf.Tensor(
[[ 1.5693681  -1.3894582 ]
 [ 0.5803021  -0.41252586]], shape=(2, 2), dtype=float32)
```


Ahora obtenemos los mismos logits para la segunda frase del lote.

Podemos ver que el √∫ltimo valor de la segunda secuencia es un ID de relleno, que es un valor 0 en la m√°scara de atenci√≥n.

> [!TIP]
> ‚úèÔ∏è **Try it out!** Aplique la tokenizaci√≥n manualmente a las dos frases utilizadas en la secci√≥n 2 ("Llevo toda la vida esperando un curso de HuggingFace" y "¬°Odio tanto esto!"). P√°selas por el modelo y compruebe que obtiene los mismos logits que en la secci√≥n 2. Ahora j√∫ntalos usando el token de relleno, y luego crea la m√°scara de atenci√≥n adecuada. Comprueba que obtienes los mismos resultados al pasarlos por el modelo.

## Secuencias largas

Con los modelos Transformer, hay un l√≠mite en la longitud de las secuencias que podemos pasar a los modelos. La mayor√≠a de los modelos manejan secuencias de hasta 512 o 1024 tokens, y se bloquean cuando se les pide que procesen secuencias m√°s largas. Hay dos soluciones a este problema:

- Usar un modelo que soporte secuencias largas
- Truncar tus secuencias

Los modelos tienen diferentes longitudes de secuencia soportadas, y algunos se especializan en el manejo de secuencias muy largas. Un ejemplo es [Longformer](https://huggingface.co/transformers/model_doc/longformer.html) y otro es [LED](https://huggingface.co/transformers/model_doc/led.html). Si est√°s trabajando en una tarea que requiere secuencias muy largas, te recomendamos que eches un vistazo a esos modelos.

En caso contrario, le recomendamos que trunque sus secuencias especificando el par√°metro `max_sequence_length`:

```py
sequence = sequence[:max_sequence_length]
```


---



# Poniendo todo junto


En las √∫ltimas secciones, hemos hecho nuestro mejor esfuerzo para realizar la mayor parte del trabajo a mano. Exploramos como funcionan los tokenizadores y vimos la tokenizaci√≥n, conversi√≥n a IDs de entrada, relleno, truncado, y m√°scaras de atenci√≥n.

Sin embargo, como vimos en la secci√≥n 3, la API de transformadores ü§ó puede manejar todo esto por nosotros con una funci√≥n de alto nivel la cual trataremos aqu√≠. Cuando llamas a tu `tokenizer` directamente en una sentencia, obtienes entradas que est√°n lista para pasar a tu modelo:

```py
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

Aqu√≠ la variable `model_inputs` contiene todo lo necesario para que un modelo opere bien. Para DistilBERT, que incluye los IDs de entrada tambi√©n como la m√°scara de atenci√≥n. Otros modelos que aceptan entradas adicionales tambi√©n tendr√°n las salidas del objeto `tokenizer`.

Como veremos en los ejemplos de abajo, este m√©todo es muy poderoso. Primero, puede tokenizar una sola secuencia:

```py
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

Tambi√©n maneja m√∫ltiples secuencias a la vez, sin cambios en la API:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

model_inputs = tokenizer(sequences)
```

Puede rellenar de acuerdo a varios objetivos:

```py
# Rellenar las secuencias hasta la mayor longitud de secuencia
model_inputs = tokenizer(sequences, padding="longest")

# Rellenar las secuencias hasta la m√°xima longitud del modelo
# (512 para BERT o DistilBERT)
model_inputs = tokenizer(sequences, padding="max_length")

# Rellenar las secuencias hasta la m√°xima longitud especificada
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)
```

Tambi√©n puede truncar secuencias:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# Truncar las secuencias m√°s largas que la m√°xima longitud del modelo
# (512 para BERT o DistilBERT)
model_inputs = tokenizer(sequences, truncation=True)

# Truncar las secuencias m√°s largas que la longitud especificada
model_inputs = tokenizer(sequences, max_length=8, truncation=True)
```

El objeto `tokenizer` puede manejar la conversi√≥n a tensores de frameworks espec√≠ficos, los cuales pueden ser enviados directamente al modelo. Por ejemplo, en el siguiente c√≥digo de ejemplo estamos solicitando al tokenizer que regrese los tensores de los distintos frameworks ‚Äî `"pt"` regresa tensores de PyTorch, `"tf"` regresa tensores de TensorFlow, y `"np"` regresa arreglos de NumPy: 

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# Devuelve tensores PyTorch
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# Devuelve tensores TensorFlow
model_inputs = tokenizer(sequences, padding=True, return_tensors="tf")

# Devuelve arrays Numpy
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")
```

## Tokens especiales

Si damos un vistazo a los IDs de entrada retornados por el tokenizer, veremos que son un poquito diferentes a lo que ten√≠amos anteriormente:

```py
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
print(model_inputs["input_ids"])

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
```

```python out
[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]
[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]
```

Se agreg√≥ un ID de token al principio, y uno al final. Decodifiquemos las dos secuencias de IDs de arriba para ver de que se trata:

```py
print(tokenizer.decode(model_inputs["input_ids"]))
print(tokenizer.decode(ids))
```

```python out
"[CLS] i've been waiting for a huggingface course my whole life. [SEP]"
"i've been waiting for a huggingface course my whole life."
```

El tokenizador agreg√≥ la palabra especial `[CLS]` al principio y la palabra especial `[SEP]` al final. Esto se debe a que el modelo fue preentrenado con esos, as√≠ para obtener los mismos resultados por inferencia necesitamos agregarlos tambi√©n. Nota que algunos modelos no agregan palabras especiales, o agregan unas distintas; los modelos tambi√©n pueden agregar estas palabras especiales s√≥lo al principio, o s√≥lo al final. En cualquier caso, el tokenizador sabe cu√°les son las esperadas y se encargar√° de ello por t√≠.

## Conclusi√≥n: Del tokenizador al modelo

Ahora que hemos visto todos los pasos individuales que el objeto `tokenizer` usa cuando se aplica a textos, veamos una √∫ltima vez c√≥mo maneja varias secuencias (¬°relleno!), secuencias muy largas (¬°truncado!), y m√∫ltiples tipos de tensores con su API principal:


**PyTorch:**

```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
output = model(**tokens)
```

**TensorFlow/Keras:**

```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="tf")
output = model(**tokens)
```



---

# ¬°Has completado el uso b√°sico!


¬°Buen trabajo siguiendo el curso hasta ahora! Para recapitular, en este cap√≠tulo t√∫:

- Aprendiste los bloques de construcci√≥n b√°sicos de un modelo Transformer.
- Aprendiste lo que compone a un pipeline de tokenizaci√≥n.
- Viste c√≥mo usar un modelo Transformer en la pr√°ctica.
- Aprendiste c√≥mo aprovechar un tokenizador para convertir texto a tensores que sean entendibles por el modelo.
- Configuraste un tokenizador y un modelo juntos para pasar dle texto a predicciones.
- Aprendiste las limitaciones de los IDs de entrada, y aprendiste acerca de m√°scaras de atenci√≥n.
- Jugaste con los m√©todos del tokenizador vers√°tiles y configurables.

A partir de ahora, ser√°s capaz de navegar libremente por la documentaci√≥n de ü§ó Transformers: el vocabulario te sonar√° familiar, ya que has visto los m√©todos que usar√°s la mayor parte del tiempo.


---




# Quiz de final de cap√≠tulo


### 1. ¬øCu√°l es el orden del pipeline de modelado del lenguaje?


- Primero, el modelo que maneja el texto y devuelve las peticiones sin procesar. El tokenizador luego da sentido a estas predicciones y las convierte nuevamente en texto cuando es necesario.
- Primero, el tokenizador, que maneja el texto y regresa IDs. El modelo maneja estos IDs y produce una predicci√≥n, la cual puede ser alg√∫n texto.
- El tokenizador maneja texto y regresa IDs. El modelo maneja estos IDs y produce una predicci√≥n. El tokenizador puede luego ser usado de nuevo para convertir estas predicciones de vuelta a texto.


### 2. ¬øCu√°ntas dimensiones tiene el tensor producido por el modelo base de Transformer y cu√°les son?


- 1: La longitud de secuencia y el tama√±o del lote
- 2: La longitud de secuencia y el tama√±o oculto
- 3: La longitud de secuencia, el tama√±o de lote y el tama√±o oculto


### 3. ¬øCu√°l de los siguientes es un ejemplo de tokenizaci√≥n de subpalabras?


- WordPiece
- Tokenizaci√≥n basada en caracteres
- Divisi√≥n por espacios en blanco y puntuaci√≥n
- BPE
- Unigrama
- Ninguno de los anteriores


### 4. ¬øQu√© es una cabeza del modelo?


- Un componente de la red de Transformer base que redirecciona los tensores a sus capas correctas
- Tambi√©n conocido como el mecanismo de autoatenci√≥n, adapta la representaci√≥n de un token de acuerdo a los otros tokens de la secuencia
- Un componente adicional, compuesto usualmente de una o unas pocas capas, para convertir las predicciones del transformador a una salida espec√≠fica de la tarea


**PyTorch:**

### 5. ¬øQu√© es un AutoModel?


- Un modelo que entrena autom√°ticamente en tus datos
- Un objeto que devuelve la arquitectura correcta basado en el punto de control
- Un modelo que detecta autom√°ticamente el lenguaje usado por sus entradas para cargar los pesos correctos


**TensorFlow/Keras:**

### 5. ¬øQu√© es un TFAutoModel?


- Un modelo que entrena autom√°ticamente en tus datos
- Un objeto que devuelve la arquitectura correcta basado en el punto de control
- Un modelo que detecta autom√°ticamente el lenguaje usado por sus entradas para cargar los pesos correctos


### 6. ¬øCu√°les son las t√©cnicas a tener en cuenta al realizar batching de secuencias de diferentes longitudes juntas?


- Truncado
- Returning tensors
- Relleno
- Enmascarado de atenci√≥n


### 7. ¬øCu√°l es el punto de aplicar una funci√≥n SoftMax a las salidas logits por un modelo de clasificaci√≥n de secuencias?


- Suaviza los logits para que sean m√°s fiables.
- Aplica un l√≠mite inferior y superior de modo que sean comprensibles.
- La suma total de la salida es entonces 1, dando como resultado una posible interpretaci√≥n probabil√≠stica.


### 8. ¬øEn qu√© m√©todo se centra la mayor parte de la API del tokenizador?


- `encode`, ya que puede codificar texto en IDs e IDs en predicciones
- Llamar al objeto tokenizador directamente.
- `pad`
- `tokenize`


### 9. ¬øQu√© contiene la variable `result` en este c√≥digo de ejemplo?

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
result = tokenizer.tokenize("Hello!")
```


- Una lista de strings, cada string es un token
- Una lista de IDs
- Una cadena que contiene todos los tokens


**PyTorch:**

### 10. ¬øHay algo mal con el siguiente c√≥digo?

```py
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = AutoModel.from_pretrained("gpt2")

encoded = tokenizer("Hey!", return_tensors="pt")
result = model(**encoded)
```


- No, parece correcto.
- El tokenizador y el modelo siempre deben ser del mismo punto de control.
- Es una buena pr√°ctica rellenar y truncar con el tokenizador ya que cada entrada es un lote.


**TensorFlow/Keras:**

### 10. ¬øHay algo mal con el siguiente c√≥digo?

```py
from transformers import AutoTokenizer, TFAutoModel

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = TFAutoModel.from_pretrained("gpt2")

encoded = tokenizer("Hey!", return_tensors="pt")
result = model(**encoded)
```


- No, parece correcto.
- El tokenizador y el modelo siempre deben ser del mismo punto de control.
- Es una buena pr√°ctica rellenar y truncar con el tokenizador ya que cada entrada es un lote.




---




# Cuestionario de fin de capitulo[[end-of-chapter-quiz]]


### 1. Cual es el orden del pipeline de modelado de lenguaje?


- Primero, el modelo, que maneja texto y devuelve predicciones crudas. El tokenizador luego da sentido a estas predicciones y las convierte de vuelta a texto cuando es necesario.
- Primero, el tokenizador, que maneja texto y devuelve IDs. El modelo maneja estos IDs y produce una prediccion, que puede ser algun texto.
- El tokenizador maneja texto y devuelve IDs. El modelo maneja estos IDs y produce una prediccion. El tokenizador puede entonces usarse nuevamente para convertir estas predicciones de vuelta a texto.


### 2. Cuantas dimensiones tiene el tensor de salida del modelo Transformer base, y cuales son?


- 2: La longitud de secuencia y el tamano del lote
- 2: La longitud de secuencia y el tamano oculto
- 3: La longitud de secuencia, el tamano del lote y el tamano oculto


### 3. Cual de los siguientes es un ejemplo de tokenizacion por subpalabras?


- WordPiece
- Tokenizacion basada en caracteres
- Division por espacios en blanco y puntuacion
- BPE
- Unigram
- Ninguna de las anteriores


### 4. Que es una cabeza de modelo?


- Un componente de la red Transformer base que redirige tensores a sus capas correctas
- Tambien conocido como el mecanismo de auto-atencion, adapta la representacion de un token segun los otros tokens de la secuencia
- Un componente adicional, usualmente compuesto de una o pocas capas, para convertir las predicciones del transformer en una salida especifica de la tarea


### 5. Que es un AutoModel?


- Un modelo que entrena automaticamente con tus datos
- Un objeto que devuelve la arquitectura correcta basandose en el checkpoint
- Un modelo que detecta automaticamente el idioma usado en sus entradas para cargar los pesos correctos


### 6. Cuales son las tecnicas a tener en cuenta al agrupar secuencias de diferentes longitudes juntas?


- Truncamiento
- Devolver tensores
- Relleno (Padding)
- Enmascaramiento de atencion


### 7. Cual es el proposito de aplicar una funcion SoftMax a los logits producidos por un modelo de clasificacion de secuencias?


- Suaviza los logits para que sean mas confiables.
- Aplica un limite inferior y superior para que sean comprensibles.
- La suma total de la salida es entonces 1, resultando en una posible interpretacion probabilistica.


### 8. Alrededor de que metodo se centra la mayor parte de la API del tokenizador?


- `encode`, ya que puede codificar texto en IDs e IDs en predicciones
- Llamando al objeto tokenizador directamente.
- `pad`
- `tokenize`


### 9. Que contiene la variable `result` en este ejemplo de codigo?

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
result = tokenizer.tokenize("Hello!")
```


- Una lista de cadenas, cada cadena siendo un token
- Una lista de IDs
- Una cadena conteniendo todos los tokens


### 10. Hay algo mal con el siguiente codigo?

```py
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = AutoModel.from_pretrained("gpt2")

encoded = tokenizer("Hey!", return_tensors="pt")
result = model(**encoded)
```


- No, parece correcto.
- El tokenizador y el modelo siempre deben ser del mismo checkpoint.
- Es buena practica rellenar y truncar con el tokenizador ya que cada entrada es un lote.



---

# 3. Ajuste fino de un modelo preentrenado



# Introducci√≥n


En el [Cap√≠tulo 2](/course/chapter2) exploramos c√≥mo usar los tokenizadores y modelos preentrenados para realizar predicciones. Pero, ¬øqu√© pasa si deseas ajustar un modelo preentrenado con tu propio conjunto de datos?


**PyTorch:**

* C√≥mo preparar un conjunto de datos grande desde el Hub.
* C√≥mo usar la API de alto nivel del entrenador para ajustar un modelo.
* C√≥mo usar un bucle personalizado de entrenamiento.
* C√≥mo aprovechar la librer√≠a ü§ó Accelerate para f√°cilmente ejecutar el bucle personalizado de entrenamiento en cualquier configuraci√≥n distribuida.

**TensorFlow/Keras:**

* C√≥mo preparar un conjunto de datos grande desde el Hub.
* C√≥mo usar Keras para ajustar un modelo.
* C√≥mo usar Keras para obtener predicciones.
* C√≥mo usar una m√©trica personalizada.


Para subir tus puntos de control (*checkpoints*) en el Hub de Hugging Face, necesitas una cuenta en huggingface.co: [crea una cuenta](https://huggingface.co/join)

---



# Procesando los datos


**PyTorch:**

Continuando con el ejemplo del [cap√≠tulo anterior](/course/chapter2), aqu√≠ mostraremos como podr√≠amos entrenar un clasificador de oraciones/sentencias en PyTorch.:

```python
import torch
from torch.optim import AdamW
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Same as before
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# This is new
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
```

**TensorFlow/Keras:**

Continuando con el ejemplo del [cap√≠tulo anterior](/course/chapter2), aqu√≠ mostraremos como podr√≠amos entrenar un clasificador de oraciones/sentencias en TensorFlow:

```python
import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# Igual que antes
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors="tf"))

# Esto es nuevo
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
labels = tf.convert_to_tensor([1, 1])
model.train_on_batch(batch, labels)
```


Por supuesto, entrenando el modelo con solo dos oraciones no va a producir muy buenos resultados. Para obtener mejores resultados, debes preparar un conjunto de datos m√°s grande.

En esta secci√≥n usaremos como ejemplo el conjunto de datos MRPC (Cuerpo de par√°frasis de investigaciones de Microsoft), que fue presentado en el [art√≠culo](https://www.aclweb.org/anthology/I05-5002.pdf) de William B. Dolan and Chris Brockett. El conjunto de datos consiste en 5,801 pares of oraciones, con una etiqueta que indica si son par√°frasis o no. (es decir, si ambas oraciones significan lo mismo). Hemos seleccionado el mismo para este cap√≠tulo porque es un conjunto de datos peque√±o que facilita la experimentaci√≥n y entrenamiento sobre √©l.

### Cargando un conjunto de datos desde el Hub


**PyTorch:**

**Video:** [Ver en YouTube](https://youtu.be/_BZearw7f0w)

**TensorFlow/Keras:**

**Video:** [Ver en YouTube](https://youtu.be/W_gMJF0xomE)


El Hub no solo contiene modelos; sino que tambi√©n tiene m√∫ltiples conjunto de datos en diferentes idiomas. Puedes explorar los conjuntos de datos [aqu√≠](https://huggingface.co/datasets), y recomendamos que trates de cargar y procesar un nuevo conjunto de datos una vez que hayas revisado esta secci√≥n (mira la documentaci√≥n general [aqu√≠](https://huggingface.co/docs/datasets/loading)). Por ahora, enfoqu√©monos en el conjunto de datos MRPC! Este es uno de los 10 conjuntos de datos que comprende el [punto de referencia GLUE](https://gluebenchmark.com/), el cual es un punto de referencia acad√©mico que se usa para medir el desempe√±o de modelos ML sobre 10 tareas de clasificaci√≥n de texto.

La librer√≠a ü§ó Datasets provee un comando muy simple para descargar y memorizar un conjunto de datos en el Hub. Podemos descargar el conjunto de datos de la siguiente manera:

> [!TIP]
> ‚ö†Ô∏è **Advertencia** Aseg√∫rate de que `datasets` est√© instalado ejecutando `pip install datasets`. Luego, carga el conjunto de datos MRPC y impr√≠melo para ver qu√© contiene. 

```py
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```

Como puedes ver, obtenemos un objeto `DatasetDict` que contiene los conjuntos de datos de entrenamiento, de validaci√≥n y de pruebas. Cada uno de estos contiene varias columnas (`sentence1`, `sentence2`, `label`, and `idx`) y un n√∫mero variable de filas, que son el n√∫mero de elementos en cada conjunto (asi, que hay 3,668 pares de oraciones en el conjunto de entrenamiento, 408 en el de validaci√≥n, y 1,725 en el pruebas)

Este comando descarga y almacena el conjunto de datos, por defecto en *~/.cache/huggingface/dataset*. Recuerda del Cap√≠tulo 2 que puedes personalizar tu carpeta mediante la configuraci√≥n de la variable de entorno `HF_HOME`.

Podemos acceder a cada par de oraciones en nuestro objeto `raw_datasets` usando indexaci√≥n, como con un diccionario.

```py
raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]
```

```python out
{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'}
```

Podemos ver que las etiquetas ya son n√∫meros enteros, as√≠ que no es necesario hacer ning√∫n preprocesamiento. Para saber cual valor corresponde con cual etiqueta, podemos inspeccionar el atributo `features` de nuestro `raw_train_dataset`. Esto indicara el tipo dato de cada columna:

```py
raw_train_dataset.features
```

```python out
{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}
```

Internamente, `label` es del tipo de dato `ClassLabel`, y la asociaci√≥n de valores enteros y sus etiquetas esta almacenado en la carpeta *names*. `0` corresponde con `not_equivalent`, y `1` corresponde con `equivalent`.

> [!TIP]
> ‚úèÔ∏è **¬°Int√©ntalo!** Mira el elemento 15 del conjunto de datos de entrenamiento y el elemento 87 del conjunto de datos de validaci√≥n. Cu√°les son sus etiquetas?

### Preprocesando un conjunto de datos


**PyTorch:**

**Video:** [Ver en YouTube](https://youtu.be/0u3ioSwev3s)

**TensorFlow/Keras:**

**Video:** [Ver en YouTube](https://youtu.be/P-rZWqcB6CE)


Para preprocesar el conjunto de datos, necesitamos convertir el texto en n√∫meros que puedan ser entendidos por el modelo. Como viste en el [cap√≠tulo anterior](/course/chapter2), esto se hace con el tokenizador. Podemos darle al tokenizador una oraci√≥n o una lista de oraciones, as√≠ podemos tokenizar directamente todas las primeras y las segundas oraciones de cada par de la siguiente manera:

```py
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])
```

Sin embargo, no podemos simplemente pasar dos secuencias al modelo y obtener una predicci√≥n indicando si estas son par√°frasis o no. Necesitamos manipular las dos secuencias como un par y aplicar el preprocesamiento apropiado.
Afortunadamente, el tokenizador puede recibir tambi√©n un par de oraciones y preparar las misma de una forma que nuestro modelo BERT espera:

```py
inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs
```

```python out
{
  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

Nosotros consideramos las llaves `input_ids` y `attention_mask` en el [Cap√≠tulo 2](/course/chapter2), pero postergamos hablar sobre la llave `token_type_ids`. En este ejemplo, esta es la que le dice al modelo cual parte de la entrada es la primera oraci√≥n y cual es la segunda.

> [!TIP]
> ‚úèÔ∏è **¬°Int√©ntalo!** Toma el elemento 15 del conjunto de datos de entrenamiento y tokeniza las dos oraciones independientemente y como un par. Cu√°l es la diferencia entre los dos resultados?

Si convertimos los IDs dentro de `input_ids` en palabras:

```py
tokenizer.convert_ids_to_tokens(inputs["input_ids"])
```

obtendremos:

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
```

De esta manera vemos que el modelo espera las entradas de la siguiente forma `[CLS] sentence1 [SEP] sentence2 [SEP]` cuando hay dos oraciones. Alineando esto con los `token_type_ids` obtenemos:

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]
```

Como puedes observar, las partes de la entrada que corresponden a `[CLS] sentence1 [SEP]` todas tienen un tipo de token ID `0`, mientras que las otras partes que corresponden a `sentence2 [SEP]`, todas tienen tipo ID `1`.

N√≥tese que si seleccionas un punto de control diferente, no necesariamente tendr√°s el `token_type_ids` en tus entradas tokenizadas (por ejemplo, ellas no aparecen si usas un modelo DistilBERT). Estas aparecen cuando el modelo sabe que hacer con ellas, porque las ha visto durante su etapa de preentrenamiento.

Aqu√≠, BERT est√° preentrenado con tokens de tipo ID, y adem√°s del objetivo de modelado de lenguaje oculto que mencionamos en el [Cap√≠tulo 1](/course/chapter1), tambi√©n tiene el objetivo llamado _predicci√≥n de la siguiente oraci√≥n_. El objetivo con esta tarea es modelar la relaci√≥n entre pares de oraciones.

Para predecir la siguiente oraci√≥n, el modelo recibe pares de oraciones (con tokens ocultados aleatoriamente) y se le pide que prediga si la segunda secuencia sigue a la primera. Para que la tarea no sea tan simple, la mitad de las veces las oraciones est√°n seguidas en el texto original de donde se obtuvieron, y la otra mitad las oraciones vienen de dos documentos distintos.

En general, no debes preocuparte si los `token_type_ids` est√°n o no en las entradas tokenizadas: con tal de que uses el mismo punto de control para el tokenizador y el modelo, todo estar√° bien porque el tokenizador sabe qu√© pasarle a su modelo.

Ahora que hemos visto como nuestro tokenizador puede trabajar con un par de oraciones, podemos usarlo para tokenizar todo el conjunto de datos: como en el [cap√≠tulo anterior](/course/es/chapter2), podemos darle al tokenizador una lista de pares de oraciones, d√°ndole la lista de las primeras oraciones, y luego la lista de las segundas oraciones. Esto tambi√©n es compatible con las opciones de relleno y truncamiento que vimos en el [Cap√≠tulo 2](/course/chapter2). Por lo tanto, una manera de preprocesar el conjunto de datos de entrenamiento ser√≠a:

```py
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```

Esto funciona bien, pero tiene la desventaja de que devuelve un diccionario (con nuestras llaves, `input_ids`, `attention_mask`, and `token_type_ids`, y valores que son listas de listas). Adem√°s va a trabajar solo si tienes suficiente memoria principal para almacenar todo el conjunto de datos durante la tokenizaci√≥n (mientras que los conjuntos de datos de la librer√≠a ü§ó Datasets son archivos [Apache Arrow](https://arrow.apache.org/) almacenados en disco, y as√≠ solo mantienes en memoria las muestras que necesitas).

Para mantener los datos como un conjunto de datos, usaremos el m√©todo [`Dataset.map()`](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.map). Este tambi√©n nos ofrece una flexibilidad adicional en caso de que necesitemos preprocesamiento mas all√° de la tokenizaci√≥n. El m√©todo `map()` trabaja aplicando una funci√≥n sobre cada elemento del conjunto de datos, as√≠ que definamos una funci√≥n para tokenizar nuestras entradas:

```py
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
```

Esta funci√≥n recibe un diccionario (como los elementos de nuestro conjunto de datos) y devuelve un nuevo diccionario con las llaves `input_ids`, `attention_mask`, y `token_type_ids`. N√≥tese que tambi√©n funciona si el diccionario `example` contiene m√∫ltiples elementos (cada llave con una lista de oraciones) debido a que el `tokenizador` funciona con listas de pares de oraciones, como se vio anteriormente. Esto nos va a permitir usar la opci√≥n `batched=True` en nuestra llamada a `map()`, lo que acelera la tokenizaci√≥n significativamente. El `tokenizador` es respaldado por un tokenizador escrito en Rust que viene de la librer√≠a [ü§ó Tokenizers](https://github.com/huggingface/tokenizers). Este tokenizador puede ser muy r√°pido, pero solo si le da muchas entradas al mismo tiempo.

N√≥tese que por ahora hemos dejado el argumento `padding` fuera de nuestra funci√≥n de tokenizaci√≥n. Esto es porque rellenar todos los elementos hasta su m√°xima longitud no es eficiente: es mejor rellenar los elementos cuando se esta construyendo el lote, debido a que solo debemos rellenar hasta la m√°xima longitud en el lote, pero no en todo el conjunto de datos. Esto puede ahorrar mucho tiempo y poder de procesamiento cuando las entradas tienen longitudes variables.

Aqu√≠ se muestra como se aplica la funci√≥n de tokenizaci√≥n a todo el conjunto de datos en un solo paso. Estamos usando `batched=True` en nuestra llamada a `map` para que la funci√≥n sea aplicada a m√∫ltiples elementos de nuestro conjunto de datos al mismo tiempo, y no a cada elemento por separado. Esto permite un preprocesamiento m√°s r√°pido.

```py
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets
```

La manera en que la librer√≠a ü§ó Datasets aplica este procesamiento es a trav√©s de campos a√±adidos al conjunto de datos, uno por cada diccionario devuelto por la funci√≥n de preprocesamiento.

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})
```

Hasta puedes usar multiprocesamiento cuando aplicas la funci√≥n de preprocesamiento con `map()` pasando el argumento `num_proc`. Nosotros no usamos esta opci√≥n porque los tokenizadores de la librer√≠a ü§ó Tokenizers usa m√∫ltiples hilos de procesamiento para tokenizar r√°pidamente nuestros elementos, pero sino estas usando un tokenizador r√°pido respaldado por esta librer√≠a, esta opci√≥n puede acelerar tu preprocesamiento.

Nuestra funci√≥n `tokenize_function` devuelve un diccionario con las llaves `input_ids`, `attention_mask`, y `token_type_ids`, as√≠ que esos tres campos son adicionados a todas las divisiones de nuestro conjunto de datos. N√≥tese que pudimos haber cambiado los campos existentes si nuestra funci√≥n de preprocesamiento hubiese devuelto un valor nuevo para cualquiera de las llaves en el conjunto de datos al que le aplicamos `map()`.

Lo √∫ltimo que necesitamos hacer es rellenar todos los elementos hasta la longitud del elemento m√°s largo al momento de agrupar los elementos - a esta t√©cnica la llamamos *relleno din√°mico*.

### Relleno Din√°mico


**Video:** [Ver en YouTube](https://youtu.be/7q5NyFT8REg)


**PyTorch:**

La funci√≥n responsable de juntar los elementos dentro de un lote es llamada *funci√≥n de cotejo*. Esta es un argumento que puedes pasar cuando construyes un `DataLoader`, cuya funci√≥n por defecto convierte tus elementos a tensores PyTorch y los concatena (recursivamente si los elementos son listas, tuplas o diccionarios). Esto no ser√° posible en nuestro caso debido a que las entradas que tenemos no tienen el mismo tama√±o. Hemos pospuesto el relleno, para aplicarlo s√≥lo cuando se necesita en cada lote y evitar tener entradas muy largas con mucho relleno. Esto va a acelerar el entrenamiento significativamente, pero n√≥tese que esto puede causar problemas si est√°s entrenando en un TPU - Los TPUs prefieren tama√±os fijos, a√∫n cuando requieran relleno adicional.

**TensorFlow/Keras:**

La funci√≥n responsable de juntar los elementos dentro de un lote es llamada *funci√≥n de cotejo*. Esta es un argumento que puedes pasar cuando construyes un `DataLoader`, cuya funci√≥n por defecto convierte tus elementos a un tf.Tensor y los concatena (recursivamente si los elementos son listas, tuplas o diccionarios). Esto no ser√° posible en nuestro caso debido a que las entradas que tenemos no tienen el mismo tama√±o. Hemos pospuesto el relleno, para aplicarlo s√≥lo cuando se necesita en cada lote y evitar tener entradas muy largas con mucho relleno. Esto va a acelerar el entrenamiento significativamente, pero n√≥tese que esto puede causar problemas si est√°s entrenando en un TPU - Los TPUs prefieren tama√±os fijos, a√∫n cuando requieran relleno adicional.


Para poner esto en pr√°ctica, tenemos que definir una funci√≥n de cotejo que aplique la cantidad correcta de relleno a los elementos del conjunto de datos que queremos agrupar. Afortunadamente, la librer√≠a ü§ó Transformers nos provee esta funci√≥n mediante `DataCollatorWithPadding`. Esta recibe un tokenizador cuando la creas (para saber cual token de relleno se debe usar, y si el modelo espera el relleno a la izquierda o la derecha en las entradas) y hace todo lo que necesitas:


**PyTorch:**

```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

**TensorFlow/Keras:**

```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
```


Para probar este nuevo juguete, tomemos algunos elementos de nuestro conjunto de datos de entrenamiento para agruparlos. Aqu√≠, removemos las columnas `idx`, `sentence1`, and `sentence2` ya que √©stas no se necesitan y contienen cadenas (y no podemos crear tensores con cadenas), miremos las longitudes de cada elemento en el lote.

```py
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]
```

```python out
[50, 59, 47, 67, 59, 50, 62, 32]
```

Como era de esperarse, obtenemos elementos de longitud variable, desde 32 hasta 67. El relleno din√°mico significa que los elementos en este lote deben ser rellenos hasta una longitud de 67, que es la m√°xima longitud en el lote. Sin relleno din√°mico, todos los elementos tendr√≠an que haber sido rellenos hasta el m√°ximo de todo el conjunto de datos, o el m√°ximo aceptado por el modelo. Verifiquemos que nuestro `data_collator` esta rellenando din√°micamente el lote de la manera apropiada:

```py
batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}
```

{#if fw === 'tf'}

```python out
{'attention_mask': TensorShape([8, 67]),
 'input_ids': TensorShape([8, 67]),
 'token_type_ids': TensorShape([8, 67]),
 'labels': TensorShape([8])}
```

{:else}

```python out
{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}
```

¬°Luce bien! Ahora que hemos convertido el texto crudo a lotes que nuestro modelo puede aceptar, estamos listos para ajustarlo!

{/if}

> [!TIP]
> ‚úèÔ∏è **¬°Int√©ntalo!** Reproduce el preprocesamiento en el conjunto de datos GLUE SST-2. Es un poco diferente ya que esta compuesto de oraciones individuales en lugar de pares, pero el resto de lo que hicimos deber√≠a ser igual. Para un reto mayor, intenta escribir una funci√≥n de preprocesamiento que trabaje con cualquiera de las tareas GLUE.

{#if fw === 'tf'}

Ahora que tenemos nuestro conjunto de datos y el cotejador de datos, necesitamos juntarlos. Nosotros podr√≠amos cargar lotes de datos y cotejarlos, pero eso ser√≠a mucho trabajo, y probablemente no muy eficiente. En cambio, existe un m√©todo que ofrece una soluci√≥n eficiente para este problema: `to_tf_dataset()`. Este envuelve un `tf.data.Dataset` alrededor de tu conjunto de datos, con una funci√≥n opcional de cotejo. `tf.data.Dataset` es un formato nativo de TensorFlow que Keras puede usar con el `model.fit()`, as√≠ este m√©todo convierte inmediatamente un conjunto de datos ü§ó a un formato que viene listo para entrenamiento. Ve√°moslo en acci√≥n con nuestro conjunto de datos.

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

¬°Y eso es todo! Ahora podemos usar esos conjuntos de datos en nuestra pr√≥xima clase, donde el entrenamiento ser√° mas sencillo despu√©s de todo el trabajo de preprocesamiento de datos.

{/if}


---



# Ajuste de un modelo con la API Trainer


**Video:** [Ver en YouTube](https://youtu.be/nvBXf7s7vTI)


ü§ó Transformers incluye una clase `Trainer` para ayudarte a ajustar cualquiera de los modelos preentrenados proporcionados en tu dataset. Una vez que hayas hecho todo el trabajo de preprocesamiento de datos de la √∫ltima secci√≥n, s√≥lo te quedan unos pocos pasos para definir el `Trainer`. La parte m√°s dif√≠cil ser√° preparar el entorno para ejecutar `Trainer.train()`, ya que se ejecutar√° muy lentamente en una CPU. Si no tienes una GPU preparada, puedes acceder a GPUs o TPUs gratuitas en [Google Colab](https://colab.research.google.com/).

Los siguientes ejemplos de c√≥digo suponen que ya has ejecutado los ejemplos de la secci√≥n anterior. Aqu√≠ tienes un breve resumen de lo que necesitas:

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

### Entrenamiento

El primer paso antes de que podamos definir nuestro `Trainer` es definir una clase `TrainingArguments` que contendr√° todos los hiperpar√°metros que el `Trainer` utilizar√° para el entrenamiento y la evaluaci√≥n del modelo. El √∫nico argumento que tienes que proporcionar es el directorio donde se guardar√°n tanto el modelo entrenado como los puntos de control (checkpoints). Para los dem√°s par√°metros puedes dejar los valores por defecto, deber√≠an funcionar bastante bien para un ajuste b√°sico.

```py
from transformers import TrainingArguments

training_args = TrainingArguments("test-trainer")
```

> [!TIP]
> üí° Si quieres subir autom√°ticamente tu modelo al Hub durante el entrenamiento, incluye `push_to_hub=True` en `TrainingArguments`. Aprenderemos m√°s sobre esto en el [Cap√≠tulo 4](/course/chapter4/3).

El segundo paso es definir nuestro modelo. Como en el [cap√≠tulo anterior](/course/chapter2), utilizaremos la clase `AutoModelForSequenceClassification`, con dos etiquetas:

```py
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

Observar√°s que, a diferencia del [Cap√≠tulo 2](/course/chapter2), aparece una advertencia despu√©s de instanciar este modelo preentrenado. Esto se debe a que BERT no ha sido preentrenado para la clasificaci√≥n de pares de frases, por lo que la cabeza del modelo preentrenado se ha eliminado y en su lugar se ha a√±adido una nueva cabeza adecuada para la clasificaci√≥n de secuencias. Las advertencias indican que algunos pesos no se han utilizado (los correspondientes a la cabeza de preentrenamiento eliminada) y que otros se han inicializado aleatoriamente (los correspondientes a la nueva cabeza). La advertencia concluye anim√°ndote a entrenar el modelo, que es exactamente lo que vamos a hacer ahora.

Una vez que tenemos nuestro modelo, podemos definir un `Trainer` pas√°ndole todos los objetos construidos hasta ahora: el `model`, los `training_args`, los datasets de entrenamiento y validaci√≥n, nuestro `data_collator`, y nuestro `tokenizer`:

```py
from transformers import Trainer

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

Ten en cuenta que cuando pasas el `tokenizer` como hicimos aqu√≠, el `data_collator` por defecto utilizado por el `Trainer` ser√° un `DataCollatorWithPadding` como definimos anteriormente, por lo que puedes omitir la l√≠nea `data_collator=data_collator`. De todas formas, era importante mostrarte esta parte del proceso en la secci√≥n 2.

Para ajustar el modelo en nuestro dataset, s√≥lo tenemos que llamar al m√©todo `train()` de nuestro `Trainer`:

```py
trainer.train()
```

Esto iniciar√° el ajuste (que deber√≠a tardar un par de minutos en una GPU) e informar√° de la training loss cada 500 pasos. Sin embargo, no te dir√° lo bien (o mal) que est√° rindiendo tu modelo. Esto se debe a que:

1. No le hemos dicho al `Trainer` que eval√∫e el modelo durante el entrenamiento especificando un valor para `evaluation_strategy`: `steps` (evaluar cada `eval_steps`) o `epoch` (evaluar al final de cada √©poca).
2. No hemos proporcionado al `Trainer` una funci√≥n `compute_metrics()` para calcular una m√©trica durante dicha evaluaci√≥n (de lo contrario, la evaluaci√≥n s√≥lo habr√≠a impreso la p√©rdida, que no es un n√∫mero muy intuitivo).

### Evaluaci√≥n

Veamos c√≥mo podemos construir una buena funci√≥n `compute_metrics()` para utilizarla la pr√≥xima vez que entrenemos. La funci√≥n debe tomar un objeto `EvalPrediction` (que es una tupla nombrada con un campo `predictions` y un campo `label_ids`) y devolver√° un diccionario que asigna cadenas a flotantes (las cadenas son los nombres de las m√©tricas devueltas, y los flotantes sus valores). Para obtener algunas predicciones de nuestro modelo, podemos utilizar el comando `Trainer.predict()`:

```py
predictions = trainer.predict(tokenized_datasets["validation"])
print(predictions.predictions.shape, predictions.label_ids.shape)
```

```python out
(408, 2) (408,)
```

La salida del m√©todo `predict()` es otra tupla con tres campos: `predictions`, `label_ids`, y `metrics`. El campo `metrics` s√≥lo contendr√° la p√©rdida en el dataset proporcionado, as√≠ como algunas m√©tricas de tiempo (cu√°nto se tard√≥ en predecir, en total y de media). Una vez que completemos nuestra funci√≥n `compute_metrics()` y la pasemos al `Trainer`, ese campo tambi√©n contendr√° las m√©tricas devueltas por `compute_metrics()`.

Como puedes ver, `predictions` es una matriz bidimensional con forma 408 x 2 (408 es el n√∫mero de elementos del dataset que hemos utilizado). Esos son los logits de cada elemento del dataset que proporcionamos a `predict()` (como viste en el [cap√≠tulo anterior](/curso/cap√≠tulo2), todos los modelos Transformer devuelven logits). Para convertirlos en predicciones que podamos comparar con nuestras etiquetas, necesitamos tomar el √≠ndice con el valor m√°ximo en el segundo eje:

```py
import numpy as np

preds = np.argmax(predictions.predictions, axis=-1)
```

Ahora podemos comparar esas predicciones `preds` con las etiquetas. Para construir nuestra funci√≥n `compute_metric()`, nos basaremos en las m√©tricas de la librer√≠a ü§ó [Evaluate](https://github.com/huggingface/evaluate/). Podemos cargar las m√©tricas asociadas al dataset MRPC tan f√°cilmente como cargamos el dataset, esta vez con la funci√≥n `evaluate.load()`. El objeto devuelto tiene un m√©todo `compute()` que podemos utilizar para calcular de la m√©trica:

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
metric.compute(predictions=preds, references=predictions.label_ids)
```

```python out
{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}
```

Los resultados exactos que obtengas pueden variar, ya que la inicializaci√≥n aleatoria de la cabeza del modelo podr√≠a cambiar las m√©tricas obtenidas. Aqu√≠, podemos ver que nuestro modelo tiene una precisi√≥n del 85,78% en el conjunto de validaci√≥n y una puntuaci√≥n F1 de 89,97. Estas son las dos m√©tricas utilizadas para evaluar los resultados en el dataset MRPC para la prueba GLUE. La tabla del [paper de BERT](https://arxiv.org/pdf/1810.04805.pdf) recoge una puntuaci√≥n F1 de 88,9 para el modelo base. Se trataba del modelo "uncased" (el texto se reescribe en min√∫sculas antes de la tokenizaci√≥n), mientras que nosotros hemos utilizado el modelo "cased" (el texto se tokeniza sin reescribir), lo que explica el mejor resultado.

Junt√°ndolo todo obtenemos nuestra funci√≥n `compute_metrics()`:

```py
def compute_metrics(eval_preds):
    metric = evaluate.load("glue", "mrpc")
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```

Y para ver c√≥mo se utiliza para informar de las m√©tricas al final de cada √©poca, as√≠ es como definimos un nuevo `Trainer` con nuestra funci√≥n `compute_metrics()`:

```py
training_args = TrainingArguments("test-trainer", eval_strategy="epoch")
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

Ten en cuenta que hemos creado un nuevo `TrainingArguments` con su `evaluation_strategy` configurado como `"epoch"` y un nuevo modelo. De lo contrario s√≥lo estar√≠amos continuando el entrenamiento del modelo que ya hab√≠amos entrenado. Para lanzar una nueva ejecuci√≥n de entrenamiento, ejecutamos:

```py
trainer.train()
```

Esta vez, nos informar√° de la p√©rdida de validaci√≥n y las m√©tricas al final de cada √©poca, adem√°s de la p√©rdida de entrenamiento. De nuevo, la puntuaci√≥n exacta de precisi√≥n/F1 que alcances puede ser un poco diferente de la que encontramos nosotros, debido a la inicializaci√≥n aleatoria del modelo, pero deber√≠a estar en el mismo rango.

El `Trainer` funciona en m√∫ltiples GPUs o TPUs y proporciona muchas opciones, como el entrenamiento de precisi√≥n mixta (usa `fp16 = True` en tus argumentos de entrenamiento). Repasaremos todo lo que ofrece en el cap√≠tulo 10.

Con esto concluye la introducci√≥n al ajuste utilizando la API de `Trainer`. En el [Cap√≠tulo 7](/course/chapter7) se dar√° un ejemplo de c√≥mo hacer esto para las tareas m√°s comunes de PLN, pero ahora veamos c√≥mo hacer lo mismo en PyTorch puro.

> [!TIP]
> ‚úèÔ∏è **¬°Int√©ntalo!** Ajusta un modelo sobre el dataset GLUE SST-2 utilizando el procesamiento de datos que has implementado en la secci√≥n 2.


---



# Ajuste de un modelo con Keras


Una vez que hayas realizado todo el trabajo de preprocesamiento de datos de la √∫ltima secci√≥n, s√≥lo te quedan unos pocos pasos para entrenar el modelo. Sin embargo, ten en cuenta que el comando `model.fit()` se ejecutar√° muy lentamente en una CPU. Si no dispones de una GPU, puedes acceder a GPUs o TPUs gratuitas en [Google Colab](https://colab.research.google.com/).

Los siguientes ejemplos de c√≥digo suponen que ya has ejecutado los ejemplos de la secci√≥n anterior. Aqu√≠ tienes un breve resumen de lo que necesitas:

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding
import numpy as np

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")

tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

### Entrenamiento

Los modelos TensorFlow importados de ü§ó Transformers ya son modelos Keras. A continuaci√≥n, una breve introducci√≥n a Keras.


**Video:** [Ver en YouTube](https://youtu.be/rnTGBy2ax1c)


Eso significa que, una vez que tenemos nuestros datos, se requiere muy poco trabajo para empezar a entrenar con ellos.


**Video:** [Ver en YouTube](https://youtu.be/AUozVp78dhk)


Como en el [cap√≠tulo anterior](/course/chapter2), utilizaremos la clase `TFAutoModelForSequenceClassification`, con dos etiquetas:

```py
from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

Observar√°s que, a diferencia del [Cap√≠tulo 2](/course/es/chapter2), aparece una advertencia despu√©s de instanciar este modelo preentrenado. Esto se debe a que BERT no ha sido preentrenado para la clasificaci√≥n de pares de frases, por lo que la cabeza del modelo preentrenado se ha eliminado y en su lugar se ha a√±adido una nueva cabeza adecuada para la clasificaci√≥n de secuencias. Las advertencias indican que algunos pesos no se han utilizado (los correspondientes a la cabeza de preentrenamiento eliminada) y que otros se han inicializado aleatoriamente (los correspondientes a la nueva cabeza). La advertencia concluye anim√°ndote a entrenar el modelo, que es exactamente lo que vamos a hacer ahora.

Para afinar el modelo en nuestro dataset, s√≥lo tenemos que compilar nuestro modelo con `compile()` y luego pasar nuestros datos al m√©todo `fit()`. Esto iniciar√° el proceso de ajuste (que deber√≠a tardar un par de minutos en una GPU) e informar√° de la p√©rdida de entrenamiento a medida que avanza, adem√°s de la p√©rdida de validaci√≥n al final de cada √©poca.

> [!TIP]
> Ten en cuenta que los modelos ü§ó Transformers tienen una caracter√≠stica especial que la mayor√≠a de los modelos Keras no tienen - pueden usar autom√°ticamente una p√©rdida apropiada que calculan internamente. Usar√°n esta p√©rdida por defecto si no estableces un argumento de p√©rdida en `compile()`. Tea en cuenta que para utilizar la p√©rdida interna tendr√°s que pasar las etiquetas como parte de la entrada, en vez de como una etiqueta separada como es habitual en los modelos Keras. Veremos ejemplos de esto en la Parte 2 del curso, donde definir la funci√≥n de p√©rdida correcta puede ser complicado. Para la clasificaci√≥n de secuencias, sin embargo, una funci√≥n de p√©rdida est√°ndar de Keras funciona bien, as√≠ que eso es lo que usaremos aqu√≠.

```py
from tensorflow.keras.losses import SparseCategoricalCrossentropy

model.compile(
    optimizer="adam",
    loss=SparseCategoricalCrossentropy(from_logits=True),
    metrics=["accuracy"],
)
model.fit(
    tf_train_dataset,
    validation_data=tf_validation_dataset,
)
```

> [!WARNING]
> Ten en cuenta un fallo muy com√∫n aqu√≠: por poder, _puedes_ pasar simplemente el nombre de la funci√≥n de p√©rdida como una cadena a Keras, pero por defecto Keras asumir√° que ya has aplicado una funci√≥n softmax a tus salidas. Sin embargo, muchos modelos devuelven los valores justo antes de que se aplique la funci√≥n softmax, tambi√©n conocidos como _logits_. Tenemos que decirle a la funci√≥n de p√©rdida que eso es lo que hace nuestro modelo, y la √∫nica manera de hacerlo es llam√°ndola directamente, en lugar de pasar su nombre con una cadena.

### Mejorar el rendimiento del entrenamiento


**Video:** [Ver en YouTube](https://youtu.be/cpzq6ESSM5c)


Si ejecutas el c√≥digo anterior seguro que funciona, pero comprobar√°s que la p√©rdida s√≥lo disminuye lenta o espor√°dicamente. La causa principal es la _tasa de aprendizaje_ (learning rate en ingl√©s). Al igual que con la p√©rdida, cuando pasamos a Keras el nombre de un optimizador como una cadena, Keras inicializa ese optimizador con valores por defecto para todos los par√°metros, incluyendo la tasa de aprendizaje. Sin embargo, por experiencia sabemos que los transformadores se benefician de una tasa de aprendizaje mucho menor que la predeterminada para Adam, que es 1e-3, tambi√©n escrito
como 10 a la potencia de -3, o 0,001. 5e-5 (0,00005), que es unas veinte veces menor, es un punto de partida mucho mejor.

Adem√°s de reducir la tasa de aprendizaje, tenemos un segundo truco en la manga: podemos reducir lentamente la tasa de aprendizaje a lo largo del entrenamiento. En la literatura, a veces se habla de _decrecimiento_ o _reducci√≥n_ de la tasa de aprendizaje. En Keras, la mejor manera de hacer esto es utilizar un _programador de tasa de aprendizaje_. Una buena opci√≥n es `PolynomialDecay` que, a pesar del nombre, con la configuraci√≥n por defecto simplemente hace que la tasa de aprendizaje decaiga decae linealmente desde el valor inicial hasta el valor final durante el transcurso del entrenamiento, que es exactamente lo que queremos. Con el fin de utilizar un programador correctamente, necesitamos decirle cu√°nto tiempo va a durar el entrenamiento. Especificamos esto a continuaci√≥n como el n√∫mero de pasos de entrenamiento: `num_train_steps`.

```py
from tensorflow.keras.optimizers.schedules import PolynomialDecay

batch_size = 8
num_epochs = 3

# El n√∫mero de pasos de entrenamiento es el n√∫mero de muestras del conjunto de datos
# dividido por el tama√±o del lote y multiplicado por el n√∫mero total de √©pocas.
# Ten en cuenta que el conjunto de datos tf_train_dataset es un conjunto de datos
# tf.data.Dataset por lotes, no el conjunto de datos original de Hugging Face
# por lo que su len() ya es num_samples // batch_size.

num_train_steps = len(tf_train_dataset) * num_epochs
lr_scheduler = PolynomialDecay(
    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps
)
from tensorflow.keras.optimizers import Adam

opt = Adam(learning_rate=lr_scheduler)
```

> [!TIP]
> La librer√≠a ü§ó Transformers tambi√©n tiene una funci√≥n `create_optimizer()` que crear√° un optimizador `AdamW` con descenso de tasa de aprendizaje. Ver√°s en detalle este √∫til atajo en pr√≥ximas secciones del curso.

Ahora tenemos nuestro nuevo optimizador, y podemos intentar entrenar con √©l. En primer lugar, vamos a recargar el modelo, para restablecer los cambios en los pesos del entrenamiento que acabamos de hacer, y luego podemos compilarlo con el nuevo optimizador:

```py
import tensorflow as tf

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer=opt, loss=loss, metrics=["accuracy"])
```

Ahora, ajustamos de nuevo:

```py
model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)
```

> [!TIP]
> üí° Si quieres subir autom√°ticamente tu modelo a Hub durante el entrenamiento, puedes pasar un `PushToHubCallback` en el m√©todo `model.fit()`. Aprenderemos m√°s sobre esto en el [Cap√≠tulo 4](/course/es/chapter4/3)

### Predicciones del Modelo[[model-predictions]]


**Video:** [Ver en YouTube](https://youtu.be/nx10eh4CoOs)


Entrenar y ver c√≥mo disminuye la p√©rdida est√° muy bien, pero ¬øqu√© pasa si queremos obtener la salida del modelo entrenado, ya sea para calcular algunas m√©tricas o para utilizar el modelo en producci√≥n? Para ello, podemos utilizar el m√©todo `predict()`. Este devuelve los _logits_ de la cabeza de salida del modelo, uno por clase.

```py
preds = model.predict(tf_validation_dataset)["logits"]
```

Podemos convertir estos logits en predicciones de clases del modelo utilizando `argmax` para encontrar el logit m√°s alto, que corresponde a la clase m√°s probable:

```py
class_preds = np.argmax(preds, axis=1)
print(preds.shape, class_preds.shape)
```

```python out
(408, 2) (408,)
```

Ahora, ¬°utilicemos esos `preds` (predicciones) para calcular m√©tricas! Podemos cargar las m√©tricas asociadas al conjunto de datos MRPC tan f√°cilmente como cargamos el conjunto de datos, esta vez con la funci√≥n `evaluate.load()`. El objeto devuelto tiene un m√©todo `compute()` que podemos utilizar para calcular las m√©tricas:

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
metric.compute(predictions=class_preds, references=raw_datasets["validation"]["label"])
```

```python out
{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}
```

Los resultados exactos que obtengas pueden variar, ya que la inicializaci√≥n aleatoria de la cabeza del modelo podr√≠a cambiar los valores resultantes de las m√©tricas. Aqu√≠, podemos ver que nuestro modelo tiene una precisi√≥n del 85,78% en el conjunto de validaci√≥n y una puntuaci√≥n F1 de 89,97. Estas son las dos m√©tricas utilizadas para evaluar los resultados del conjunto de datos MRPC del benchmark GLUE. La tabla del [paper de BERT](https://arxiv.org/pdf/1810.04805.pdf) muestra una puntuaci√≥n F1 de 88,9 para el modelo base. Se trataba del modelo `uncased` ("no encasillado"), mientras que nosotros utilizamos el modelo `cased` ("encasillado"), lo que explica el mejor resultado.

Con esto concluye la introducci√≥n al ajuste de modelos utilizando la API de Keras. En el [Cap√≠tulo 7](/course/es/chapter7) se dar√° un ejemplo de c√≥mo hacer esto para las tareas de PLN m√°s comunes. Si quieres perfeccionar tus habilidades con la API Keras, intenta ajustar un modelo con el conjunto de datos GLUE SST-2, utilizando el procesamiento de datos que hiciste en la secci√≥n 2.


---

# Un entrenamiento completo


**Video:** [Ver en YouTube](https://youtu.be/Dh9CL8fyG80)


Ahora veremos como obtener los mismos resultados de la √∫ltima secci√≥n sin hacer uso de la clase `Trainer`. De nuevo, asumimos que has hecho el procesamiento de datos en la secci√≥n 2. Aqu√≠ mostramos un resumen que cubre todo lo que necesitar√°s.

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

### Prep√°rate para el entrenamiento

Antes de escribir nuestro bucle de entrenamiento, necesitaremos definir algunos objetos. Los primeros son los `dataloaders` (literalmente, "cargadores de datos") que usaremos para iterar sobre lotes. Pero antes de que podamos definir esos `dataloaders`, necesitamos aplicar un poquito de preprocesamiento a nuestro `tokenized_datasets`, para encargarnos de algunas cosas que el `Trainer` hizo por nosotros de manera autom√°tica. Espec√≠ficamente, necesitamos:

- Remover las columnas correspondientes a valores que el model no espera (como las columnas `sentence1` y `sentence2`).
- Renombrar la columna `label` con `labels` (porque el modelo espera el argumento llamado `labels`).
- Configurar el formato de los conjuntos de datos para que retornen tensores PyTorch en lugar de listas.

Nuestro `tokenized_datasets` tiene un m√©todo para cada uno de esos pasos:

```py
tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")
tokenized_datasets["train"].column_names
```

Ahora podemos verificar que el resultado solo tiene columnas que nuestro modelo aceptar√°:

```python
["attention_mask", "input_ids", "labels", "token_type_ids"]
```

Ahora que esto esta hecho, es f√°cil definir nuestros `dataloaders`:

```py
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
)
```

Para verificar r√°pidamente que no hubo errores en el procesamiento de datos, podemos inspeccionar un lote de la siguiente manera:

```py
for batch in train_dataloader:
    break
{k: v.shape for k, v in batch.items()}
```

```python out
{'attention_mask': torch.Size([8, 65]),
 'input_ids': torch.Size([8, 65]),
 'labels': torch.Size([8]),
 'token_type_ids': torch.Size([8, 65])}
```

N√≥tese que los tama√±os ser√°n un poco distintos en tu caso ya que configuramos `shuffle=True` para el dataloader de entrenamiento y estamos rellenando a la m√°xima longitud dentro del lote.

Ahora que hemos completado el preprocesamiento de datos (un objetivo gratificante y al mismo tiempo elusivo para cual cualquier practicante de ML), enfoqu√©monos en el modelo. Lo vamos a crear exactamente como lo hicimos en la secci√≥n anterior.

```py
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

Para asegurarnos de que todo va a salir sin problems durante el entrenamiento, vamos a pasar un lote a este modelo:

```py
outputs = model(**batch)
print(outputs.loss, outputs.logits.shape)
```

```python out
tensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2])
```

Todos los modelos ü§ó Transformers van a retornar la p√©rdida cuando se pasan los `labels`, y tambi√©n obtenemos los logits (dos por cada entrada en nuestro lote, asi que es un tensor de tama√±o 8 x 2).

Estamos casi listos para escribir nuestro bucle de entrenamiento! Nos est√°n faltando dos cosas: un optimizador y un programador de la tasa de aprendizaje. Ya que estamos tratando de replicar a mano lo que el `Trainer` estaba haciendo, usaremos los mismos valores por defecto. El optimizador usado por el `Trainer` es `AdamW`, que es el mismo que Adam, pero con un cambio para la regularizaci√≥n de decremento de los pesos (ver ["Decoupled Weight Decay Regularization"](https://arxiv.org/abs/1711.05101) por Ilya Loshchilov y Frank Hutter):

```py
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
```

Finalmente, el programador por defecto de la tasa de aprendizaje es un decremento lineal desde al valor m√°ximo (5e-5) hasta 0. Para definirlo apropiadamente, necesitamos saber el n√∫mero de pasos de entrenamiento que vamos a tener, el cual viene dado por el n√∫mero de √©pocas que deseamos correr multiplicado por el n√∫mero de lotes de entrenamiento (que es el largo de nuestro dataloader de entrenamiento). El `Trainer` usa tres √©pocas por defecto, asi que usaremos eso:

```py
from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
print(num_training_steps)
```

```python out
1377
```

### El bucle de entrenamiento

Una √∫ltima cosa: vamos a querer usar el GPU si tenemos acceso a uno (en un CPU, el entrenamiento puede tomar varias horas en lugar de unos pocos minutos). Para hacer esto, definimos un `device` sobre el que pondremos nuestro modelo y nuestros lotes:

```py
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
device
```

```python out
device(type='cuda')
```

¬°Ya est√° todo listo para entrenar! Para tener una idea de cu√°ndo va a terminar el entrenamiento, adicionamos una barra de progreso sobre el n√∫mero de pasos de entrenamiento, usando la librer√≠a `tqdm`:

```py
from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

Puedes ver que la parte central del bucle de entrenamiento luce bastante como el de la introducci√≥n. No se incluy√≥ ning√∫n tipo de reportes, asi que este bucle de entrenamiento no va a indicar como se esta desempe√±ando el modelo. Para eso necesitamos a√±adir un bucle de evaluaci√≥n.

### El bucle de evaluaci√≥n

Como lo hicimos anteriormente, usaremos una m√©trica ofrecida por la librer√≠a ü§ó Evaluate. Ya hemos visto el m√©todo `metric.compute()`, pero de hecho las m√©tricas se pueden acumular sobre los lotes a medida que avanzamos en el bucle de predicci√≥n con el m√©todo `add_batch()`. Una vez que hemos acumulado todos los lotes, podemos obtener el resultado final con `metric.compute()`. Aqu√≠ se muestra c√≥mo se puede implementar en un bucle de evaluaci√≥n:

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
model.eval()
for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()
```

```python out
{'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}
```

De nuevo, tus resultados ser√°n un tanto diferente debido a la inicializaci√≥n aleatoria en la cabeza del modelo y el mezclado de los datos, pero deber√≠an tener valores similares.

> [!TIP]
> ‚úèÔ∏è **Int√©ntalo!** Modifica el bucle de entrenamiento anterior para ajustar tu modelo en el conjunto de datos SST-2.

### Repotencia tu bucle de entrenamiento con Accelerate ü§ó


**Video:** [Ver en YouTube](https://youtu.be/s7dy8QRgjJ0)


El bucle de entrenamiento que definimos anteriormente trabaja bien en una sola CPU o GPU. Pero usando la librer√≠a [Accelerate ü§ó](https://github.com/huggingface/accelerate), con solo pocos ajustes podemos habilitar el entrenamiento distribuido en m√∫ltiples GPUs o CPUs. Comenzando con la creaci√≥n de los dataloaders de entrenamiento y validaci√≥n, aqu√≠ se muestra como luce nuestro bucle de entrenamiento:

```py
from torch.optim import AdamW
from transformers import AutoModelForSequenceClassification, get_scheduler

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

Y aqu√≠ est√°n los cambios:

```diff
+ from accelerate import Accelerator
  from torch.optim import AdamW
  from transformers import AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
  optimizer = AdamW(model.parameters(), lr=3e-5)

- device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

  num_epochs = 3
  num_training_steps = num_epochs * len(train_dataloader)
  lr_scheduler = get_scheduler(
      "linear",
      optimizer=optimizer,
      num_warmup_steps=0,
      num_training_steps=num_training_steps
  )

  progress_bar = tqdm(range(num_training_steps))

  model.train()
  for epoch in range(num_epochs):
      for batch in train_dataloader:
-         batch = {k: v.to(device) for k, v in batch.items()}
          outputs = model(**batch)
          loss = outputs.loss
-         loss.backward()
+         accelerator.backward(loss)

          optimizer.step()
          lr_scheduler.step()
          optimizer.zero_grad()
          progress_bar.update(1)
```

La primera l√≠nea a agregarse es la l√≠nea del `import`. La segunda l√≠nea crea un objeto `Accelerator` que revisa el ambiente e inicializa la configuraci√≥n distribuida apropiada. La librer√≠a ü§ó Accelerate se encarga de asignarte el dispositivo, para que puedas remover las l√≠neas que ponen el modelo en el dispositivo (o si prefieres, c√°mbialas para usar el `accelerator.device` en lugar de `device`).

Ahora la mayor parte del trabajo se hace en la l√≠nea que env√≠a los `dataloaders`, el modelo y el optimizador al `accelerator.prepare()`. Este va a envolver esos objetos en el contenedor apropiado para asegurarse que tu entrenamiento distribuido funcione como se espera. Los cambios que quedan son remover la l√≠nea que coloca el lote en el `device` (de nuevo, si deseas dejarlo as√≠ bastar√≠a con cambiarlo para que use el `accelerator.device`) y reemplazar `loss.backward()` con `accelerator.backward(loss)`.

> [!TIP]
> ‚ö†Ô∏è Para obtener el beneficio de la aceleraci√≥n ofrecida por los TPUs de la
>   nube, recomendamos rellenar las muestras hasta una longitud fija con los
>   argumentos `padding="max_length"` y `max_length` del tokenizador.

Si deseas copiarlo y pegarlo para probar, as√≠ es como luce el bucle completo de entrenamiento con ü§ó Accelerate:

```py
from accelerate import Accelerator
from torch.optim import AdamW
from transformers import AutoModelForSequenceClassification, get_scheduler

accelerator = Accelerator()

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

train_dl, eval_dl, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dl)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dl:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

Colocando esto en un script `train.py` permitir√° que el mismo sea ejecutable en cualquier configuraci√≥n distribuida. Para probarlo en tu configuraci√≥n distribuida, ejecuta el siguiente comando:

```bash
accelerate config
```

el cual har√° algunas preguntas y guardar√° tus respuestas en un archivo de configuraci√≥n usado por este comando:

```
accelerate launch train.py
```

el cual iniciar√° en entrenamiento distribuido.

Si deseas ejecutar esto en un Notebook (por ejemplo, para probarlo con TPUs en Colab), solo pega el c√≥digo en una `training_function()` y ejecuta la √∫ltima celda con:

```python
from accelerate import notebook_launcher

notebook_launcher(training_function)
```

Puedes encontrar m√°s ejemplos en el [repositorio ü§ó Accelerate](https://github.com/huggingface/accelerate/tree/main/examples).


---



# Ajuste de modelos, ¬°hecho!


¬°Qu√© divertido! En los dos primeros cap√≠tulos aprendiste sobre modelos y tokenizadores, y ahora sabes c√≥mo ajustarlos a tus propios datos. Para recapitular, en este cap√≠tulo:


**PyTorch:**

- Aprendiste sobre los conjuntos de datos del [Hub](https://huggingface.co/datasets)
- Aprendiste a cargar y preprocesar conjuntos de datos, incluyendo el uso de padding din√°mico y los "collators"
- Implementaste tu propio ajuste (fine-tuning) y c√≥mo evaluar un modelo
- Implementaste un bucle de entrenamiento de bajo nivel
- Utilizaste ü§ó Accelerate para adaptar f√°cilmente tu bucle de entrenamiento para que funcione en m√∫ltiples GPUs o TPUs

**TensorFlow/Keras:**

- Aprendiste sobre los conjuntos de datos en [Hub](https://huggingface.co/datasets)
- Aprendiste a cargar y preprocesar conjuntos de datos
- Aprendiste a ajustar (fine-tuning) y evaluar un modelo con Keras
- Implementaste una m√©trica personalizada



---




# Quiz de final de cap√≠tulo


A ver qu√© has aprendido en este cap√≠tulo:

### 1. El dataset `emotion` contiene mensajes de Twitter etiquetados con emociones. B√∫scalo en el [Hub](https://huggingface.co/datasets), y lee la tarjeta del dataset. ¬øCu√°l de estas no es una de sus emociones b√°sicas?


- Alegr√≠a
- Amor
- Confusi√≥n
- Sorpresa


### 2. Busca el dataset `ar_sarcasm` en el [Hub](https://huggingface.co/datasets). ¬øCon qu√© tarea es compatible?


- Clasificaci√≥n de sentimientos
- Traducci√≥n autom√°tica
- Reconocimiento de entidades nombradas
- Responder preguntas


### 3. ¬øC√≥mo se procesan un par de frases seg√∫n el modelo BERT?


- tokens_frase_1 [SEP] tokens_frase_2
- [CLS] tokens_frase_1 tokens_frase_2
- [CLS] tokens_frase_1 [SEP] tokens_frase_2 [SEP]
- [CLS] tokens_frase_1 [SEP] tokens_frase_2


**PyTorch:**

### 4. ¬øCu√°les son las ventajas del m√©todo `Dataset.map()`?


- Los resultados de la funci√≥n se almacenan en cach√©, por lo que no tardaremos nada en volver a ejecutar el c√≥digo.
- Puede aplicar multiprocesamiento para ir m√°s r√°pido que si se aplicara la funci√≥n a cada elemento del conjunto de datos.
- No carga todo el conjunto de datos en memoria, sino que guarda los resultados en cuanto se procesa un elemento.


### 5. ¬øQu√© significa padding din√°mico?


- Es cuando se rellenan las entradas de cada lote con la longitud m√°xima de todo el conjunto de datos.
- Es cuando rellenas tus entradas cuando se crea el lote, a la longitud m√°xima de las frases de ese lote.
- Es cuando se rellenan las entradas para que cada frase tenga el mismo n√∫mero de tokens que la anterior en el conjunto de datos.


### 6. ¬øCu√°l es el objetivo de la funci√≥n "collate"?


- Se asegura de que todas las secuencias del conjunto de datos tengan la misma longitud.
- Combina todas las muestras del conjunto de datos en un lote.
- Preprocesa todo el conjunto de datos.
- Trunca las secuencias del conjunto de datos.


### 7. ¬øQu√© ocurre cuando instancias una de las clases `AutoModelForXxx` con un modelo del lenguaje preentrenado (como `bert-base-uncased`) que corresponde a una tarea distinta de aquella para la que fue entrenado?


- Nada, pero recibes una advertencia.
- La cabeza del modelo preentrenado se elimina y en su lugar se inserta una nueva cabeza adecuada para la tarea.
- La cabeza del modelo preentrenado es eliminada.
- Nada, ya que el modelo se puede seguir ajustando para la otra tarea.


### 8. ¬øPara qu√© sirve `TrainingArguments`?


- Contiene todos los hiperpar√°metros utilizados para el entrenamiento y la evaluaci√≥n con `Trainer`.
- Especifica el tama√±o del modelo.
- Solo contiene los hiperpar√°metros utilizados para la evaluaci√≥n.
- Solo contiene los hiperpar√°metros utilizados para el entrenamiento.


### 9. ¬øPor qu√© deber√≠as utilizar la librer√≠a ü§ó Accelerate?


- Facilita acceso a modelos m√°s r√°pidos.
- Proporciona una API de alto nivel para que no tenga que implementar mi propio bucle de entrenamiento.
- Hace que nuestros bucles de entrenamiento funcionen con estrategias distribuidas.
- Ofrece m√°s funciones de optimizaci√≥n.


**TensorFlow/Keras:**

### 4. ¬øQu√© ocurre cuando instancias una de las clases `TFAutoModelForXxx` con un modelo del lenguaje preentrenado (como `bert-base-uncased`) que corresponde a una tarea distinta de aquella para la que fue entrenado?


- Nada, pero recibes una advertencia.
- La cabeza del modelo preentrenado se elimina y en su lugar se inserta una nueva cabeza adecuada para la tarea.
- La cabeza del modelo preentrenado es eliminada
- Nada, ya que el modelo se puede seguir ajustando para la otra tarea.


### 5. Los modelos TensorFlow de `transformers` ya son modelos Keras. ¬øQu√© ventajas ofrece esto?


- Los modelos funcionan directamente en una TPU.
- Puede aprovechar los m√©todos existentes, como `compile()`, `fit()` y `predict()`.
- Tienes la oportunidad de aprender Keras a la vez que transformadores.
- Puede calcular f√°cilmente las m√©tricas relacionadas con el dataset.


### 6. ¬øC√≥mo puedes definir tu propia m√©trica personalizada?


- Creando una subclase de `tf.keras.metrics.Metric`.
- Utilizando la API funcional de Keras.
- Utilizando una funci√≥n cuya firma sea `metric_fn(y_true, y_pred)`.
- Busc√°ndolo en Google.




---

# 4. Compartiendo modelos y tokenizadores

# El Hub de Hugging Face[[the-hugging-face-hub]]


El [Hub de Hugging Face](https://huggingface.co/) ‚Äì- nuestro sitio web principal ‚Äì- es una plataforma central que permite a cualquier persona descubrir, usar y contribuir con nuevos modelos y conjuntos de datos de ultima generacion. Aloja una amplia variedad de modelos, con mas de 10,000 disponibles publicamente. Nos centraremos en los modelos en este capitulo, y examinaremos los conjuntos de datos en el Capitulo 5.

Los modelos en el Hub no estan limitados a ü§ó Transformers o incluso a NLP. Hay modelos de [Flair](https://github.com/flairNLP/flair) y [AllenNLP](https://github.com/allenai/allennlp) para NLP, [Asteroid](https://github.com/asteroid-team/asteroid) y [pyannote](https://github.com/pyannote/pyannote-audio) para audio, y [timm](https://github.com/rwightman/pytorch-image-models) para vision, por nombrar algunos.

Cada uno de estos modelos esta alojado como un repositorio Git, lo que permite versionado y reproducibilidad. Compartir un modelo en el Hub significa abrirlo a la comunidad y hacerlo accesible a cualquier persona que busque usarlo facilmente, a su vez eliminando su necesidad de entrenar un modelo por su cuenta y simplificando el intercambio y uso.

Adicionalmente, compartir un modelo en el Hub automaticamente despliega una API de Inferencia alojada para ese modelo. Cualquier persona en la comunidad es libre de probarlo directamente en la pagina del modelo, con entradas personalizadas y widgets apropiados.

La mejor parte es que compartir y usar cualquier modelo publico en el Hub es completamente gratis! Tambien existen [planes pagos](https://huggingface.co/pricing) si deseas compartir modelos de forma privada.

El video a continuacion muestra como navegar por el Hub.


**Video:** [Ver en YouTube](https://youtu.be/XvSGPZFEjDY)


Se requiere tener una cuenta de huggingface.co para seguir esta parte, ya que crearemos y administraremos repositorios en el Hub de Hugging Face: [crear una cuenta](https://huggingface.co/join)


---



# Usando modelos preentrenados[[using-pretrained-models]]


El Model Hub hace que seleccionar el modelo apropiado sea simple, de modo que usarlo en cualquier biblioteca posterior se pueda hacer en unas pocas lineas de codigo. Veamos como usar realmente uno de estos modelos, y como contribuir de vuelta a la comunidad.

Digamos que estamos buscando un modelo basado en frances que pueda realizar relleno de mascara.

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/camembert.gif" alt="Seleccionando el modelo Camembert." width="80%"/>
</div>

Seleccionamos el checkpoint `camembert-base` para probarlo. El identificador `camembert-base` es todo lo que necesitamos para comenzar a usarlo! Como has visto en capitulos anteriores, podemos instanciarlo usando la funcion `pipeline()`:

```py
from transformers import pipeline

camembert_fill_mask = pipeline("fill-mask", model="camembert-base")
results = camembert_fill_mask("Le camembert est <mask> :)")
```

```python out
[
  {'sequence': 'Le camembert est d√©licieux :)', 'score': 0.49091005325317383, 'token': 7200, 'token_str': 'd√©licieux'},
  {'sequence': 'Le camembert est excellent :)', 'score': 0.1055697426199913, 'token': 2183, 'token_str': 'excellent'},
  {'sequence': 'Le camembert est succulent :)', 'score': 0.03453313186764717, 'token': 26202, 'token_str': 'succulent'},
  {'sequence': 'Le camembert est meilleur :)', 'score': 0.0330314114689827, 'token': 528, 'token_str': 'meilleur'},
  {'sequence': 'Le camembert est parfait :)', 'score': 0.03007650189101696, 'token': 1654, 'token_str': 'parfait'}
]
```

Como puedes ver, cargar un modelo dentro de un pipeline es extremadamente simple. Lo unico que debes tener en cuenta es que el checkpoint elegido sea adecuado para la tarea en la que se va a usar. Por ejemplo, aqui estamos cargando el checkpoint `camembert-base` en el pipeline `fill-mask`, lo cual es completamente correcto. Pero si cargaramos este checkpoint en el pipeline `text-classification`, los resultados no tendrian ningun sentido porque la cabeza de `camembert-base` no es adecuada para esta tarea! Recomendamos usar el selector de tareas en la interfaz del Hub de Hugging Face para seleccionar los checkpoints apropiados:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/tasks.png" alt="El selector de tareas en la interfaz web." width="80%"/>
</div>

Tambien puedes instanciar el checkpoint usando la arquitectura del modelo directamente:


**PyTorch:**

```py
from transformers import CamembertTokenizer, CamembertForMaskedLM

tokenizer = CamembertTokenizer.from_pretrained("camembert-base")
model = CamembertForMaskedLM.from_pretrained("camembert-base")
```

Sin embargo, recomendamos usar las [clases `Auto*`](https://huggingface.co/transformers/model_doc/auto?highlight=auto#auto-classes) en su lugar, ya que estan disenadas para ser agnosticas de la arquitectura. Mientras el ejemplo de codigo anterior limita a los usuarios a checkpoints cargables en la arquitectura CamemBERT, usar las clases `Auto*` hace que cambiar checkpoints sea simple:

```py
from transformers import AutoTokenizer, AutoModelForMaskedLM

tokenizer = AutoTokenizer.from_pretrained("camembert-base")
model = AutoModelForMaskedLM.from_pretrained("camembert-base")
```

**TensorFlow/Keras:**

```py
from transformers import CamembertTokenizer, TFCamembertForMaskedLM

tokenizer = CamembertTokenizer.from_pretrained("camembert-base")
model = TFCamembertForMaskedLM.from_pretrained("camembert-base")
```

Sin embargo, recomendamos usar las [clases `TFAuto*`](https://huggingface.co/transformers/model_doc/auto?highlight=auto#auto-classes) en su lugar, ya que estan disenadas para ser agnosticas de la arquitectura. Mientras el ejemplo de codigo anterior limita a los usuarios a checkpoints cargables en la arquitectura CamemBERT, usar las clases `TFAuto*` hace que cambiar checkpoints sea simple:

```py
from transformers import AutoTokenizer, TFAutoModelForMaskedLM

tokenizer = AutoTokenizer.from_pretrained("camembert-base")
model = TFAutoModelForMaskedLM.from_pretrained("camembert-base")
```


> [!TIP]
> Cuando uses un modelo preentrenado, asegurate de verificar como fue entrenado, en que conjuntos de datos, sus limites y sus sesgos. Toda esta informacion deberia estar indicada en su tarjeta de modelo.


---



# Compartiendo modelos preentrenados[[sharing-pretrained-models]]


En los pasos a continuacion, examinaremos las formas mas faciles de compartir modelos preentrenados en el ü§ó Hub. Hay herramientas y utilidades disponibles que hacen simple compartir y actualizar modelos directamente en el Hub, las cuales exploraremos a continuacion.


**Video:** [Ver en YouTube](https://youtu.be/9yY3RB_GSPM)


Animamos a todos los usuarios que entrenan modelos a contribuir compartiendolos con la comunidad ‚Äî compartir modelos, incluso cuando son entrenados en conjuntos de datos muy especificos, ayudara a otros, ahorrandoles tiempo y recursos computacionales y proporcionando acceso a artefactos entrenados utiles. A su vez, tu puedes beneficiarte del trabajo que otros han hecho!

Hay tres formas de crear nuevos repositorios de modelos:

- Usando la API `push_to_hub`
- Usando la biblioteca Python `huggingface_hub`
- Usando la interfaz web

Una vez que hayas creado un repositorio, puedes subir archivos a el via git y git-lfs. Te guiaremos a traves de la creacion de repositorios de modelos y la subida de archivos a ellos en las siguientes secciones.

## Usando la API `push_to_hub`[[using-the-pushtohub-api]]


**PyTorch:**

**Video:** [Ver en YouTube](https://youtu.be/Zh0FfmVrKX0)

**TensorFlow/Keras:**

**Video:** [Ver en YouTube](https://youtu.be/pUh5cGmNV8Y)


La forma mas simple de subir archivos al Hub es aprovechando la API `push_to_hub`.

Antes de continuar, necesitaras generar un token de autenticacion para que la API `huggingface_hub` sepa quien eres y a que espacios de nombres tienes acceso de escritura. Asegurate de estar en un entorno donde tengas `transformers` instalado (ver [Configuracion](/course/chapter0)). Si estas en un notebook, puedes usar la siguiente funcion para iniciar sesion:

```python
from huggingface_hub import notebook_login

notebook_login()
```

En una terminal, puedes ejecutar:

```bash
huggingface-cli login
```

En ambos casos, se te pedira tu nombre de usuario y contrasena, que son los mismos que usas para iniciar sesion en el Hub. Si aun no tienes un perfil en el Hub, deberias crear uno [aqui](https://huggingface.co/join).

Genial! Ahora tienes tu token de autenticacion almacenado en tu carpeta de cache. Creemos algunos repositorios!


**PyTorch:**

Si has jugado con la API `Trainer` para entrenar un modelo, la forma mas facil de subirlo al Hub es establecer `push_to_hub=True` cuando definas tus `TrainingArguments`:

```py
from transformers import TrainingArguments

training_args = TrainingArguments(
    "bert-finetuned-mrpc", save_strategy="epoch", push_to_hub=True
)
```

Cuando llames a `trainer.train()`, el `Trainer` entonces subira tu modelo al Hub cada vez que sea guardado (aqui cada epoca) en un repositorio en tu espacio de nombres. Ese repositorio tendra el nombre del directorio de salida que elegiste (aqui `bert-finetuned-mrpc`) pero puedes elegir un nombre diferente con `hub_model_id = "un_nombre_diferente"`.

Para subir tu modelo a una organizacion de la que eres miembro, simplemente pasala con `hub_model_id = "mi_organizacion/nombre_de_mi_repo"`.

Una vez que tu entrenamiento haya terminado, deberias hacer un `trainer.push_to_hub()` final para subir la ultima version de tu modelo. Tambien generara una tarjeta de modelo con todos los metadatos relevantes, reportando los hiperparametros usados y los resultados de evaluacion! Aqui hay un ejemplo del contenido que podrias encontrar en una tarjeta de modelo asi:

<div class="flex justify-center">
  <img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/model_card.png" alt="Un ejemplo de una tarjeta de modelo autogenerada." width="100%"/>
</div>

**TensorFlow/Keras:**

Si estas usando Keras para entrenar tu modelo, la forma mas facil de subirlo al Hub es pasar un `PushToHubCallback` cuando llames a `model.fit()`:

```py
from transformers import PushToHubCallback

callback = PushToHubCallback(
    "bert-finetuned-mrpc", save_strategy="epoch", tokenizer=tokenizer
)
```

Luego deberias agregar `callbacks=[callback]` en tu llamada a `model.fit()`. El callback entonces subira tu modelo al Hub cada vez que sea guardado (aqui cada epoca) en un repositorio en tu espacio de nombres. Ese repositorio tendra el nombre del directorio de salida que elegiste (aqui `bert-finetuned-mrpc`) pero puedes elegir un nombre diferente con `hub_model_id = "un_nombre_diferente"`.

Para subir tu modelo a una organizacion de la que eres miembro, simplemente pasala con `hub_model_id = "mi_organizacion/nombre_de_mi_repo"`.


A un nivel mas bajo, acceder al Model Hub puede hacerse directamente en modelos, tokenizadores y objetos de configuracion via su metodo `push_to_hub()`. Este metodo se encarga tanto de la creacion del repositorio como de subir los archivos del modelo y tokenizador directamente al repositorio. No se requiere manejo manual, a diferencia de la API que veremos a continuacion.

Para tener una idea de como funciona, primero inicialicemos un modelo y un tokenizador:


**PyTorch:**

```py
from transformers import AutoModelForMaskedLM, AutoTokenizer

checkpoint = "camembert-base"

model = AutoModelForMaskedLM.from_pretrained(checkpoint)
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

**TensorFlow/Keras:**

```py
from transformers import TFAutoModelForMaskedLM, AutoTokenizer

checkpoint = "camembert-base"

model = TFAutoModelForMaskedLM.from_pretrained(checkpoint)
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```


Eres libre de hacer lo que quieras con estos ‚Äî agregar tokens al tokenizador, entrenar el modelo, ajustarlo. Una vez que estes satisfecho con el modelo, pesos y tokenizador resultantes, puedes aprovechar el metodo `push_to_hub()` disponible directamente en el objeto `model`:

```py
model.push_to_hub("dummy-model")
```

Esto creara el nuevo repositorio `dummy-model` en tu perfil, y lo poblara con tus archivos de modelo. Haz lo mismo con el tokenizador, para que todos los archivos esten ahora disponibles en este repositorio:

```py
tokenizer.push_to_hub("dummy-model")
```

Si perteneces a una organizacion, simplemente especifica el argumento `organization` para subir al espacio de nombres de esa organizacion:

```py
tokenizer.push_to_hub("dummy-model", organization="huggingface")
```

Si deseas usar un token de Hugging Face especifico, eres libre de especificarlo al metodo `push_to_hub()` tambien:

```py
tokenizer.push_to_hub("dummy-model", organization="huggingface", use_auth_token="<TOKEN>")
```

Ahora ve al Model Hub para encontrar tu modelo recien subido: *https://huggingface.co/user-or-organization/dummy-model*.

Haz clic en la pestana "Files and versions", y deberias ver los archivos visibles en la siguiente captura de pantalla:


**PyTorch:**

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/push_to_hub_dummy_model.png" alt="Modelo dummy que contiene tanto los archivos del tokenizador como del modelo." width="80%"/>
</div>

**TensorFlow/Keras:**

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/push_to_hub_dummy_model_tf.png" alt="Modelo dummy que contiene tanto los archivos del tokenizador como del modelo." width="80%"/>
</div>


> [!TIP]
> ‚úèÔ∏è **Pruebalo!** Toma el modelo y tokenizador asociados con el checkpoint `bert-base-cased` y subeloss a un repo en tu espacio de nombres usando el metodo `push_to_hub()`. Verifica que el repo aparezca correctamente en tu pagina antes de eliminarlo.

Como has visto, el metodo `push_to_hub()` acepta varios argumentos, lo que hace posible subir a un repositorio especifico o espacio de nombres de organizacion, o usar un token de API diferente. Te recomendamos echar un vistazo a la especificacion del metodo disponible directamente en la [documentacion de ü§ó Transformers](https://huggingface.co/transformers/model_sharing) para tener una idea de lo que es posible.

El metodo `push_to_hub()` esta respaldado por el paquete Python [`huggingface_hub`](https://github.com/huggingface/huggingface_hub), que ofrece una API directa al Hub de Hugging Face. Esta integrado dentro de ü§ó Transformers y varias otras bibliotecas de machine learning, como [`allenlp`](https://github.com/allenai/allennlp). Aunque nos enfocamos en la integracion con ü§ó Transformers en este capitulo, integrarlo en tu propio codigo o biblioteca es simple.

Salta a la ultima seccion para ver como subir archivos a tu repositorio recien creado!

## Usando la biblioteca Python `huggingface_hub`[[using-the-huggingfacehub-python-library]]

La biblioteca Python `huggingface_hub` es un paquete que ofrece un conjunto de herramientas para los hubs de modelos y conjuntos de datos. Proporciona metodos y clases simples para tareas comunes como obtener informacion sobre repositorios en el hub y administrarlos. Proporciona APIs simples que funcionan sobre git para administrar el contenido de esos repositorios e integrar el Hub en tus proyectos y bibliotecas.

Similar a usar la API `push_to_hub`, esto requerira que tengas tu token de API guardado en tu cache. Para hacer esto, necesitaras usar el comando `login` de la CLI, como se menciono en la seccion anterior (de nuevo, asegurate de anteponer estos comandos con el caracter `!` si estas ejecutando en Google Colab):

```bash
huggingface-cli login
```

El paquete `huggingface_hub` ofrece varios metodos y clases que son utiles para nuestro proposito. Primero, hay algunos metodos para administrar la creacion, eliminacion y otros de repositorios:

```python no-format
from huggingface_hub import (
    # Administracion de usuarios
    login,
    logout,
    whoami,

    # Creacion y administracion de repositorios
    create_repo,
    delete_repo,
    update_repo_visibility,

    # Y algunos metodos para recuperar/cambiar informacion sobre el contenido
    list_models,
    list_datasets,
    list_metrics,
    list_repo_files,
    upload_file,
    delete_file,
)
```

Adicionalmente, ofrece la muy poderosa clase `Repository` para administrar un repositorio local. Exploraremos estos metodos y esa clase en las proximas secciones para entender como aprovecharlos.

El metodo `create_repo` puede usarse para crear un nuevo repositorio en el hub:

```py
from huggingface_hub import create_repo

create_repo("dummy-model")
```

Esto creara el repositorio `dummy-model` en tu espacio de nombres. Si lo deseas, puedes especificar a que organizacion deberia pertenecer el repositorio usando el argumento `organization`:

```py
from huggingface_hub import create_repo

create_repo("dummy-model", organization="huggingface")
```

Esto creara el repositorio `dummy-model` en el espacio de nombres `huggingface`, asumiendo que perteneces a esa organizacion. Otros argumentos que pueden ser utiles son:

- `private`, para especificar si el repositorio deberia ser visible para otros o no.
- `token`, si deseas anular el token almacenado en tu cache por un token dado.
- `repo_type`, si deseas crear un `dataset` o un `space` en lugar de un modelo. Los valores aceptados son `"dataset"` y `"space"`.

Una vez que el repositorio este creado, debemos agregarle archivos! Salta a la siguiente seccion para ver las tres formas en que esto puede manejarse.

## Usando la interfaz web[[using-the-web-interface]]

La interfaz web ofrece herramientas para administrar repositorios directamente en el Hub. Usando la interfaz, puedes facilmente crear repositorios, agregar archivos (incluso grandes!), explorar modelos, visualizar diffs y mucho mas.

Para crear un nuevo repositorio, visita [huggingface.co/new](https://huggingface.co/new):

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/new_model.png" alt="Pagina que muestra el modelo usado para la creacion de un nuevo repositorio de modelo." width="80%"/>
</div>

Primero, especifica el propietario del repositorio: este puede ser tu o cualquiera de las organizaciones a las que estas afiliado. Si eliges una organizacion, el modelo sera destacado en la pagina de la organizacion y cada miembro de la organizacion tendra la capacidad de contribuir al repositorio.

Luego, ingresa el nombre de tu modelo. Este tambien sera el nombre del repositorio. Finalmente, puedes especificar si quieres que tu modelo sea publico o privado. Los modelos privados estan ocultos de la vista publica.

Despues de crear tu repositorio de modelo, deberias ver una pagina como esta:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/empty_model.png" alt="Una pagina de modelo vacia despues de crear un nuevo repositorio." width="80%"/>
</div>

Aqui es donde tu modelo sera alojado. Para comenzar a poblarlo, puedes agregar un archivo README directamente desde la interfaz web.

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/dummy_model.png" alt="El archivo README mostrando las capacidades de Markdown." width="80%"/>
</div>

El archivo README esta en Markdown ‚Äî sientete libre de experimentar con el! La tercera parte de este capitulo esta dedicada a construir una tarjeta de modelo. Estas son de primera importancia para dar valor a tu modelo, ya que es donde le dices a otros lo que puede hacer.

Si miras la pestana "Files and versions", veras que no hay muchos archivos ahi todavia ‚Äî solo el *README.md* que acabas de crear y el archivo *.gitattributes* que mantiene registro de archivos grandes.

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/files.png" alt="La pestana 'Files and versions' solo muestra los archivos .gitattributes y README.md." width="80%"/>
</div>

Veremos como agregar algunos archivos nuevos a continuacion.

## Subiendo los archivos del modelo[[uploading-the-model-files]]

El sistema para administrar archivos en el Hub de Hugging Face esta basado en git para archivos regulares, y git-lfs (que significa [Git Large File Storage](https://git-lfs.github.com/)) para archivos mas grandes.

En la siguiente seccion, repasamos tres formas diferentes de subir archivos al Hub: a traves de `huggingface_hub` y a traves de comandos git.

### El enfoque `upload_file`[[the-uploadfile-approach]]

Usar `upload_file` no requiere que git y git-lfs esten instalados en tu sistema. Sube archivos directamente al ü§ó Hub usando solicitudes HTTP POST. Una limitacion de este enfoque es que no maneja archivos que sean mas grandes de 5GB de tamano. Si tus archivos son mas grandes de 5GB, por favor sigue los otros dos metodos detallados a continuacion.

La API puede usarse de la siguiente manera:

```py
from huggingface_hub import upload_file

upload_file(
    "<path_to_file>/config.json",
    path_in_repo="config.json",
    repo_id="<namespace>/dummy-model",
)
```

Esto subira el archivo `config.json` disponible en `<path_to_file>` a la raiz del repositorio como `config.json`, al repositorio `dummy-model`. Otros argumentos que pueden ser utiles son:

- `token`, si deseas anular el token almacenado en tu cache por un token dado.
- `repo_type`, si deseas subir a un `dataset` o un `space` en lugar de un modelo. Los valores aceptados son `"dataset"` y `"space"`.

### La clase `Repository`[[the-repository-class]]

La clase `Repository` administra un repositorio local de manera similar a git. Abstrae la mayoria de los puntos problematicos que uno puede tener con git para proporcionar todas las caracteristicas que requerimos.

Usar esta clase requiere tener git y git-lfs instalados, asi que asegurate de tener git-lfs instalado (ver [aqui](https://git-lfs.github.com/) para instrucciones de instalacion) y configurado antes de comenzar.

Para comenzar a jugar con el repositorio que acabamos de crear, podemos empezar inicializandolo en una carpeta local clonando el repositorio remoto:

```py
from huggingface_hub import Repository

repo = Repository("<path_to_dummy_folder>", clone_from="<namespace>/dummy-model")
```

Esto creo la carpeta `<path_to_dummy_folder>` en nuestro directorio de trabajo. Esta carpeta solo contiene el archivo `.gitattributes` ya que ese es el unico archivo creado al instanciar el repositorio a traves de `create_repo`.

Desde este punto, podemos aprovechar varios de los metodos tradicionales de git:

```py
repo.git_pull()
repo.git_add()
repo.git_commit()
repo.git_push()
repo.git_tag()
```

Y otros! Recomendamos echar un vistazo a la documentacion de `Repository` disponible [aqui](https://github.com/huggingface/huggingface_hub/tree/main/src/huggingface_hub#advanced-programmatic-repository-management) para una vision general de todos los metodos disponibles.

Actualmente, tenemos un modelo y un tokenizador que nos gustaria subir al hub. Hemos clonado exitosamente el repositorio, por lo tanto podemos guardar los archivos dentro de ese repositorio.

Primero nos aseguramos de que nuestro clon local este actualizado obteniendo los ultimos cambios:

```py
repo.git_pull()
```

Una vez hecho eso, guardamos los archivos del modelo y tokenizador:

```py
model.save_pretrained("<path_to_dummy_folder>")
tokenizer.save_pretrained("<path_to_dummy_folder>")
```

El `<path_to_dummy_folder>` ahora contiene todos los archivos del modelo y tokenizador. Seguimos el flujo de trabajo usual de git agregando archivos al area de preparacion, haciendo commit y subiendo al hub:

```py
repo.git_add()
repo.git_commit("Add model and tokenizer files")
repo.git_push()
```

Felicidades! Acabas de subir tus primeros archivos al hub.

### El enfoque basado en git[[the-git-based-approach]]

Este es el enfoque mas basico para subir archivos: lo haremos con git y git-lfs directamente. La mayor parte de la dificultad es abstraida por los enfoques anteriores, pero hay algunas advertencias con el siguiente metodo asi que seguiremos un caso de uso mas complejo.

Usar esta clase requiere tener git y git-lfs instalados, asi que asegurate de tener [git-lfs](https://git-lfs.github.com/) instalado (ver aqui para instrucciones de instalacion) y configurado antes de comenzar.

Primero comienza inicializando git-lfs:

```bash
git lfs install
```

```bash
Updated git hooks.
Git LFS initialized.
```

Una vez hecho eso, el primer paso es clonar tu repositorio de modelo:

```bash
git clone https://huggingface.co/<namespace>/<your-model-id>
```

Mi nombre de usuario es `lysandre` y he usado el nombre de modelo `dummy`, asi que para mi el comando termina luciendo asi:

```
git clone https://huggingface.co/lysandre/dummy
```

Ahora tengo una carpeta llamada *dummy* en mi directorio de trabajo. Puedo entrar a la carpeta con `cd` y echar un vistazo a los contenidos:

```bash
cd dummy && ls
```

```bash
README.md
```

Si acabas de crear tu repositorio usando el metodo `create_repo` del Hub de Hugging Face, esta carpeta solo deberia contener un archivo oculto `.gitattributes`. Si seguiste las instrucciones en la seccion anterior para crear un repositorio usando la interfaz web, la carpeta deberia contener un solo archivo *README.md* junto al archivo oculto `.gitattributes`, como se muestra aqui.

Agregar un archivo de tamano regular, como un archivo de configuracion, un archivo de vocabulario, o basicamente cualquier archivo menor a unos pocos megabytes, se hace exactamente como se haria en cualquier sistema basado en git. Sin embargo, archivos mas grandes deben registrarse a traves de git-lfs para poder subirlos a *huggingface.co*.

Volvamos a Python por un momento para generar un modelo y tokenizador que nos gustaria enviar a nuestro repositorio dummy:


**PyTorch:**

```py
from transformers import AutoModelForMaskedLM, AutoTokenizer

checkpoint = "camembert-base"

model = AutoModelForMaskedLM.from_pretrained(checkpoint)
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

# Haz lo que quieras con el modelo, entrenalo, ajustalo...

model.save_pretrained("<path_to_dummy_folder>")
tokenizer.save_pretrained("<path_to_dummy_folder>")
```

**TensorFlow/Keras:**

```py
from transformers import TFAutoModelForMaskedLM, AutoTokenizer

checkpoint = "camembert-base"

model = TFAutoModelForMaskedLM.from_pretrained(checkpoint)
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

# Haz lo que quieras con el modelo, entrenalo, ajustalo...

model.save_pretrained("<path_to_dummy_folder>")
tokenizer.save_pretrained("<path_to_dummy_folder>")
```


Ahora que hemos guardado algunos artefactos de modelo y tokenizador, echemos otro vistazo a la carpeta *dummy*:

```bash
ls
```


**PyTorch:**

```bash
config.json  pytorch_model.bin  README.md  sentencepiece.bpe.model  special_tokens_map.json tokenizer_config.json  tokenizer.json
```

Si miras los tamanos de archivo (por ejemplo, con `ls -lh`), deberias ver que el archivo de diccionario de estado del modelo (*pytorch_model.bin*) es el unico atipico, con mas de 400 MB.

**TensorFlow/Keras:**

```bash
config.json  README.md  sentencepiece.bpe.model  special_tokens_map.json  tf_model.h5  tokenizer_config.json  tokenizer.json
```

Si miras los tamanos de archivo (por ejemplo, con `ls -lh`), deberias ver que el archivo de diccionario de estado del modelo (*t5_model.h5*) es el unico atipico, con mas de 400 MB.


> [!TIP]
> ‚úèÔ∏è Al crear el repositorio desde la interfaz web, el archivo *.gitattributes* se configura automaticamente para considerar archivos con ciertas extensiones, como *.bin* y *.h5*, como archivos grandes, y git-lfs los rastreara sin ninguna configuracion necesaria de tu parte.

Ahora podemos continuar y proceder como normalmente lo hariamos con repositorios Git tradicionales. Podemos agregar todos los archivos al entorno de preparacion de Git usando el comando `git add`:

```bash
git add .
```

Luego podemos echar un vistazo a los archivos que estan actualmente preparados:

```bash
git status
```


**PyTorch:**

```bash
On branch main
Your branch is up to date with 'origin/main'.

Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
  modified:   .gitattributes
	new file:   config.json
	new file:   pytorch_model.bin
	new file:   sentencepiece.bpe.model
	new file:   special_tokens_map.json
	new file:   tokenizer.json
	new file:   tokenizer_config.json
```

**TensorFlow/Keras:**

```bash
On branch main
Your branch is up to date with 'origin/main'.

Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
  modified:   .gitattributes
  	new file:   config.json
	new file:   sentencepiece.bpe.model
	new file:   special_tokens_map.json
	new file:   tf_model.h5
	new file:   tokenizer.json
	new file:   tokenizer_config.json
```


Similarmente, podemos asegurarnos de que git-lfs este rastreando los archivos correctos usando su comando `status`:

```bash
git lfs status
```


**PyTorch:**

```bash
On branch main
Objects to be pushed to origin/main:


Objects to be committed:

	config.json (Git: bc20ff2)
	pytorch_model.bin (LFS: 35686c2)
	sentencepiece.bpe.model (LFS: 988bc5a)
	special_tokens_map.json (Git: cb23931)
	tokenizer.json (Git: 851ff3e)
	tokenizer_config.json (Git: f0f7783)

Objects not staged for commit:


```

Podemos ver que todos los archivos tienen `Git` como manejador, excepto *pytorch_model.bin* y *sentencepiece.bpe.model*, que tienen `LFS`. Genial!

**TensorFlow/Keras:**

```bash
On branch main
Objects to be pushed to origin/main:


Objects to be committed:

	config.json (Git: bc20ff2)
	sentencepiece.bpe.model (LFS: 988bc5a)
	special_tokens_map.json (Git: cb23931)
	tf_model.h5 (LFS: 86fce29)
	tokenizer.json (Git: 851ff3e)
	tokenizer_config.json (Git: f0f7783)

Objects not staged for commit:


```

Podemos ver que todos los archivos tienen `Git` como manejador, excepto *t5_model.h5*, que tiene `LFS`. Genial!


Procedamos a los pasos finales, haciendo commit y subiendo al repositorio remoto de *huggingface.co*:

```bash
git commit -m "First model version"
```


**PyTorch:**

```bash
[main b08aab1] First model version
 7 files changed, 29027 insertions(+)
  6 files changed, 36 insertions(+)
 create mode 100644 config.json
 create mode 100644 pytorch_model.bin
 create mode 100644 sentencepiece.bpe.model
 create mode 100644 special_tokens_map.json
 create mode 100644 tokenizer.json
 create mode 100644 tokenizer_config.json
```

**TensorFlow/Keras:**

```bash
[main b08aab1] First model version
 6 files changed, 36 insertions(+)
 create mode 100644 config.json
 create mode 100644 sentencepiece.bpe.model
 create mode 100644 special_tokens_map.json
 create mode 100644 tf_model.h5
 create mode 100644 tokenizer.json
 create mode 100644 tokenizer_config.json
```


Subir puede tomar un poco de tiempo, dependiendo de la velocidad de tu conexion a internet y el tamano de tus archivos:

```bash
git push
```

```bash
Uploading LFS objects: 100% (1/1), 433 MB | 1.3 MB/s, done.
Enumerating objects: 11, done.
Counting objects: 100% (11/11), done.
Delta compression using up to 12 threads
Compressing objects: 100% (9/9), done.
Writing objects: 100% (9/9), 288.27 KiB | 6.27 MiB/s, done.
Total 9 (delta 1), reused 0 (delta 0), pack-reused 0
To https://huggingface.co/lysandre/dummy
   891b41d..b08aab1  main -> main
```


**PyTorch:**

Si echamos un vistazo al repositorio del modelo cuando esto termine, podemos ver todos los archivos agregados recientemente:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/full_model.png" alt="La pestana 'Files and versions' ahora contiene todos los archivos subidos recientemente." width="80%"/>
</div>

La interfaz de usuario te permite explorar los archivos del modelo y commits y ver el diff introducido por cada commit:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/diffs.gif" alt="El diff introducido por el commit reciente." width="80%"/>
</div>

**TensorFlow/Keras:**

Si echamos un vistazo al repositorio del modelo cuando esto termine, podemos ver todos los archivos agregados recientemente:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/full_model_tf.png" alt="La pestana 'Files and versions' ahora contiene todos los archivos subidos recientemente." width="80%"/>
</div>

La interfaz de usuario te permite explorar los archivos del modelo y commits y ver el diff introducido por cada commit:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/diffstf.gif" alt="El diff introducido por el commit reciente." width="80%"/>
</div>



---

# Construyendo una tarjeta de modelo[[building-a-model-card]]


La tarjeta de modelo es un archivo que es posiblemente tan importante como los archivos del modelo y tokenizador en un repositorio de modelo. Es la definicion central del modelo, asegurando la reutilizacion por otros miembros de la comunidad y la reproducibilidad de resultados, y proporcionando una plataforma sobre la cual otros miembros pueden construir sus artefactos.

Documentar el proceso de entrenamiento y evaluacion ayuda a otros a entender que esperar de un modelo ‚Äî y proporcionar informacion suficiente sobre los datos que fueron usados y el preprocesamiento y postprocesamiento que se hicieron asegura que las limitaciones, sesgos y contextos en los que el modelo es y no es util puedan ser identificados y entendidos.

Por lo tanto, crear una tarjeta de modelo que defina claramente tu modelo es un paso muy importante. Aqui, proporcionamos algunos consejos que te ayudaran con esto. Crear la tarjeta de modelo se hace a traves del archivo *README.md* que viste antes, que es un archivo Markdown.

El concepto de "tarjeta de modelo" se origina de una direccion de investigacion de Google, compartida primero en el articulo ["Model Cards for Model Reporting"](https://arxiv.org/abs/1810.03993) por Margaret Mitchell et al. Mucha de la informacion contenida aqui esta basada en ese articulo, y te recomendamos echarle un vistazo para entender por que las tarjetas de modelo son tan importantes en un mundo que valora la reproducibilidad, reutilizacion y equidad.

La tarjeta de modelo usualmente comienza con una descripcion muy breve y de alto nivel de para que es el modelo, seguida de detalles adicionales en las siguientes secciones:

- Descripcion del modelo
- Usos previstos y limitaciones
- Como usar
- Limitaciones y sesgo
- Datos de entrenamiento
- Procedimiento de entrenamiento
- Resultados de evaluacion

Veamos que deberia contener cada una de estas secciones.

### Descripcion del modelo[[model-description]]

La descripcion del modelo proporciona detalles basicos sobre el modelo. Esto incluye la arquitectura, version, si fue introducido en un articulo, si hay una implementacion original disponible, el autor e informacion general sobre el modelo. Cualquier derecho de autor deberia atribuirse aqui. Informacion general sobre procedimientos de entrenamiento, parametros y avisos importantes tambien puede mencionarse en esta seccion.

### Usos previstos y limitaciones[[intended-uses-limitations]]

Aqui describes los casos de uso para los que el modelo esta destinado, incluyendo los idiomas, campos y dominios donde puede aplicarse. Esta seccion de la tarjeta de modelo tambien puede documentar areas que se sabe que estan fuera del alcance del modelo, o donde es probable que tenga un rendimiento suboptimo.

### Como usar[[how-to-use]]

Esta seccion deberia incluir algunos ejemplos de como usar el modelo. Esto puede mostrar el uso de la funcion `pipeline()`, uso de las clases de modelo y tokenizador, y cualquier otro codigo que creas que podria ser util.

### Datos de entrenamiento[[training-data]]

Esta parte deberia indicar en que conjunto(s) de datos fue entrenado el modelo. Una breve descripcion del conjunto o conjuntos de datos tambien es bienvenida.

### Procedimiento de entrenamiento[[training-procedure]]

En esta seccion deberias describir todos los aspectos relevantes del entrenamiento que son utiles desde una perspectiva de reproducibilidad. Esto incluye cualquier preprocesamiento y postprocesamiento que se hizo en los datos, asi como detalles como el numero de epocas por las que el modelo fue entrenado, el tamano del lote, la tasa de aprendizaje, y asi sucesivamente.

### Variables y metricas[[variable-and-metrics]]

Aqui deberias describir las metricas que usas para evaluacion, y los diferentes factores que estas midiendo. Mencionar que metrica(s) fueron usadas, en que conjunto de datos y que division del conjunto de datos, facilita comparar el rendimiento de tu modelo con el de otros modelos. Esto deberia estar informado por las secciones anteriores, como los usuarios previstos y casos de uso.

### Resultados de evaluacion[[evaluation-results]]

Finalmente, proporciona una indicacion de que tan bien se desempena el modelo en el conjunto de datos de evaluacion. Si el modelo usa un umbral de decision, proporciona el umbral de decision usado en la evaluacion, o proporciona detalles sobre la evaluacion en diferentes umbrales para los usos previstos.

## Ejemplo[[example]]

Consulta los siguientes ejemplos de tarjetas de modelo bien elaboradas:

- [`bert-base-cased`](https://huggingface.co/bert-base-cased)
- [`gpt2`](https://huggingface.co/gpt2)
- [`distilbert`](https://huggingface.co/distilbert-base-uncased)

Mas ejemplos de diferentes organizaciones y companias estan disponibles [aqui](https://github.com/huggingface/model_card/blob/master/examples.md).

## Nota[[note]]

Las tarjetas de modelo no son un requisito al publicar modelos, y no necesitas incluir todas las secciones descritas arriba cuando hagas una. Sin embargo, documentacion explicita del modelo solo puede beneficiar a futuros usuarios, por lo que recomendamos que completes tantas secciones como sea posible con lo mejor de tu conocimiento y capacidad.

## Metadatos de la tarjeta de modelo[[model-card-metadata]]

Si has explorado un poco el Hub de Hugging Face, deberias haber visto que algunos modelos pertenecen a ciertas categorias: puedes filtrarlos por tareas, idiomas, bibliotecas y mas. Las categorias a las que pertenece un modelo se identifican segun los metadatos que agregas en el encabezado de la tarjeta de modelo.

Por ejemplo, si echas un vistazo a la [tarjeta de modelo `camembert-base`](https://huggingface.co/camembert-base/blob/main/README.md), deberias ver las siguientes lineas en el encabezado de la tarjeta de modelo:

```
---
language: fr
license: mit
datasets:
- oscar
---
```

Estos metadatos son analizados por el Hub de Hugging Face, que luego identifica este modelo como un modelo frances, con una licencia MIT, entrenado en el conjunto de datos Oscar.

La [especificacion completa de la tarjeta de modelo](https://github.com/huggingface/hub-docs/blame/main/modelcard.md) permite especificar idiomas, licencias, etiquetas, conjuntos de datos, metricas, asi como los resultados de evaluacion que el modelo obtuvo durante el entrenamiento.


---

# Parte 1 completada![[part-1-completed]]


Este es el final de la primera parte del curso! La Parte 2 sera lanzada el 15 de noviembre con un gran evento comunitario, consulta mas informacion [aqui](https://huggingface.co/blog/course-launch-event).

Ahora deberias ser capaz de ajustar finamente un modelo preentrenado en un problema de clasificacion de texto (oraciones individuales o pares de oraciones) y subir el resultado al Model Hub. Para asegurarte de que dominaste esta primera seccion, deberias hacer exactamente eso en un problema que te interese (y no necesariamente en ingles si hablas otro idioma)! Puedes encontrar ayuda en los [foros de Hugging Face](https://discuss.huggingface.co/) y compartir tu proyecto en [este tema](https://discuss.huggingface.co/t/share-your-projects/6803) una vez que hayas terminado.

No podemos esperar a ver lo que construiras con esto!


---




# Cuestionario de fin de capitulo[[end-of-chapter-quiz]]


Probemos lo que aprendiste en este capitulo!

### 1. A que estan limitados los modelos en el Hub?


- Modelos de la biblioteca ü§ó Transformers.
- Todos los modelos con una interfaz similar a ü§ó Transformers.
- No hay limites.
- Modelos que estan de alguna manera relacionados con NLP.


### 2. Como puedes administrar modelos en el Hub?


- A traves de una cuenta de GCP.
- A traves de distribucion peer-to-peer.
- A traves de git y git-lfs.


### 3. Que puedes hacer usando la interfaz web del Hub de Hugging Face?


- Hacer fork de un repositorio existente.
- Crear un nuevo repositorio de modelo.
- Administrar y editar archivos.
- Subir archivos.
- Ver diffs entre versiones.


### 4. Que es una tarjeta de modelo?


- Una descripcion aproximada del modelo, por lo tanto menos importante que los archivos del modelo y tokenizador.
- Una forma de asegurar reproducibilidad, reutilizacion y equidad.
- Un archivo Python que puede ejecutarse para recuperar informacion sobre el modelo.


### 5. Cuales de estos objetos de la biblioteca ü§ó Transformers pueden compartirse directamente en el Hub con `push_to_hub()`?


**PyTorch:**


- Un tokenizador
- Una configuracion de modelo
- Un modelo
- Un Trainer


**TensorFlow/Keras:**


- Un tokenizador
- Una configuracion de modelo
- Un modelo
- Todo lo anterior con un callback dedicado


### 6. Cual es el primer paso al usar el metodo `push_to_hub()` o las herramientas CLI?


- Iniciar sesion en el sitio web.
- Ejecutar 
- Ejecutar 


### 7. Estas usando un modelo y un tokenizador ‚Äî como puedes subirlos al Hub?


- Llamando al metodo push_to_hub directamente en el modelo y el tokenizador.
- Dentro del entorno de ejecucion de Python, envolviendolos en una utilidad de `huggingface_hub`.
- Guardandolos en disco y llamando a `transformers-cli upload-model`


### 8. Que operaciones de git puedes hacer con la clase `Repository`?


- Un commit.
- Un pull
- Un push
- Un merge



---

